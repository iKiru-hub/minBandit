@book{sutton1998,
  author    = {Richard S. Sutton and Andrew G. Barto},
  title     = {Reinforcement Learning: An Introduction},
  year      = {1998},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
}

@article{dayan2008,
  title={Decision theory, reinforcement learning, and the brain},
  author={Dayan, Peter and Daw, Nathaniel D},
  journal={Cognitive, Affective, \& Behavioral Neuroscience},
  volume={8},
  number={4},
  pages={429--453},
  year={2008},
  publisher={Springer}
}

@inproceedings{agrawalAnalysisThompsonSampling2012,
  title = {Analysis of {{Thompson Sampling}} for the {{Multi-armed Bandit Problem}}},
  booktitle = {Proceedings of the 25th {{Annual Conference}} on {{Learning Theory}}},
  author = {Agrawal, Shipra and Goyal, Navin},
  year = {2012},
  month = jun,
  pages = {39.1-39.26},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2024-03-25},
  abstract = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the stochastic multi-armed bandit problem. More precisely, for the stochastic two-armed bandit problem, the expected regret in time T is O({\textbackslash}frac{\textbackslash}ln T∆ + {\textbackslash}frac1∆{\textasciicircum}3). And, for the stochastic N-armed bandit problem, the expected regret in time T is O({\textbackslash}left[{\textbackslash}left({\textbackslash}sum\_i=2{\textasciicircum}N {\textbackslash}frac1{\textbackslash}Delta\_i{\textasciicircum}2{\textbackslash}right){\textasciicircum}2{\textbackslash}right] {\textbackslash}ln T). Our bounds are optimal but for the dependence on {\textbackslash}Delta\_i and the constant factors in big-Oh.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/FIASC57Q/Agrawal and Goyal - 2012 - Analysis of Thompson Sampling for the Multi-armed .pdf}
}

@article{auerFinitetimeAnalysisMultiarmed,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/N9AC4Q7C/Auer and Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf}
}

@article{averbeckTheoryChoiceBandit2015,
  title = {Theory of {{Choice}} in {{Bandit}}, {{Information Sampling}} and {{Foraging Tasks}}},
  author = {Averbeck, Bruno B.},
  year = {2015},
  month = mar,
  journal = {PLoS Computational Biology},
  volume = {11},
  number = {3},
  pages = {e1004164},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1004164},
  urldate = {2024-03-25},
  abstract = {Decision making has been studied with a wide array of tasks. Here we examine the theoretical structure of bandit, information sampling and foraging tasks. These tasks move beyond tasks where the choice in the current trial does not affect future expected rewards. We have modeled these tasks using Markov decision processes (MDPs). MDPs provide a general framework for modeling tasks in which decisions affect the information on which future choices will be made. Under the assumption that agents are maximizing expected rewards, MDPs provide normative solutions. We find that all three classes of tasks pose choices among actions which trade-off immediate and future expected rewards. The tasks drive these trade-offs in unique ways, however. For bandit and information sampling tasks, increasing uncertainty or the time horizon shifts value to actions that pay-off in the future. Correspondingly, decreasing uncertainty increases the relative value of actions that pay-off immediately. For foraging tasks the time-horizon plays the dominant role, as choices do not affect future uncertainty in these tasks., Numerous choice tasks have been used to study decision processes. Some of these choice tasks, specifically n-armed bandit, information sampling and foraging tasks, pose choices that trade-off immediate and future reward. Specifically, the best choice may not be the choice that pays off the highest reward immediately, and exploration of unknown options vs. exploiting known options can be a normatively useful strategy. We characterized the optimal choice strategies across these tasks using Markov Decision Processes (MDPs). The MDP framework can characterize optimal choice strategies when choices are affected by the value of future rewards. We found that uncertainty and time horizon have important effects on the choice strategies in these tasks. Specifically, in bandit and information sampling tasks, increasing uncertainty increases the value of exploring choice options that tend to pay off in the future, while decreasing uncertainty increases the value of choice options that pay off immediately. These effects are increased when time horizons are longer. Foraging tasks differ in that uncertainty plays a minimal role. However, time horizon is still important in foraging. Specifically, for long time horizons, travel delays to rewards become less relevant.},
  pmcid = {PMC4376795},
  pmid = {25815510},
  file = {/Users/daniekru/Zotero/storage/SIVS27TJ/Averbeck - 2015 - Theory of Choice in Bandit, Information Sampling a.pdf}
}

@article{backmanEffectsWorkingMemoryTraining2011,
  title = {Effects of {{Working-Memory Training}} on {{Striatal Dopamine Release}}},
  author = {B{\"a}ckman, Lars and Nyberg, Lars and Soveri, Anna and Johansson, Jarkko and Andersson, Micael and Dahlin, Erika and Neely, Anna S. and Virta, Jere and Laine, Matti and Rinne, Juha O.},
  year = {2011},
  month = aug,
  journal = {Science},
  volume = {333},
  number = {6043},
  pages = {718--718},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1204978},
  urldate = {2024-04-26},
  abstract = {Updating of working memory has been associated with striato-frontal brain regions and phasic dopaminergic neurotransmission. We assessed raclopride binding to striatal dopamine (DA) D2 receptors during a letter-updating task and a control condition before and after 5 weeks of updating training. Results showed that updating affected DA activity before training and that training further increased striatal DA release during updating. These findings highlight the pivotal role of transient neural processes associated with D2 receptor activity in working memory.},
  file = {/Users/daniekru/Zotero/storage/CM3EC9SP/Bäckman et al. - 2011 - Effects of Working-Memory Training on Striatal Dop.pdf}
}

@misc{banMultifacetContextualBandits2021,
  title = {Multi-Facet {{Contextual Bandits}}: {{A Neural Network Perspective}}},
  shorttitle = {Multi-Facet {{Contextual Bandits}}},
  author = {Ban, Yikun and He, Jingrui and Cook, Curtiss B.},
  year = {2021},
  month = jun,
  number = {arXiv:2106.03039},
  eprint = {2106.03039},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-03},
  abstract = {Contextual multi-armed bandit has shown to be an effective tool in recommender systems. In this paper, we study a novel problem of multi-facet bandits involving a group of bandits, each characterizing the users' needs from one unique aspect. In each round, for the given user, we need to select one arm from each bandit, such that the combination of all arms maximizes the final reward. This problem can find immediate applications in E-commerce, healthcare, etc. To address this problem, we propose a novel algorithm, named MuFasa, which utilizes an assembled neural network to jointly learn the underlying reward functions of multiple bandits. It estimates an Upper Confidence Bound (UCB) linked with the expected reward to balance between exploitation and exploration. Under mild assumptions, we provide the regret analysis of MuFasa. It can achieve the near-optimal \${\textbackslash}widetilde\{ {\textbackslash}mathcal\{O\}\}((K+1){\textbackslash}sqrt\{T\})\$ regret bound where \$K\$ is the number of bandits and \$T\$ is the number of played rounds. Furthermore, we conduct extensive experiments to show that MuFasa outperforms strong baselines on real-world data sets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/3LTK9QB4/Ban et al. - 2021 - Multi-facet Contextual Bandits A Neural Network P.pdf}
}

@article{behrensLearningValueInformation2007,
  title = {Learning the Value of Information in an Uncertain World},
  author = {Behrens, Timothy E. J. and Woolrich, Mark W. and Walton, Mark E. and Rushworth, Matthew F. S.},
  year = {2007},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {9},
  pages = {1214--1221},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn1954},
  urldate = {2024-05-02},
  abstract = {Our decisions are guided by outcomes that are associated with decisions made in the past. However, the amount of influence each past outcome has on our next decision remains unclear. To ensure optimal decision-making, the weight given to decision outcomes should reflect their salience in predicting future outcomes, and this salience should be modulated by the volatility of the reward environment. We show that human subjects assess volatility in an optimal manner and adjust decision-making accordingly. This optimal estimate of volatility is reflected in the fMRI signal in the anterior cingulate cortex (ACC) when each trial outcome is observed. When a new piece of information is witnessed, activity levels reflect its salience for predicting future outcomes. Furthermore, variations in this ACC signal across the population predict variations in subject learning rates. Our results provide a formal account of how we weigh our different experiences in guiding our future actions.},
  copyright = {2007 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/HQA3X56G/Behrens et al. - 2007 - Learning the value of information in an uncertain .pdf}
}

@inproceedings{besbesStochasticMultiArmedBanditProblem2014,
  title = {Stochastic {{Multi-Armed-Bandit Problem}} with {{Non-stationary Rewards}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
  year = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-25},
  abstract = {In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward variation" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks."},
  file = {/Users/daniekru/Zotero/storage/DIVA8FYS/Besbes et al. - 2014 - Stochastic Multi-Armed-Bandit Problem with Non-sta.pdf}
}

@misc{binzModelingHumanExploration2022,
  title = {Modeling {{Human Exploration Through Resource-Rational Reinforcement Learning}}},
  author = {Binz, Marcel and Schulz, Eric},
  year = {2022},
  month = nov,
  number = {arXiv:2201.11817},
  eprint = {2201.11817},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-03},
  abstract = {Equipping artificial agents with useful exploration mechanisms remains a challenge to this day. Humans, on the other hand, seem to manage the trade-off between exploration and exploitation effortlessly. In the present article, we put forward the hypothesis that they accomplish this by making optimal use of limited computational resources. We study this hypothesis by meta-learning reinforcement learning algorithms that sacrifice performance for a shorter description length (defined as the number of bits required to implement the given algorithm). The emerging class of models captures human exploration behavior better than previously considered approaches, such as Boltzmann exploration, upper confidence bound algorithms, and Thompson sampling. We additionally demonstrate that changing the description length in our class of models produces the intended effects: reducing description length captures the behavior of brain-lesioned patients while increasing it mirrors cognitive development during adolescence.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/RIP3QH3R/Binz and Schulz - 2022 - Modeling Human Exploration Through Resource-Ration.pdf}
}

@book{burtiniImprovingOnlineMarketing2015,
  title = {￼{{Improving Online Marketing Experiments}} with {{Drifting Multi-Armed Bandits}}},
  author = {Burtini, Giuseppe and Loeppky, Jason and Lawrence, Ramon},
  year = {2015},
  month = apr,
  journal = {ICEIS 2015 - 17th International Conference on Enterprise Information Systems, Proceedings},
  volume = {1},
  doi = {10.5220/0005458706300636},
  abstract = {Restless bandits model the exploration vs. exploitation trade-off in a changing (non-stationary) world. Restless bandits have been studied in both the context of continuously-changing (drifting) and change-point (sudden) restlessness. In this work, we study specific classes of drifting restless bandits selected for their relevance to modelling an online website optimization process. The contribution in this work is a simple, feasible weighted least squares technique capable of utilizing contextual arm parameters while considering the parameter space drifting non-stationary within reasonable bounds. We produce a reference implementation, then evaluate and compare its performance in several different true world states, finding experimentally that performance is robust to time drifting factors similar to those seen in many real world cases.},
  file = {/Users/daniekru/Zotero/storage/RV3P2QBX/Burtini et al. - 2015 - ￼Improving Online Marketing Experiments with Drift.pdf}
}

@article{cavenaghiNonStationaryMultiArmed2021,
  title = {Non {{Stationary Multi-Armed Bandit}}: {{Empirical Evaluation}} of a {{New Concept Drift-Aware Algorithm}}},
  shorttitle = {Non {{Stationary Multi-Armed Bandit}}},
  author = {Cavenaghi, Emanuele and Sottocornola, Gabriele and Stella, Fabio and Zanker, Markus},
  year = {2021},
  month = mar,
  journal = {Entropy},
  volume = {23},
  number = {3},
  pages = {380},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23030380},
  urldate = {2024-03-25},
  abstract = {The Multi-Armed Bandit (MAB) problem has been extensively studied in order to address real-world challenges related to sequential decision making. In this setting, an agent selects the best action to be performed at time-step t, based on the past rewards received by the environment. This formulation implicitly assumes that the expected payoff for each action is kept stationary by the environment through time. Nevertheless, in many real-world applications this assumption does not hold and the agent has to face a non-stationary environment, that is, with a changing reward distribution. Thus, we present a new MAB algorithm, named f-Discounted-Sliding-Window Thompson Sampling (f-dsw TS), for non-stationary environments, that is, when the data streaming is affected by concept drift. The f-dsw TS algorithm is based on Thompson Sampling (TS) and exploits a discount factor on the reward history and an arm-related sliding window to contrast concept drift in non-stationary environments. We investigate how to combine these two sources of information, namely the discount factor and the sliding window, by means of an aggregation function f(.). In particular, we proposed a pessimistic (f=min), an optimistic (f=max), as well as an averaged (f=mean) version of the f-dsw TS algorithm. A rich set of numerical experiments is performed to evaluate the f-dsw TS algorithm compared to both stationary and non-stationary state-of-the-art TS baselines. We exploited synthetic environments (both randomly-generated and controlled) to test the MAB algorithms under different types of drift, that is, sudden/abrupt, incremental, gradual and increasing/decreasing drift. Furthermore, we adapt four real-world active learning tasks to our framework---a prediction task on crimes in the city of Baltimore, a classification task on insects species, a recommendation task on local web-news, and a time-series analysis on microbial organisms in the tropical air ecosystem. The f-dsw TS approach emerges as the best performing MAB algorithm. At least one of the versions of f-dsw TS performs better than the baselines in synthetic environments, proving the robustness of f-dsw TS under different concept drift types. Moreover, the pessimistic version (f=min) results as the most effective in all real-world tasks.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {concept drift,machine learning,multi-armed bandit,non-stationary multi-armed bandit,Thompson Sampling,time-series analysis},
  file = {/Users/daniekru/Zotero/storage/IE86HVJ3/Cavenaghi et al. - 2021 - Non Stationary Multi-Armed Bandit Empirical Evalu.pdf}
}

@article{chenSpikingNeuralNetwork2023,
  title = {Spiking Neural Network with Working Memory Can Integrate and Rectify Spatiotemporal Features},
  author = {Chen, Yi and Liu, Hanwen and Shi, Kexin and Zhang, Malu and Qu, Hong},
  year = {2023},
  month = jun,
  journal = {Frontiers in Neuroscience},
  volume = {17},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2023.1167134},
  urldate = {2024-05-03},
  abstract = {{$<$}p{$>$}In the real world, information is often correlated with each other in the time domain. Whether it can effectively make a decision according to the global information is the key indicator of information processing ability. Due to the discrete characteristics of spike trains and unique temporal dynamics, spiking neural networks (SNNs) show great potential in applications in ultra-low-power platforms and various temporal-related real-life tasks. However, the current SNNs can only focus on the information a short time before the current moment, its sensitivity in the time domain is limited. This problem affects the processing ability of SNN in different kinds of data, including static data and time-variant data, and reduces the application scenarios and scalability of SNN. In this work, we analyze the impact of such information loss and then integrate SNN with working memory inspired by recent neuroscience research. Specifically, we propose Spiking Neural Networks with Working Memory (SNNWM) to handle input spike trains segment by segment. On the one hand, this model can effectively increase SNN's ability to obtain global information. On the other hand, it can effectively reduce the information redundancy between adjacent time steps. Then, we provide simple methods to implement the proposed network architecture from the perspectives of biological plausibility and neuromorphic hardware friendly. Finally, we test the proposed method on static and sequential data sets, and the experimental results show that the proposed model can better process the whole spike train, and achieve state-of-the-art results in short time steps. This work investigates the contribution of introducing biologically inspired mechanisms, e.g., working memory, and multiple delayed synapses to SNNs, and provides a new perspective to design future SNNs.{$<$}/p{$>$}},
  langid = {english},
  keywords = {CIFAR10,Convolutional Neural Network,Multi-dendrite,Spiking Neural network,working memory},
  file = {/Users/daniekru/Zotero/storage/F87L27R5/Chen et al. - 2023 - Spiking neural network with working memory can int.pdf}
}

@article{cohenShouldStayShould2007,
  title = {Should {{I}} Stay or Should {{I}} Go? {{How}} the Human Brain Manages the Trade-off between Exploitation and Exploration [{{Proceedings}} Paper]},
  shorttitle = {Should {{I}} Stay or Should {{I}} Go?},
  author = {Cohen, Jonathan and McClure, Samuel M. and Yu, Angela},
  year = {2007},
  month = mar,
  journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
  volume = {362},
  pages = {933--42},
  doi = {10.1098/rstb.2007.2098},
  abstract = {Many large and small decisions we make in our daily lives-which ice cream to choose, what research projects to pursue, which partner to marry-require an exploration of alternatives before committing to and exploiting the benefits of a particular choice. Furthermore, many decisions require re-evaluation, and further exploration of alternatives, in the face of changing needs or circumstances. That is, often our decisions depend on a higher level choice: whether to exploit well known but possibly suboptimal alternatives or to explore risky but potentially more profitable ones. How adaptive agents choose between exploitation and exploration remains an important and open question that has received relatively limited attention in the behavioural and brain sciences. The choice could depend on a number of factors, including the familiarity of the environment, how quickly the environment is likely to change and the relative value of exploiting known sources of reward versus the cost of reducing uncertainty through exploration. There is no known generally optimal solution to the exploration versus exploitation problem, and a solution to the general case may indeed not be possible. However, there have been formal analyses of the optimal policy under constrained circumstances. There have also been specific suggestions of how humans and animals may respond to this problem under particular experimental conditions as well as proposals about the brain mechanisms involved. Here, we provide a brief review of this work, discuss how exploration and exploitation may be mediated in the brain and highlight some promising future directions for research.},
  file = {/Users/daniekru/Zotero/storage/RYJ3REI3/Cohen et al. - 2007 - Should I stay or should I go How the human brain .pdf}
}

@article{dardenneRolePrefrontalCortex2012,
  title = {Role of Prefrontal Cortex and the Midbrain Dopamine System in Working Memory Updating},
  author = {D'Ardenne, Kimberlee and Eshel, Neir and Luka, Joseph and Lenartowicz, Agatha and Nystrom, Leigh E. and Cohen, Jonathan D.},
  year = {2012},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {49},
  pages = {19900--19909},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1116727109},
  urldate = {2024-04-26},
  abstract = {Humans are adept at switching between goal-directed behaviors quickly and effectively. The prefrontal cortex (PFC) is thought to play a critical role by encoding, updating, and maintaining internal representations of task context in working memory. It has also been hypothesized that the encoding of context representations in PFC is regulated by phasic dopamine gating signals. Here we use multimodal methods to test these hypotheses. First we used functional MRI (fMRI) to identify regions of PFC associated with the representation of context in a working memory task. Next we used single-pulse transcranial magnetic stimulation (TMS), guided spatially by our fMRI findings and temporally by previous event-related EEG recordings, to disrupt context encoding while participants performed the same working memory task. We found that TMS pulses to the right dorsolateral PFC (DLPFC) immediately after context presentation, and well in advance of the response, adversely impacted context-dependent relative to context-independent responses. This finding causally implicates right DLPFC function in context encoding. Finally, using the same paradigm, we conducted high-resolution fMRI measurements in brainstem dopaminergic nuclei (ventral tegmental area and substantia nigra) and found phasic responses after presentation of context stimuli relative to other stimuli, consistent with the timing of a gating signal that regulates the encoding of representations in PFC. Furthermore, these responses were positively correlated with behavior, as well as with responses in the same region of right DLPFC targeted in the TMS experiment, lending support to the hypothesis that dopamine phasic signals regulate encoding, and thereby the updating, of context representations in PFC.},
  file = {/Users/daniekru/Zotero/storage/J3WVZM2K/D’Ardenne et al. - 2012 - Role of prefrontal cortex and the midbrain dopamin.pdf}
}

@article{didomenicoDopaminergicModulationPrefrontal2023,
  title = {Dopaminergic {{Modulation}} of {{Prefrontal Cortex Inhibition}}},
  author = {Di Domenico, Danila and Mapelli, Lisa},
  year = {2023},
  month = may,
  journal = {Biomedicines},
  volume = {11},
  number = {5},
  pages = {1276},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-9059},
  doi = {10.3390/biomedicines11051276},
  urldate = {2024-04-26},
  abstract = {The prefrontal cortex is the highest stage of integration in the mammalian brain. Its functions vary greatly, from working memory to decision-making, and are primarily related to higher cognitive functions. This explains the considerable effort devoted to investigating this area, revealing the complex molecular, cellular, and network organization, and the essential role of various regulatory controls. In particular, the dopaminergic modulation and the impact of local interneurons activity are critical for prefrontal cortex functioning, controlling the excitatory/inhibitory balance and the overall network processing. Though often studied separately, the dopaminergic and GABAergic systems are deeply intertwined in influencing prefrontal network processing. This mini review will focus on the dopaminergic modulation of GABAergic inhibition, which plays a significant role in shaping prefrontal cortex activity.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {dopaminergic system,GABAergic system,prefrontal cortex},
  file = {/Users/daniekru/Zotero/storage/6J7N6B8T/Di Domenico and Mapelli - 2023 - Dopaminergic Modulation of Prefrontal Cortex Inhib.pdf}
}

@misc{DopamineConductsPrefrontal,
  title = {Dopamine Conducts Prefrontal Cortex Ensembles},
  journal = {ScienceDaily},
  urldate = {2024-04-26},
  abstract = {New research in rodents reveals for the first time how dopamine changes the function of the brain's prefrontal cortex. Researchers found that dopamine has little effect on individual cells. Instead, it generates sustained activity in the ensemble of cells in the prefrontal cortex that lasts for up to 20 minutes.},
  howpublished = {https://www.sciencedaily.com/releases/2019/04/190402113136.htm},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/84ZNICPF/190402113136.html}
}

@misc{garivierUpperConfidenceBoundPolicies2008,
  title = {On {{Upper-Confidence Bound Policies}} for {{Non-Stationary Bandit Problems}}},
  author = {Garivier, Aur{\'e}lien and Moulines, Eric},
  year = {2008},
  month = may,
  number = {arXiv:0805.3415},
  eprint = {0805.3415},
  primaryclass = {math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.0805.3415},
  urldate = {2024-03-25},
  abstract = {Multi-armed bandit problems are considered as a paradigm of the trade-off between exploring the environment to find profitable actions and exploiting what is already known. In the stationary case, the distributions of the rewards do not change in time, Upper-Confidence Bound (UCB) policies have been shown to be rate optimal. A challenging variant of the MABP is the non-stationary bandit problem where the gambler must decide which arm to play while facing the possibility of a changing environment. In this paper, we consider the situation where the distributions of rewards remain constant over epochs and change at unknown time instants. We analyze two algorithms: the discounted UCB and the sliding-window UCB. We establish for these two algorithms an upper-bound for the expected regret by upper-bounding the expectation of the number of times a suboptimal arm is played. For that purpose, we derive a Hoeffding type inequality for self normalized deviations with a random number of summands. We establish a lower-bound for the regret in presence of abrupt changes in the arms reward distributions. We show that the discounted UCB and the sliding-window UCB both match the lower-bound up to a logarithmic factor.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/daniekru/Zotero/storage/XPLJKI2D/Garivier and Moulines - 2008 - On Upper-Confidence Bound Policies for Non-Station.pdf;/Users/daniekru/Zotero/storage/65CD88JS/0805.html}
}

@article{gittinsBanditProcessesDynamic1979,
  title = {Bandit {{Processes}} and {{Dynamic Allocation Indices}}},
  author = {Gittins, J. C.},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {41},
  number = {2},
  eprint = {2985029},
  eprinttype = {jstor},
  pages = {148--177},
  abstract = {The paperaimsto givea unifiedaccountofthecentracl onceptsin recentworkon banditprocessesand dynamicallocationindices;to showhow thesereducesome previouslyintractablperoblemsto theproblemof calculatingsuchindices;and to describehow thesecalculationsmay be carriedout. Applicationsto stochastic schedulings,equentialclinicaltrialsand a class of searchproblemsare discussed.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FYGDZ4DT/Gittins - 1979 - Bandit Processes and Dynamic Allocation Indices.pdf}
}

@article{gordleevaModelingWorkingMemory2021,
  title = {Modeling {{Working Memory}} in a {{Spiking Neuron Network Accompanied}} by {{Astrocytes}}},
  author = {Gordleeva, Susanna Yu and Tsybina, Yuliya A. and Krivonosov, Mikhail I. and Ivanchenko, Mikhail V. and Zaikin, Alexey A. and Kazantsev, Victor B. and Gorban, Alexander N.},
  year = {2021},
  month = mar,
  journal = {Frontiers in Cellular Neuroscience},
  volume = {15},
  publisher = {Frontiers},
  issn = {1662-5102},
  doi = {10.3389/fncel.2021.631485},
  urldate = {2024-05-03},
  abstract = {We propose a novel biologically plausible computational model of working memory (WM) implemented by the spiking neuron network (SNN) interacting with a network of astrocytes. SNN is modelled by the synaptically coupled Izhikevich neurons with a non-specific architecture connection topology. Astrocytes generating calcium signals are connected by local gap junction diffusive couplings and interact with neurons by chemicals diffused in the extracellular space. Calcium elevations occur in response to the increased concentration of the neurotransmitter released by spiking neurons when a group of them fire coherently. In turn, gliotransmitters are released by activated astrocytes modulating the strengths of synaptic connections in the corresponding neuronal group. Input information is encoded as two-dimensional patterns of short applied current pulses stimulating neurons. The output is taken from frequencies of transient discharges of corresponding neurons. We show how a set of information patterns with quite significant overlapping areas can be uploaded into the neuron-astrocyte network and stored for several seconds. Information retrieval is organised by the application of a cue pattern representing the one from the memory set distorted by noise. We found that successful retrieval with the level of the correlation between the recalled pattern and ideal pattern exceeding 90\% is possible for multi-item WM task. Having analysed the dynamical mechanism of WM formation, we discovered that astrocytes operating at a time scale of a dozen of seconds can successfully store traces of neuronal activations corresponding to information patterns. In the retrieval stage, the astrocytic network selectively modulates synaptic connections in SNN leading to the successful recall. Information and dynamical characteristics of the proposed WM model agrees with classical concepts and other WM models.},
  langid = {english},
  keywords = {astrocyte,delayed activity,neuron-astrocyte interaction,Spiking Neural network,working memory},
  file = {/Users/daniekru/Zotero/storage/CLGHGHEZ/Gordleeva et al. - 2021 - Modeling Working Memory in a Spiking Neuron Networ.pdf}
}

@incollection{gotoPrefrontalCorticalSynaptic2007,
  title = {Prefrontal {{Cortical Synaptic Plasticity}}: {{The Roles}} of {{Dopamine}} and {{Implication}} for {{Schizophrenia}}},
  shorttitle = {Prefrontal {{Cortical Synaptic Plasticity}}},
  booktitle = {Monoaminergic {{Modulation}} of {{Cortical Excitability}}},
  author = {Goto, Yukiori and Otani, Satoru},
  editor = {Tseng, Kuei-Yuan and Atzori, Marco},
  year = {2007},
  pages = {165--174},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-0-387-72256-6_10},
  urldate = {2024-04-26},
  abstract = {The prefrontal cortex (PFC) is central in mediating executive functions in goaldirected behavior, for which proper dopamine (DA) actions of information processing modulation is essential in this area. It is now evident that, as in the case of the hippocampus, the PFC undergoes neuronal adaptation processes in its networks with induction of synaptic plasticity such as long-term potentiation (LTP) and short-term potentiation (STP). A prominent characteristic of synaptic plasticity in the PFC is that its induction mechanisms involve DA as an essential modulatory molecule. As such, DA-dependent plastic changes occurring in PFC network have important roles for PFC-mediated cognitive functions. Nevertheless, little attempt has been made to characterize the nature of PFC neuronal adaptation by synaptic plasticity, given that the PFC is thought to be the area of temporary storage and manipulation of information, known as working memory. However, accumulating evidences now indicate that the functions of the PFC cannot be fully explained just as the region of an online representation and handling of information. Importance of DA-dependent synaptic plasticity is further encouraged by possible disruption of synaptic plasticity mechanism in the PFC in psychiatric disorders such as schizophrenia, drug addiction, and depression.},
  isbn = {978-0-387-72256-6},
  langid = {english},
  keywords = {Prefrontal Cortex,Synaptic Plasticity,Tetanic Stimulation,Trace Fear Conditioning,Ventral Tegmental Area},
  file = {/Users/daniekru/Zotero/storage/5PNVFBP5/Goto and Otani - 2007 - Prefrontal Cortical Synaptic Plasticity The Roles.pdf}
}

@article{harleAlteredStatisticalLearning2015,
  title = {Altered {{Statistical Learning}} and {{Decision-Making}} in {{Methamphetamine Dependence}}: {{Evidence}} from a {{Two-Armed Bandit Task}}},
  shorttitle = {Altered {{Statistical Learning}} and {{Decision-Making}} in {{Methamphetamine Dependence}}},
  author = {Harl{\'e}, Katia M. and Zhang, Shunan and Schiff, Max and Mackey, Scott and Paulus, Martin P. and Yu, Angela J.},
  year = {2015},
  month = dec,
  journal = {Frontiers in Psychology},
  volume = {6},
  pages = {1910},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.01910},
  urldate = {2024-04-29},
  abstract = {Understanding how humans weigh long-term and short-term goals is important for both basic cognitive science and clinical neuroscience, as substance users need to balance the appeal of an immediate high vs. the long-term goal of sobriety. We use a computational model to identify learning and decision-making abnormalities in methamphetamine-dependent individuals (MDI, n = 16) vs. healthy control subjects (HCS, n = 16), in a two-armed bandit task. In this task, subjects repeatedly choose between two arms with fixed but unknown reward rates. Each choice not only yields potential immediate reward but also information useful for long-term reward accumulation, thus pitting exploration against exploitation. We formalize the task as comprising a learning component, the updating of estimated reward rates based on ongoing observations, and a decision-making component, the choice among options based on current beliefs and uncertainties about reward rates. We model the learning component as iterative Bayesian inference (the Dynamic Belief Model), and the decision component using five competing decision policies: Win-stay/Lose-shift (WSLS), {$\varepsilon$}-Greedy, {$\tau$}-Switch, Softmax, Knowledge Gradient. HCS and MDI significantly differ in how they learn about reward rates and use them to make decisions. HCS learn from past observations but weigh recent data more, and their decision policy is best fit as Softmax. MDI are more likely to follow the simple learning-independent policy of WSLS, and among MDI best fit by Softmax, they have more pessimistic prior beliefs about reward rates and are less likely to choose the option estimated to be most rewarding. Neurally, MDI's tendency to avoid the most rewarding option is associated with a lower gray matter volume of the thalamic dorsal lateral nucleus. More broadly, our work illustrates the ability of our computational framework to help reveal subtle learning and decision-making abnormalities in substance use.},
  pmcid = {PMC4683191},
  pmid = {26733906},
  file = {/Users/daniekru/Zotero/storage/WH6J7IYU/Harlé et al. - 2015 - Altered Statistical Learning and Decision-Making i.pdf}
}

@article{jayPlasticityHippocampalPrefrontal2004,
  title = {Plasticity at Hippocampal to Prefrontal Cortex Synapses Is Impaired by Loss of Dopamine and Stress: Importance for Psychiatric Diseases},
  shorttitle = {Plasticity at Hippocampal to Prefrontal Cortex Synapses Is Impaired by Loss of Dopamine and Stress},
  author = {Jay, Th{\'e}r{\`e}se M. and Rocher, Cyril and Hotte, Ma{\"i}te and Naudon, Laurent and Gurden, Hirac and Spedding, Michael},
  year = {2004},
  journal = {Neurotoxicity Research},
  volume = {6},
  number = {3},
  pages = {233--244},
  issn = {1029-8428},
  doi = {10.1007/BF03033225},
  abstract = {The direct hippocampal to prefrontal cortex pathway and its changes in synaptic plasticity is a useful framework for investigating the functional operations of hippocampal-prefrontal cortex communication in cognitive functions. Synapses on this pathway are modifiable and synaptic strength can be turned up or down depending on specific patterns of activity in the pathway. The objective of this review will be to summarize the different studies carried out on this topic including very recent data and to underline the importance of animal models for the development of new and effective medications in psychiatric diseases. We have shown that long-term potentiation (LTP) of hippocampal-prefrontal synapses is driven by the level of mesocortical dopaminergic (DA) activity and more recently that stress is also an environmental determinant of LTP at these cortical synapses. Stimulation of the ventral tegmental area at a frequency known to evoke DA overflow in the prefrontal cortex produces a long-lasting enhancement of the magnitude of hippocampal-prefrontal cortex LTP whereas a depletion of cortical DA levels generates a dramatic decrease in this LTP. Moreover, hippocampal stimulation induces a transient but significant increase in DA release in the prefrontal cortex and an optimal level of D1 receptor activation is essential for LTP expression. We recently investigated the impact of stress on hippocampal-prefrontal LTP and demonstrated that exposure to an acute stress causes a remarkable and long-lasting inhibition of LTP. Furthermore, we demonstrated that tianeptine, an antidepressant which has a unique mode of action, and clozapine an atypical antipsychotic when administered at doses normally used in human testing are able to reverse the impairment in LTP. Stressful life events have a substantial causal association with psychiatric disorders like schizophrenia and depression and recent imaging studies have shown an important role of the limbic-cortical circuit in the pathophysiology of these illnesses. Therefore, we proposed that agents capable of reversing the impairment of plasticity at hippocampal to prefrontal cortex synapses have the potential of becoming new therapeutic classes of antidepressant or antipsychotic drugs.},
  langid = {english},
  pmid = {15325962},
  keywords = {Animals,Dopamine,Hippocampus,Humans,Neuronal Plasticity,Prefrontal Cortex,Psychotropic Drugs,Receptors Dopamine D1,Stress Physiological,Synapses,Synaptic Transmission}
}

@misc{kannanUnsupervisedSpikingNeural2023,
  title = {Unsupervised {{Spiking Neural Network Model}} of {{Prefrontal Cortex}} to Study {{Task Switching}} with {{Synaptic}} Deficiency},
  author = {Kannan, Ashwin Viswanathan and Mylavarapu, Goutam and Thomas, Johnson P.},
  year = {2023},
  month = may,
  number = {arXiv:2305.14394},
  eprint = {2305.14394},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14394},
  urldate = {2024-05-03},
  abstract = {In this study, we build a computational model of Prefrontal Cortex (PFC) using Spiking Neural Networks (SNN) to understand how neurons adapt and respond to tasks switched under short and longer duration of stimulus changes. We also explore behavioral deficits arising out of the PFC lesions by simulating lesioned states in our Spiking architecture model. Although there are some computational models of the PFC, SNN's have not been used to model them. In this study, we use SNN's having parameters close to biologically plausible values and train the model using unsupervised Spike Timing Dependent Plasticity (STDP) learning rule. Our model is based on connectionist architectures and exhibits neural phenomena like sustained activity which helps in generating short-term or working memory. We use these features to simulate lesions by deactivating synaptic pathways and record the weight adjustments of learned patterns and capture the accuracy of learning tasks in such conditions. All our experiments are trained and recorded using a real-world Fashion MNIST (FMNIST) dataset and through this work, we bridge the gap between bio-realistic models and those that perform well in pattern recognition tasks},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/daniekru/Zotero/storage/DJTHQ7S7/Kannan et al. - 2023 - Unsupervised Spiking Neural Network Model of Prefr.pdf;/Users/daniekru/Zotero/storage/MH6K7SL2/2305.html}
}

@misc{kaufmannThompsonSamplingAsymptotically2012,
  title = {Thompson {{Sampling}}: {{An Asymptotically Optimal Finite Time Analysis}}},
  shorttitle = {Thompson {{Sampling}}},
  author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'e}mi},
  year = {2012},
  month = jul,
  number = {arXiv:1205.4217},
  eprint = {1205.4217},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-23},
  abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/ZL83R67T/Kaufmann et al. - 2012 - Thompson Sampling An Asymptotically Optimal Finit.pdf}
}

@article{laskowskiRoleMedialPrefrontal2016,
  title = {The Role of the Medial Prefrontal Cortex in Updating Reward Value and Avoiding Perseveration},
  author = {Laskowski, C. S. and Williams, R. J. and Martens, K. M. and Gruber, A. J. and Fisher, K. G. and Euston, D. R.},
  year = {2016},
  month = jun,
  journal = {Behavioural Brain Research},
  volume = {306},
  pages = {52--63},
  issn = {0166-4328},
  doi = {10.1016/j.bbr.2016.03.007},
  urldate = {2024-04-29},
  abstract = {The medial prefrontal cortex (mPFC) plays a major role in goal-directed behaviours, but it is unclear whether it plays a role in breaking away from a high-value reward in order to explore for better options. To address this question, we designed a novel 3-arm Bandit Task in which rats were required to choose one of three potential reward arms, each of which was associated with a different amount of food reward and time-out punishment. After a variable number of choice trials the reward locations were shuffled and animals had to disengage from the now devalued arm and explore the other options in order to optimise payout. Lesion and control groups' behaviours on the task were then analysed by fitting data with a reinforcement learning model. As expected, lesioned animals obtained less reward overall due to an inability to flexibly adapt their behaviours after a change in reward location. However, modelling results showed that lesioned animals were no more likely to explore than control animals. We also discovered that all animals showed a strong preference for certain maze arms, at the expense of reward. This tendency was exacerbated in the lesioned animals, with the strongest effects seen in a subset of animals with damage to dorsal mPFC. The results confirm a role for mPFC in goal-directed behaviours but suggest that rats rely on other areas to resolve the explore-exploit dilemma.},
  keywords = {Decision making,Exploration,Prefrontal cortex,Reinforcement learning,Reward,Value},
  file = {/Users/daniekru/Zotero/storage/X52KZDDB/S0166432816301322.html}
}

@article{madadiaslDopaminergicModulationSynaptic2019,
  title = {Dopaminergic {{Modulation}} of {{Synaptic Plasticity}}, {{Its Role}} in {{Neuropsychiatric Disorders}}, and {{Its Computational Modeling}}},
  author = {Madadi Asl, Mojtaba and Vahabie, Abdol-Hossein and Valizadeh, Alireza},
  year = {2019},
  journal = {Basic and Clinical Neuroscience},
  volume = {10},
  number = {1},
  pages = {1--12},
  issn = {2008-126X},
  doi = {10.32598/bcn.9.10.125},
  urldate = {2024-04-26},
  abstract = {Neuromodulators modify intrinsic characteristics of the nervous system in order to reconfigure the functional properties of neural circuits. This reconfiguration is crucial for the flexibility of the nervous system to respond on an input-modulated basis. Such a functional rearrangement is realized by modification of intrinsic properties of the neural circuits including synaptic interactions. Dopamine is an important neuromodulator involved in motivation and stimulus-reward learning process, and adjusts synaptic dynamics in multiple time scales through different pathways. The modification of synaptic plasticity by dopamine underlies the change in synaptic transmission and integration mechanisms, which affects intrinsic properties of the neural system including membrane excitability, probability of neurotransmitters release, receptors' response to neurotransmitters, protein trafficking, and gene transcription. Dopamine also plays a central role in behavioral control, whereas its malfunction can cause cognitive disorders. Impaired dopamine signaling is implicated in several neuropsychiatric disorders such as Parkinson's disease, drug addiction, schizophrenia, attention-deficit/hyperactivity disorder, obsessive-compulsive disorder and Tourette's syndrome. Therefore, dopamine plays a crucial role in the nervous system, where its proper modulation of neural circuits may enhance plasticity-related procedures, but disturbances in dopamine signaling might be involved in numerous neuropsychiatric disorders. In recent years, several computational models are proposed to formulate the involvement of dopamine in synaptic plasticity or neuropsychiatric disorders and address their connection based on the experimental findings.},
  pmcid = {PMC6484184},
  pmid = {31031889},
  file = {/Users/daniekru/Zotero/storage/S4YBA7FY/Madadi Asl et al. - 2019 - Dopaminergic Modulation of Synaptic Plasticity, It.pdf}
}

@article{montagueFrameworkMesencephalicDopamine1996,
  title = {A Framework for Mesencephalic Dopamine Systems Based on Predictive {{Hebbian}} Learning},
  author = {Montague, P. R. and Dayan, P. and Sejnowski, T. J.},
  year = {1996},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {16},
  number = {5},
  pages = {1936--1947},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.16-05-01936.1996},
  urldate = {2024-04-26},
  abstract = {We develop a theoretical framework that shows how mesencephalic dopamine systems could distribute to their targets a signal that represents information about future expectations. In particular, we show how activity in the cerebral cortex can make predictions about future receipt of reward and how fluctuations in the activity levels of neurons in diffuse dopamine systems above and below baseline levels would represent errors in these predictions that are delivered to cortical and subcortical targets. We present a model for how such errors could be constructed in a real brain that is consistent with physiological results for a subset of dopaminergic neurons located in the ventral tegmental area and surrounding dopaminergic neurons. The theory also makes testable predictions about human choice behavior on a simple decision-making task. Furthermore, we show that, through a simple influence on synaptic plasticity, fluctuations in dopamine release can act to change the predictions in an appropriate manner.},
  chapter = {Articles},
  copyright = {{\copyright} 1996 by Society for Neuroscience},
  langid = {english},
  pmid = {8774460},
  file = {/Users/daniekru/Zotero/storage/P9HGS9Z3/Montague et al. - 1996 - A framework for mesencephalic dopamine systems bas.pdf}
}

@article{ortnerNeuromorphicHardwareLearns2019,
  title = {Neuromorphic {{Hardware Learns}} to {{Learn}}},
  author = {Ortner, Thomas and Scherr, Franz and Pehle, Christian and Meier, Kyle and Maass, Wolfgang},
  year = {2019},
  month = may,
  journal = {Frontiers in Neuroscience},
  volume = {13},
  doi = {10.3389/fnins.2019.00483},
  abstract = {Hyperparameters and learning algorithms for neuromorphic hardware are usually chosen by hand to suit a particular task. In contrast, networks of neurons in the brain were optimized through extensive evolutionary and developmental processes to work well on a range of computing and learning tasks. Occasionally this process has been emulated through genetic algorithms, but these require themselves hand-design of their details and tend to provide a limited range of improvements. We employ instead other powerful gradient-free optimization tools, such as cross-entropy methods and evolutionary strategies, in order to port the function of biological optimization processes to neuromorphic hardware. As an example, we show these optimization algorithms enable neuromorphic agents to learn very efficiently from rewards. In particular, meta-plasticity, i.e., the optimization of the learning rule which they use, substantially enhances reward-based learning capability of the hardware. In addition, we demonstrate for the first time Learning-to-Learn benefits from such hardware, in particular, the capability to extract abstract knowledge from prior learning experiences that speeds up the learning of new but related tasks. Learning-to-Learn is especially suited for accelerated neuromorphic hardware, since it makes it feasible to carry out the required very large number of network computations.},
  file = {/Users/daniekru/Zotero/storage/BRUCLEYK/Ortner et al. - 2019 - Neuromorphic Hardware Learns to Learn.pdf}
}

@article{pilarskiOptimalPolicyBernoulli2021,
  title = {Optimal {{Policy}} for {{Bernoulli Bandits}}: {{Computation}} and {{Algorithm Gauge}}},
  shorttitle = {Optimal {{Policy}} for {{Bernoulli Bandits}}},
  author = {Pilarski, Sebastian and Pilarski, Slawomir and Varro, Daniel},
  year = {2021},
  month = feb,
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {2},
  number = {1},
  pages = {2--17},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3074122},
  urldate = {2024-03-25},
  abstract = {Bernoulli multi-armed bandits are a reinforcement learning model used to study a variety of choice optimization problems. Often such optimizations concern a finite-time horizon. In principle, statistically optimal policies can be computed via dynamic programming, but doing so is considered infeasible due to prohibitive computational requirements and implementation complexity. Hence, suboptimal algorithms are applied in practice, despite their unknown level of suboptimality. In this article, we demonstrate that optimal policies can be efficiently computed for large time horizons or number of arms thanks to a novel memory organization and indexing scheme. We use optimal policies to gauge the suboptimality of several well-known finite- and infinitetime horizon algorithms including Whittle and Gittins indices, epsilon-greedy, Thompson sampling, and upper-confidence bound (UCB) algorithms. Our simulation study shows that all but one evaluated algorithm perform significantly worse than the optimal policy. The Whittle index offers a nearly optimal strategy for multiarmed Bernoulli bandits despite its suboptimal decisions---up to 10\%---compared to an optimal policy table. Lastly, we discuss optimizations of known algorithms. We derive a novel solution from UCB1-tuned. It outperforms other infinite-time horizon algorithms when dealing with many arms.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FRRR75ZZ/Pilarski et al. - 2021 - Optimal Policy for Bernoulli Bandits Computation .pdf}
}

@article{puigDopamineModulationLearning2014,
  title = {Dopamine Modulation of Learning and Memory in the Prefrontal Cortex: Insights from Studies in Primates, Rodents, and Birds},
  shorttitle = {Dopamine Modulation of Learning and Memory in the Prefrontal Cortex},
  author = {Puig, M. Victoria and Rose, Jonas and Schmidt, Robert and Freund, Nadja},
  year = {2014},
  month = aug,
  journal = {Frontiers in Neural Circuits},
  volume = {8},
  publisher = {Frontiers},
  issn = {1662-5110},
  doi = {10.3389/fncir.2014.00093},
  urldate = {2024-04-26},
  abstract = {In this review, we provide a brief overview over the current knowledge about the role of dopamine transmission in the prefrontal cortex during learning and memory. We discuss work in humans, monkeys, rats, and birds in order to provide a basis for comparison across species that might help identify crucial features and constraints of the dopaminergic system in executive function. Computational models of dopamine function are introduced to provide a framework for such a comparison. We also provide a brief evolutionary perspective showing that the dopaminergic system is highly preserved across mammals. Even birds, following a largely independent evolution of higher cognitive abilities, have evolved a comparable dopaminergic system. Finally, we discuss the unique advantages and challenges of using different animal models for advancing our understanding of dopamine function in the healthy and diseased brain.},
  langid = {english},
  keywords = {dopamine receptors,evolution,Executive Function,Learning and Memory (Neurosciences),Neuromodulation,prefrontal cortex (PFC),working memory},
  file = {/Users/daniekru/Zotero/storage/T58MVUWQ/Puig et al. - 2014 - Dopamine modulation of learning and memory in the .pdf}
}

@article{roeschDopamineNeuronsEncode2007,
  title = {Dopamine Neurons Encode the Better Option in Rats Deciding between Differently Delayed or Sized Rewards},
  author = {Roesch, Matthew R. and Calu, Donna J. and Schoenbaum, Geoffrey},
  year = {2007},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {12},
  pages = {1615--1624},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn2013},
  urldate = {2024-04-26},
  abstract = {The dopamine system is thought to be involved in making decisions about reward. Here we recorded from the ventral tegmental area in rats learning to choose between differently delayed and sized rewards. As expected, the activity of many putative dopamine neurons reflected reward prediction errors, changing when the value of the reward increased or decreased unexpectedly. During learning, neural responses to reward in these neurons waned and responses to cues that predicted reward emerged. Notably, this cue-evoked activity varied with size and delay. Moreover, when rats were given a choice between two differently valued outcomes, the activity of the neurons initially reflected the more valuable option, even when it was not subsequently selected.},
  copyright = {2007 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/82Y9XINE/Roesch et al. - 2007 - Dopamine neurons encode the better option in rats .pdf}
}

@article{rouaultPrefrontalMechanismsCombining2019,
  title = {Prefrontal Mechanisms Combining Rewards and Beliefs in Human Decision-Making},
  author = {Rouault, Marion and Drugowitsch, Jan and Koechlin, Etienne},
  year = {2019},
  month = jan,
  journal = {Nature Communications},
  volume = {10},
  pages = {301},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08121-w},
  urldate = {2024-04-29},
  abstract = {In uncertain and changing environments, optimal decision-making requires integrating reward expectations with probabilistic beliefs about reward contingencies. Little is known, however, about how the prefrontal cortex (PFC), which subserves decision-making, combines these quantities. Here, using computational modelling and neuroimaging, we show that the ventromedial PFC encodes both reward expectations and proper beliefs about reward contingencies, while the dorsomedial PFC combines these quantities and guides choices that are at variance with those predicted by optimal decision theory: instead of integrating reward expectations with beliefs, the dorsomedial PFC built context-dependent reward expectations commensurable to beliefs and used these quantities as two concurrent appetitive components, driving choices. This neural mechanism accounts for well-known risk aversion effects in human decision-making. The results reveal that the irrationality of human choices commonly theorized as deriving from optimal computations over false beliefs, actually stems from suboptimal neural heuristics over rational beliefs about reward contingencies., Optimal decision-making requires integrating expectations about rewards with beliefs about reward contingencies. Here, the authors show that these aspects of reward are encoded in the ventromedial prefrontal cortex then combined in the dorsomedial prefrontal cortex, a process that guides choice biases characteristic of human decision-making.},
  pmcid = {PMC6336816},
  pmid = {30655534},
  file = {/Users/daniekru/Zotero/storage/N4FCVGQW/Rouault et al. - 2019 - Prefrontal mechanisms combining rewards and belief.pdf}
}

@article{schultzNeuralSubstratePrediction1997,
  title = {A {{Neural Substrate}} of {{Prediction}} and {{Reward}}},
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
  year = {1997},
  month = mar,
  journal = {Science},
  volume = {275},
  number = {5306},
  pages = {1593--1599},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.275.5306.1593},
  urldate = {2024-04-26},
  abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
  file = {/Users/daniekru/Zotero/storage/M99A4WN2/Schultz et al. - 1997 - A Neural Substrate of Prediction and Reward.pdf}
}

@article{sheynikhovichLongtermMemorySynaptic2023,
  title = {Long-Term Memory, Synaptic Plasticity and Dopamine in Rodent Medial Prefrontal Cortex: {{Role}} in Executive Functions},
  shorttitle = {Long-Term Memory, Synaptic Plasticity and Dopamine in Rodent Medial Prefrontal Cortex},
  author = {Sheynikhovich, Denis and Otani, Satoru and Bai, Jing and Arleo, Angelo},
  year = {2023},
  month = jan,
  journal = {Frontiers in Behavioral Neuroscience},
  volume = {16},
  publisher = {Frontiers},
  issn = {1662-5153},
  doi = {10.3389/fnbeh.2022.1068271},
  urldate = {2024-04-26},
  abstract = {Mnemonic functions, supporting rodent behavior in complex tasks, include both long-term and (short-term) working memory components. While working memory is thought to rely on persistent activity states in an active neural network, long-term memory and synaptic plasticity contribute to the formation of the underlying synaptic structure, determining the range of possible states. Whereas the implication of working memory in executive functions, mediated by the prefrontal cortex (PFC) in primates and rodents, has been extensively studied, the contribution of long-term memory component to these tasks received little attention. This review summarizes available experimental data and theoretical work concerning cellular mechanisms of synaptic plasticity in the medial region of rodent PFC and the link between plasticity, memory and behavior in PFC-dependent tasks. A special attention is devoted to unique properties of dopaminergic modulation of prefrontal synaptic plasticity and its contribution to executive functions.},
  langid = {english},
  keywords = {behavioral flexibility,Computational models,Dopamine,executive functions,Long-term memory,Neuromodulation,Prefrontal Cortex,synaptic plasticity},
  file = {/Users/daniekru/Zotero/storage/M7NJNB4Y/Sheynikhovich et al. - 2023 - Long-term memory, synaptic plasticity and dopamine.pdf}
}

@article{steyversBayesianAnalysisHuman2009,
  title = {A {{Bayesian}} Analysis of Human Decision-Making on Bandit Problems},
  author = {Steyvers, Mark and Lee, Michael D. and Wagenmakers, Eric-Jan},
  year = {2009},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  series = {Special {{Issue}}: {{Dynamic Decision Making}}},
  volume = {53},
  number = {3},
  pages = {168--179},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2008.11.002},
  urldate = {2024-05-02},
  abstract = {The bandit problem is a dynamic decision-making task that is simply described, well-suited to controlled laboratory study, and representative of a broad class of real-world problems. In bandit problems, people must choose between a set of alternatives, each with different unknown reward rates, to maximize the total reward they receive over a fixed number of trials. A key feature of the task is that it challenges people to balance the exploration of unfamiliar choices with the exploitation of familiar ones. We use a Bayesian model of optimal decision-making on the task, in which how people balance exploration with exploitation depends on their assumptions about the distribution of reward rates. We also use Bayesian model selection measures that assess how well people adhere to an optimal decision process, compared to simpler heuristic decision strategies. Using these models, we make inferences about the decision-making of 451 participants who completed a set of bandit problems, and relate various measures of their performance to other psychological variables, including psychometric assessments of cognitive abilities and personality traits. We find clear evidence of individual differences in the way the participants made decisions on the bandit problems, and some interesting correlations with measures of general intelligence.},
  keywords = {Bandit problem,Bayesian modeling,Decision-making,Exploration versus exploitation,Individual differences},
  file = {/Users/daniekru/Zotero/storage/U38WE347/S0022249608001090.html}
}

@article{toblerAdaptiveCodingReward2005,
  title = {Adaptive {{Coding}} of {{Reward Value}} by {{Dopamine Neurons}}},
  author = {Tobler, Philippe N. and Fiorillo, Christopher D. and Schultz, Wolfram},
  year = {2005},
  month = mar,
  journal = {Science},
  volume = {307},
  number = {5715},
  pages = {1642--1645},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1105370},
  urldate = {2024-05-02},
  abstract = {It is important for animals to estimate the value of rewards as accurately as possible. Because the number of potential reward values is very large, it is necessary that the brain's limited resources be allocated so as to discriminate better among more likely reward outcomes at the expense of less likely outcomes. We found that midbrain dopamine neurons rapidly adapted to the information provided by reward-predicting stimuli. Responses shifted relative to the expected reward value, and the gain adjusted to the variance of reward value. In this way, dopamine neurons maintained their reward sensitivity over a large range of reward values.},
  file = {/Users/daniekru/Zotero/storage/CBJ4X3LU/Tobler et al. - 2005 - Adaptive Coding of Reward Value by Dopamine Neuron.pdf}
}

@incollection{tokicAdaptiveEGreedyExploration2010,
  title = {Adaptive {$\varepsilon$}-{{Greedy Exploration}} in {{Reinforcement Learning Based}} on {{Value Differences}}},
  booktitle = {{{KI}} 2010: {{Advances}} in {{Artificial Intelligence}}},
  author = {Tokic, Michel},
  editor = {Dillmann, R{\"u}diger and Beyerer, J{\"u}rgen and Hanebeck, Uwe D. and Schultz, Tanja},
  year = {2010},
  volume = {6359},
  pages = {203--210},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-16111-7_23},
  urldate = {2024-03-23},
  abstract = {This paper presents ``Value-Difference Based Exploration'' (VDBE), a method for balancing the exploration/exploitation dilemma inherent to reinforcement learning. The proposed method adapts the exploration parameter of {$\varepsilon$}-greedy in dependence of the temporal-difference error observed from value-function backups, which is considered as a measure of the agent's uncertainty about the environment. VDBE is evaluated on a multi-armed bandit task, which allows for insight into the behavior of the method. Preliminary results indicate that VDBE seems to be more parameter robust than commonly used ad hoc approaches such as {$\varepsilon$}-greedy or softmax.},
  isbn = {978-3-642-16110-0 978-3-642-16111-7},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/CR7BH65S/Tokic - 2010 - Adaptive ε-Greedy Exploration in Reinforcement Lea.pdf}
}

@incollection{tokicValueDifferenceBasedExploration2011,
  title = {Value-{{Difference Based Exploration}}: {{Adaptive Control}} between {{Epsilon-Greedy}} and {{Softmax}}},
  shorttitle = {Value-{{Difference Based Exploration}}},
  booktitle = {{{KI}} 2011: {{Advances}} in {{Artificial Intelligence}}},
  author = {Tokic, Michel and Palm, G{\"u}nther},
  editor = {Bach, Joscha and Edelkamp, Stefan},
  year = {2011},
  volume = {7006},
  pages = {335--346},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-24455-1_33},
  urldate = {2024-03-23},
  abstract = {This paper proposes ``Value-Difference Based Exploration combined with Softmax action selection'' (VDBE-Softmax) as an adaptive exploration/exploitation policy for temporal-difference learning. The advantage of the proposed approach is that exploration actions are only selected in situations when the knowledge about the environment is uncertain, which is indicated by fluctuating values during learning. The method is evaluated in experiments having deterministic rewards and a mixture of both deterministic and stochastic rewards. The results show that a VDBE-Softmax policy can outperform {$\varepsilon$}-greedy, Softmax and VDBE policies in combination with on- and off-policy learning algorithms such as Q-learning and Sarsa. Furthermore, it is also shown that VDBE-Softmax is more reliable in case of value-function oscillations.},
  isbn = {978-3-642-24454-4 978-3-642-24455-1},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZM5YNNWB/Tokic and Palm - 2011 - Value-Difference Based Exploration Adaptive Contr.pdf}
}

@article{zhangCheapCleverHuman,
  title = {Cheap but {{Clever}}: {{Human Active Learning}} in a {{Bandit Setting}}},
  author = {Zhang, Shunan and Yu, Angela J},
  abstract = {How people achieve long-term goals in an imperfectly known environment, via repeated tries and noisy outcomes, is an important problem in cognitive science. There are two interrelated questions: how humans represent information, both what has been learned and what can still be learned, and how they choose actions, in particular how they negotiate the tension between exploration and exploitation. In this work, we examine human behavioral data in a multi-armed bandit setting, in which the subject choose one of four ``arms'' to pull on each trial and receives a binary outcome (win/lose). We implement both the Bayes-optimal policy, which maximizes the expected cumulative reward in this finite-horizon bandit environment, as well as a variety of heuristic policies that vary in their complexity of information representation and decision policy. We find that the knowledge gradient algorithm, which combines exact Bayesian learning with a decision policy that maximizes a combination of immediate reward gain and longterm knowledge gain, captures subjects' trial-by-trial choice best among all the models considered; it also provides the best approximation to the computationally intense optimal policy among all the heuristic policies.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/I5NDI744/Zhang and Yu - Cheap but Clever Human Active Learning in a Bandi.pdf}
}

@inproceedings{zhangForgetfulBayesMyopic2013,
  title = {Forgetful {{Bayes}} and Myopic Planning: {{Human}} Learning and Decision-Making in a Bandit Setting},
  shorttitle = {Forgetful {{Bayes}} and Myopic Planning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Shunan and Yu, Angela J},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-02},
  abstract = {How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects' choices, on a trial-to-trial basis, are best captured by a forgetful" Bayesian iterative learning model in combination with a partially myopic decision policy known as Knowledge Gradient. This model accounts for subjects' trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, epsilon-greedy and win-stay-lose-shift. It has the added benefit of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are significantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment."},
  file = {/Users/daniekru/Zotero/storage/V336BASA/Zhang and Yu - 2013 - Forgetful Bayes and myopic planning Human learnin.pdf}
}

@article{zhaoBrainInspiredDecisionMakingSpiking2018,
  title = {A {{Brain-Inspired Decision-Making Spiking Neural Network}} and {{Its Application}} in {{Unmanned Aerial Vehicle}}},
  author = {Zhao, Feifei and Zeng, Yi and Xu, Bo},
  year = {2018},
  month = sep,
  journal = {Frontiers in Neurorobotics},
  volume = {12},
  publisher = {Frontiers},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2018.00056},
  urldate = {2024-05-03},
  abstract = {Decision-making is a crucial cognitive function for various animal species surviving in nature, and it is also a fundamental ability for intelligent agents. To make a step forward in the understanding of the computational mechanism of human-like decision-making, this paper proposes a brain-inspired decision-making spiking neural network (BDM-SNN) and applies it to decision-making tasks on intelligent agents. This paper makes the following contributions: (1) A spiking neural network (SNN) is used to model human decision-making neural circuit from both connectome and functional perspectives. (2) The proposed model combines dopamine and spike-timing-dependent plasticity (STDP) mechanisms to modulate the network learning process, which indicates more biological inspiration. (3) The model considers the effects of interactions among sub-areas in PFC on accelerating the learning process. (4) The proposed model can be easily applied to decision-making tasks in intelligent agents, such as an unmanned aerial vehicle (UAV) flying through a window and a UAV avoiding an obstacle. The experimental results support the effectiveness of the model. Compared with traditional reinforcement learning and existing biologically inspired methods, our method contains more biologically-inspired mechanistic principles, has greater accuracy and is faster.},
  langid = {english},
  keywords = {brain-inspired decision-making,Dopamine Regulation,multiple brain areas coordination,reinforcement learning model,Spiking Neural network,UAV autonomous learning},
  file = {/Users/daniekru/Zotero/storage/NYWWHQ9Q/Zhao et al. - 2018 - A Brain-Inspired Decision-Making Spiking Neural Ne.pdf}
}

@article{miller2001integrative,
  title={An integrative theory of prefrontal cortex function},
  author={Miller, Earl K and Cohen, Jonathan D},
  journal={Annual review of neuroscience},
  volume={24},
  number={1},
  pages={167--202},
  year={2001},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@article{o2001abstract,
  title={Abstract reward and punishment representations in the human orbitofrontal cortex},
  author={O'Doherty, John and Kringelbach, Morten L and Rolls, Edmund T and Hornak, Julia and Andrews, Caroline},
  journal={Nature neuroscience},
  volume={4},
  number={1},
  pages={95--102},
  year={2001},
  publisher={Nature Publishing Group}
}

@article{riceberg2012reward,
  title={Reward stability determines the contribution of orbitofrontal cortex to adaptive behavior},
  author={Riceberg, Justin S and Shapiro, Matthew L},
  journal={Journal of Neuroscience},
  volume={32},
  number={46},
  pages={16402--16409},
  year={2012},
  publisher={Soc Neuroscience}
}

@article{gurney2015new,
  title={A new framework for cortico-striatal plasticity: behavioural theory meets in vitro data at the reinforcement-action interface},
  author={Gurney, Kevin N and Humphries, Mark D and Redgrave, Peter},
  journal={PLoS biology},
  volume={13},
  number={1},
  pages={e1002034},
  year={2015},
  publisher={Public Library of Science San Francisco, USA}
}
