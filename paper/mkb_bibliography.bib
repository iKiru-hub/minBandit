@article{abrahamMetaplasticityPlasticitySynaptic1996,
  title = {Metaplasticity: The Plasticity of Synaptic Plasticity},
  shorttitle = {Metaplasticity},
  author = {Abraham, W. C. and Bear, M. F.},
  year = {1996},
  month = apr,
  journal = {Trends in Neurosciences},
  volume = {19},
  number = {4},
  pages = {126--130},
  issn = {0166-2236},
  doi = {10.1016/s0166-2236(96)80018-x},
  abstract = {In this paper, we review experimental evidence for a novel form of persistent synaptic plasticity we call metaplasticity. Metaplasticity is induced by synaptic or cellular activity, but it is not necessarily expressed as a change in the efficacy of normal synaptic transmission. Instead, it is manifest as a change in the ability to induce subsequent synaptic plasticity, such as long-term potentiation or depression. Thus, metaplasticity is a higher-order form of synaptic plasticity. Metaplasticity might involve alterations in NMDA-receptor function in some cases, but there are many other candidate mechanisms. The induction of metaplasticity complicates the interpretation of many commonly studied aspects of synaptic plasticity, such as saturation and biochemical correlates.},
  langid = {english},
  pmid = {8658594},
  keywords = {Animals,Humans,Neuronal Plasticity,Synaptic Transmission}
}

@inproceedings{agrawalAnalysisThompsonSampling2012,
  title = {Analysis of {{Thompson Sampling}} for the {{Multi-armed Bandit Problem}}},
  booktitle = {Proceedings of the 25th {{Annual Conference}} on {{Learning Theory}}},
  author = {Agrawal, Shipra and Goyal, Navin},
  year = {2012},
  month = jun,
  pages = {39.1-39.26},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2024-03-25},
  abstract = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the stochastic multi-armed bandit problem. More precisely, for the stochastic two-armed bandit problem, the expected regret in time T is O({\textbackslash}frac{\textbackslash}ln T∆ + {\textbackslash}frac1∆{\textasciicircum}3). And, for the stochastic N-armed bandit problem, the expected regret in time T is O({\textbackslash}left[{\textbackslash}left({\textbackslash}sum\_i=2{\textasciicircum}N {\textbackslash}frac1{\textbackslash}Delta\_i{\textasciicircum}2{\textbackslash}right){\textasciicircum}2{\textbackslash}right] {\textbackslash}ln T). Our bounds are optimal but for the dependence on {\textbackslash}Delta\_i and the constant factors in big-Oh.},
  langid = {english},
  keywords = {to study},
  file = {/Users/daniekru/Zotero/storage/FIASC57Q/Agrawal and Goyal - 2012 - Analysis of Thompson Sampling for the Multi-armed .pdf}
}

@article{apicellaSurveyModernTrainable2021,
  title = {A Survey on Modern Trainable Activation Functions},
  author = {Apicella, Andrea and Donnarumma, Francesco and Isgr{\`o}, Francesco and Prevete, Roberto},
  year = {2021},
  month = jun,
  journal = {Neural Networks},
  volume = {138},
  pages = {14--32},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.01.026},
  urldate = {2024-12-12},
  abstract = {In neural networks literature, there is a strong interest in identifying and defining activation functions which can improve neural network performance. In recent years there has been a renovated interest in the scientific community in investigating activation functions which can be trained during the learning process, usually referred to as trainable, learnable or adaptable activation functions. They appear to lead to better network performance. Diverse and heterogeneous models of trainable activation function have been proposed in the literature. In this paper, we present a survey of these models. Starting from a discussion on the use of the term ``activation function'' in literature, we propose a taxonomy of trainable activation functions, highlight common and distinctive proprieties of recent and past models, and discuss main advantages and limitations of this type of approach. We show that many of the proposed approaches are equivalent to adding neuron layers which use fixed (non-trainable) activation functions and some simple local rule that constrains the corresponding weight layers.},
  keywords = {Activation functions,Learnable activation functions,Machine learning,Neural networks,Trainable activation functions},
  file = {/Users/daniekru/Zotero/storage/NTJ38X85/Apicella et al. - 2021 - A survey on modern trainable activation functions.pdf;/Users/daniekru/Zotero/storage/7TM4A8BM/S0893608021000344.html}
}

@article{arielIntrinsicVariabilityPv2012,
  title = {Intrinsic Variability in {{Pv}}, {{RRP}} Size, {{Ca}}(2+) Channel Repertoire, and Presynaptic Potentiation in Individual Synaptic Boutons},
  author = {Ariel, Pablo and Hoppa, Michael B. and Ryan, Timothy A.},
  year = {2012},
  journal = {Frontiers in Synaptic Neuroscience},
  volume = {4},
  pages = {9},
  issn = {1663-3563},
  doi = {10.3389/fnsyn.2012.00009},
  abstract = {The strength of individual synaptic contacts is considered a key modulator of information flow across circuits. Presynaptically the strength can be parsed into two key parameters: the size of the readily releasable pool (RRP) and the probability that a vesicle in that pool will undergo exocytosis when an action potential fires (Pv). How these variables are controlled and the degree to which they vary across individual nerve terminals is crucial to understand synaptic plasticity within neural circuits. Here we report robust measurements of these parameters in rat hippocampal neurons and their variability across populations of individual synapses. We explore the diversity of presynaptic Ca(2+) channel repertoires and evaluate their effect on synaptic strength at single boutons. Finally, we study the degree to which synapses can be differentially modified by a known potentiator of presynaptic function, forskolin. Our experiments revealed that both Pv and RRP spanned a large range, even for synapses made by the same axon, demonstrating that presynaptic efficacy is governed locally at the single synapse level. Synapses varied greatly in their dependence on N or P/Q type Ca(2+) channels for neurotransmission, but there was no association between specific channel repertoires and synaptic efficacy. Increasing cAMP concentration using forskolin enhanced synaptic transmission in a Ca(2+)-independent manner that was inversely related with a synapse's initial Pv, and independent of its RRP size. We propose a simple model based on the relationship between Pv and calcium entry that can account for the variable potentiation of synapses based on initial probability of vesicle fusion.},
  langid = {english},
  pmcid = {PMC3542534},
  pmid = {23335896},
  keywords = {exocytosis,imaging,pHluorin,readily releasable pool,release probability,synapse},
  file = {/Users/daniekru/Zotero/storage/PM4UDV8H/Ariel et al. - 2012 - Intrinsic variability in Pv, RRP size, Ca(2+) channel repertoire, and presynaptic potentiation in in.pdf}
}

@article{asaadBackPropagationNeural2019,
  title = {Back {{Propagation Neural Network}}({{BPNN}}) and {{Sigmoid Activation Function}} in {{Multi-Layer Networks}}},
  author = {Asaad, Renas and Ali, Rasan},
  year = {2019},
  month = nov,
  journal = {Academic Journal of Nawroz University},
  volume = {8},
  pages = {216},
  doi = {10.25007/ajnu.v8n4a464},
  abstract = {Back propagation neural network are known for computing the problems that cannot easily be computed (huge datasets analysis or training) in artificial neural networks. The main idea of this paper is to implement XOR logic gate by ANNs using back propagation neural network for back propagation of errors, and sigmoid activation function. This neural network to map non-linear threshold gate. The non-linear used to classify binary inputs (x1, x2) and passing it through hidden layer for computing coefficient\_errors and gradient\_errors (Cerrors, Gerrors), after computing errors by (ei = Output\_desired- Output\_actual) the weights and thetas ({$\Delta$}Wji = ({$\alpha$})(Xj)(gi), {$\Delta\upvarTheta$}j = ({$\alpha$})(-1)(gi)) are changing according to errors. Sigmoid activation function is = sig(x)=1/(1+e-x) and Derivation of sigmoid is = dsig(x) = sig(x)(1-sig(x)). The sig(x) and Dsig(x) is between 1 to 0.}
}

@article{auerFinitetimeAnalysisMultiarmed2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo},
  year = {2002},
  journal = {Machine Learning},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/N9AC4Q7C/Auer and Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf}
}

@article{averbeckTheoryChoiceBandit2015,
  title = {Theory of {{Choice}} in {{Bandit}}, {{Information Sampling}} and {{Foraging Tasks}}},
  author = {Averbeck, Bruno B.},
  year = {2015},
  month = mar,
  journal = {PLoS Computational Biology},
  volume = {11},
  number = {3},
  pages = {e1004164},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1004164},
  urldate = {2024-03-25},
  abstract = {Decision making has been studied with a wide array of tasks. Here we examine the theoretical structure of bandit, information sampling and foraging tasks. These tasks move beyond tasks where the choice in the current trial does not affect future expected rewards. We have modeled these tasks using Markov decision processes (MDPs). MDPs provide a general framework for modeling tasks in which decisions affect the information on which future choices will be made. Under the assumption that agents are maximizing expected rewards, MDPs provide normative solutions. We find that all three classes of tasks pose choices among actions which trade-off immediate and future expected rewards. The tasks drive these trade-offs in unique ways, however. For bandit and information sampling tasks, increasing uncertainty or the time horizon shifts value to actions that pay-off in the future. Correspondingly, decreasing uncertainty increases the relative value of actions that pay-off immediately. For foraging tasks the time-horizon plays the dominant role, as choices do not affect future uncertainty in these tasks., Numerous choice tasks have been used to study decision processes. Some of these choice tasks, specifically n-armed bandit, information sampling and foraging tasks, pose choices that trade-off immediate and future reward. Specifically, the best choice may not be the choice that pays off the highest reward immediately, and exploration of unknown options vs. exploiting known options can be a normatively useful strategy. We characterized the optimal choice strategies across these tasks using Markov Decision Processes (MDPs). The MDP framework can characterize optimal choice strategies when choices are affected by the value of future rewards. We found that uncertainty and time horizon have important effects on the choice strategies in these tasks. Specifically, in bandit and information sampling tasks, increasing uncertainty increases the value of exploring choice options that tend to pay off in the future, while decreasing uncertainty increases the value of choice options that pay off immediately. These effects are increased when time horizons are longer. Foraging tasks differ in that uncertainty plays a minimal role. However, time horizon is still important in foraging. Specifically, for long time horizons, travel delays to rewards become less relevant.},
  pmcid = {PMC4376795},
  pmid = {25815510},
  file = {/Users/daniekru/Zotero/storage/SIVS27TJ/Averbeck - 2015 - Theory of Choice in Bandit, Information Sampling a.pdf}
}

@article{backmanEffectsWorkingMemoryTraining2011,
  title = {Effects of {{Working-Memory Training}} on {{Striatal Dopamine Release}}},
  author = {B{\"a}ckman, Lars and Nyberg, Lars and Soveri, Anna and Johansson, Jarkko and Andersson, Micael and Dahlin, Erika and Neely, Anna S. and Virta, Jere and Laine, Matti and Rinne, Juha O.},
  year = {2011},
  month = aug,
  journal = {Science},
  volume = {333},
  number = {6043},
  pages = {718--718},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1204978},
  urldate = {2024-04-26},
  abstract = {Updating of working memory has been associated with striato-frontal brain regions and phasic dopaminergic neurotransmission. We assessed raclopride binding to striatal dopamine (DA) D2 receptors during a letter-updating task and a control condition before and after 5 weeks of updating training. Results showed that updating affected DA activity before training and that training further increased striatal DA release during updating. These findings highlight the pivotal role of transient neural processes associated with D2 receptor activity in working memory.},
  file = {/Users/daniekru/Zotero/storage/CM3EC9SP/Bäckman et al. - 2011 - Effects of Working-Memory Training on Striatal Dop.pdf}
}

@incollection{baddeleyWorkingMemory1974,
  title = {Working {{Memory}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Baddeley, Alan D. and Hitch, Graham},
  year = {1974},
  volume = {8},
  pages = {47--89},
  publisher = {Elsevier},
  doi = {10.1016/S0079-7421(08)60452-1},
  urldate = {2024-05-13},
  isbn = {978-0-12-543308-2},
  langid = {english}
}

@article{balewskiValueDynamicsAffect2023,
  title = {Value Dynamics Affect Choice Preparation during Decision-Making},
  author = {Balewski, Zuzanna Z. and Elston, Thomas W. and Knudsen, Eric B. and Wallis, Joni D.},
  year = {2023},
  month = sep,
  journal = {Nature neuroscience},
  volume = {26},
  number = {9},
  pages = {1575--1583},
  issn = {1097-6256},
  doi = {10.1038/s41593-023-01407-3},
  urldate = {2024-11-27},
  abstract = {During decision-making, neurons in the orbitofrontal cortex (OFC) sequentially represent the value of each option in turn, but it is unclear how these dynamics are translated into a choice response. One brain region that may be implicated in this process is the anterior cingulate cortex (ACC), which strongly connects with OFC and contains many neurons that encode the choice response. We investigated how OFC value signals interacted with ACC neurons encoding the choice response by performing simultaneous high-channel count recordings from the two areas in nonhuman primates. ACC neurons encoding the choice response steadily increased their firing rate throughout the decision-making process, peaking shortly before the time of the choice response. Furthermore, the value dynamics in OFC affected ACC ramping---when OFC represented the more valuable option, ACC ramping accelerated. Because OFC tended to represent the more valuable option more frequently and for a longer duration, this interaction could explain how ACC selects the more valuable response.},
  pmcid = {PMC10576429},
  pmid = {37563295},
  file = {/Users/daniekru/Zotero/storage/484SD2K3/Balewski et al. - 2023 - Value dynamics affect choice preparation during decision-making.pdf}
}

@misc{banMultifacetContextualBandits2021,
  title = {Multi-Facet {{Contextual Bandits}}: {{A Neural Network Perspective}}},
  shorttitle = {Multi-Facet {{Contextual Bandits}}},
  author = {Ban, Yikun and He, Jingrui and Cook, Curtiss B.},
  year = {2021},
  month = jun,
  number = {arXiv:2106.03039},
  eprint = {2106.03039},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-03},
  abstract = {Contextual multi-armed bandit has shown to be an effective tool in recommender systems. In this paper, we study a novel problem of multi-facet bandits involving a group of bandits, each characterizing the users' needs from one unique aspect. In each round, for the given user, we need to select one arm from each bandit, such that the combination of all arms maximizes the final reward. This problem can find immediate applications in E-commerce, healthcare, etc. To address this problem, we propose a novel algorithm, named MuFasa, which utilizes an assembled neural network to jointly learn the underlying reward functions of multiple bandits. It estimates an Upper Confidence Bound (UCB) linked with the expected reward to balance between exploitation and exploration. Under mild assumptions, we provide the regret analysis of MuFasa. It can achieve the near-optimal \${\textbackslash}widetilde\{ {\textbackslash}mathcal\{O\}\}((K+1){\textbackslash}sqrt\{T\})\$ regret bound where \$K\$ is the number of bandits and \$T\$ is the number of played rounds. Furthermore, we conduct extensive experiments to show that MuFasa outperforms strong baselines on real-world data sets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/3LTK9QB4/Ban et al. - 2021 - Multi-facet Contextual Bandits A Neural Network P.pdf}
}

@article{barakNeuronalPopulationCoding2010,
  title = {Neuronal {{Population Coding}} of {{Parametric Working Memory}}},
  author = {Barak, Omri and Tsodyks, Misha and Romo, Ranulfo},
  year = {2010},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {28},
  pages = {9424--9430},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1875-10.2010},
  urldate = {2024-05-13},
  abstract = {Comparing two sequentially presented stimuli is a widely used experimental paradigm for studying working memory. The delay activity of many single neurons in the prefrontal cortex (PFC) of monkeys was found to be stimulus-specific, however, population dynamics of stimulus representation has not been elucidated. We analyzed the population state of a large number of PFC neurons during a somatosensory discrimination task. Using the tuning curves of the neurons, we derived a compact characterization of the population state. Stimulus representation by the population was found to degrade after stimulus termination, and emerge in a different form toward the end of the delay. Specifically, the tuning properties of neurons were found to change during the task. We suggest a mechanism whereby information about the stimulus is contained in activity-dependent synaptic facilitation of recurrent connections.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2010 the authors 0270-6474/10/309424-07\$15.00/0},
  langid = {english},
  pmid = {20631171},
  file = {/Users/daniekru/Zotero/storage/ZLIXIUS4/Barak et al. - 2010 - Neuronal Population Coding of Parametric Working M.pdf}
}

@article{barakWorkingModelsWorking2014,
  title = {Working Models of Working Memory},
  author = {Barak, Omri and Tsodyks, Misha},
  year = {2014},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  series = {Theoretical and Computational Neuroscience},
  volume = {25},
  pages = {20--24},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2013.10.008},
  urldate = {2024-05-13},
  abstract = {Working memory is a system that maintains and manipulates information for several seconds during the planning and execution of many cognitive tasks. Traditionally, it was believed that the neuronal underpinning of working memory is stationary persistent firing of selective neuronal populations. Recent advances introduced new ideas regarding possible mechanisms of working memory, such as short-term synaptic facilitation, precise tuning of recurrent excitation and inhibition, and intrinsic network dynamics. These ideas are motivated by computational considerations and careful analysis of experimental data. Taken together, they may indicate the plethora of different processes underlying working memory in the brain.},
  file = {/Users/daniekru/Zotero/storage/KMXSSXRR/S0959438813002158.html}
}

@article{bariDynamicDecisionMaking2021,
  title = {Dynamic Decision Making and Value Computations in Medial Frontal Cortex},
  author = {Bari, Bilal A. and Cohen, Jeremiah Y.},
  year = {2021},
  journal = {International review of neurobiology},
  volume = {158},
  pages = {83--113},
  issn = {0074-7742},
  doi = {10.1016/bs.irn.2020.12.001},
  urldate = {2024-05-07},
  pmcid = {PMC8162729},
  pmid = {33785157},
  file = {/Users/daniekru/Zotero/storage/38ANPK5Z/Bari and Cohen - 2021 - Dynamic decision making and value computations in .pdf}
}

@article{bariDynamicDecisionMaking2021a,
  title = {Dynamic Decision Making and Value Computations in Medial Frontal Cortex},
  author = {Bari, Bilal A. and Cohen, Jeremiah Y.},
  year = {2021},
  journal = {International review of neurobiology},
  volume = {158},
  pages = {83--113},
  issn = {0074-7742},
  doi = {10.1016/bs.irn.2020.12.001},
  urldate = {2024-05-07},
  pmcid = {PMC8162729},
  pmid = {33785157},
  file = {/Users/daniekru/Zotero/storage/8FCF9538/Bari and Cohen - 2021 - Dynamic decision making and value computations in .pdf}
}

@misc{bartolHippocampalSpineHead2015,
  title = {Hippocampal {{Spine Head Sizes Are Highly Precise}}},
  author = {Bartol, Thomas M. and Bromer, Cailey and Kinney, Justin and Chirillo, Michael A. and Bourne, Jennifer N. and Harris, Kristen M. and Sejnowski, Terrence J.},
  year = {2015},
  month = mar,
  primaryclass = {New Results},
  pages = {016329},
  publisher = {bioRxiv},
  doi = {10.1101/016329},
  urldate = {2024-12-12},
  abstract = {Hippocampal synaptic activity is probabilistic and because synaptic plasticity depends on its history, the amount of information that can be stored at a synapse is limited. The strong correlation between the size and efficacy of a synapse allowed us to estimate the precision of synaptic plasticity. In an electron microscopic reconstruction of hippocampal neuropil we found single axons making two or more synaptic contacts onto the same dendrites which would have shared histories of presynaptic and postsynaptic activity. The postsynaptic spine heads, but not the spine necks, of these pairs were nearly identical in size. The precision is much greater than previous estimates and requires postsynaptic averaging over a time window many seconds to minutes in duration depending on the rate of input spikes and probability of release. One Sentence Summary Spine heads on the same dendrite that receive input from the same axon are the same size.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2015, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/IDEYXNTM/Bartol et al. - 2015 - Hippocampal Spine Head Sizes Are Highly Precise.pdf}
}

@article{bartoloPrefrontalCortexPredicts2020,
  title = {Prefrontal {{Cortex Predicts State Switches}} during {{Reversal Learning}}},
  author = {Bartolo, Ramon and Averbeck, Bruno B.},
  year = {2020},
  month = jun,
  journal = {Neuron},
  volume = {106},
  number = {6},
  pages = {1044-1054.e4},
  publisher = {Elsevier},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2020.03.024},
  urldate = {2024-05-14},
  langid = {english},
  pmid = {32315603},
  keywords = {Bayesian update,large-scale recordings,macaques,model-based,neural ensemble,prefrontal cortex,reversal learning,state inference},
  file = {/Users/daniekru/Zotero/storage/DDZ2TWRK/Bartolo and Averbeck - 2020 - Prefrontal Cortex Predicts State Switches during R.pdf}
}

@article{bastonBiologicallyInspiredComputational2015,
  title = {A {{Biologically Inspired Computational Model}} of {{Basal Ganglia}} in {{Action Selection}}},
  author = {Baston, Chiara and Ursino, Mauro},
  year = {2015},
  month = nov,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2015},
  pages = {e187417},
  publisher = {Hindawi},
  issn = {1687-5265},
  doi = {10.1155/2015/187417},
  urldate = {2024-05-14},
  abstract = {The basal ganglia (BG) are a subcortical structure implicated in action selection. The aim of this work is to present a new cognitive neuroscience model of the BG, which aspires to represent a parsimonious balance between simplicity and completeness. The model includes the 3 main pathways operating in the BG circuitry, that is, the direct (Go), indirect (NoGo), and hyperdirect pathways. The main original aspects, compared with previous models, are the use of a two-term Hebb rule to train synapses in the striatum, based exclusively on neuronal activity changes caused by dopamine peaks or dips, and the role of the cholinergic interneurons (affected by dopamine themselves) during learning. Some examples are displayed, concerning a few paradigmatic cases: action selection in basal conditions, action selection in the presence of a strong conflict (where the role of the hyperdirect pathway emerges), synapse changes induced by phasic dopamine, and learning new actions based on a previous history of rewards and punishments. Finally, some simulations show model working in conditions of altered dopamine levels, to illustrate pathological cases (dopamine depletion in parkinsonian subjects or dopamine hypermedication). Due to its parsimonious approach, the model may represent a straightforward tool to analyze BG functionality in behavioral experiments.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/D3SNG68W/Baston and Ursino - 2015 - A Biologically Inspired Computational Model of Bas.pdf}
}

@article{behrensLearningValueInformation2007,
  title = {Learning the Value of Information in an Uncertain World},
  author = {Behrens, Timothy E. J. and Woolrich, Mark W. and Walton, Mark E. and Rushworth, Matthew F. S.},
  year = {2007},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {9},
  pages = {1214--1221},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn1954},
  urldate = {2024-05-02},
  abstract = {Our decisions are guided by outcomes that are associated with decisions made in the past. However, the amount of influence each past outcome has on our next decision remains unclear. To ensure optimal decision-making, the weight given to decision outcomes should reflect their salience in predicting future outcomes, and this salience should be modulated by the volatility of the reward environment. We show that human subjects assess volatility in an optimal manner and adjust decision-making accordingly. This optimal estimate of volatility is reflected in the fMRI signal in the anterior cingulate cortex (ACC) when each trial outcome is observed. When a new piece of information is witnessed, activity levels reflect its salience for predicting future outcomes. Furthermore, variations in this ACC signal across the population predict variations in subject learning rates. Our results provide a formal account of how we weigh our different experiences in guiding our future actions.},
  copyright = {2007 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/HQA3X56G/Behrens et al. - 2007 - Learning the value of information in an uncertain .pdf}
}

@inproceedings{besbesStochasticMultiArmedBanditProblem2014,
  title = {Stochastic {{Multi-Armed-Bandit Problem}} with {{Non-stationary Rewards}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
  year = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-25},
  abstract = {In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward variation" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks."},
  file = {/Users/daniekru/Zotero/storage/DIVA8FYS/Besbes et al. - 2014 - Stochastic Multi-Armed-Bandit Problem with Non-sta.pdf}
}

@misc{binzModelingHumanExploration2022,
  title = {Modeling {{Human Exploration Through Resource-Rational Reinforcement Learning}}},
  author = {Binz, Marcel and Schulz, Eric},
  year = {2022},
  month = nov,
  number = {arXiv:2201.11817},
  eprint = {2201.11817},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-03},
  abstract = {Equipping artificial agents with useful exploration mechanisms remains a challenge to this day. Humans, on the other hand, seem to manage the trade-off between exploration and exploitation effortlessly. In the present article, we put forward the hypothesis that they accomplish this by making optimal use of limited computational resources. We study this hypothesis by meta-learning reinforcement learning algorithms that sacrifice performance for a shorter description length (defined as the number of bits required to implement the given algorithm). The emerging class of models captures human exploration behavior better than previously considered approaches, such as Boltzmann exploration, upper confidence bound algorithms, and Thompson sampling. We additionally demonstrate that changing the description length in our class of models produces the intended effects: reducing description length captures the behavior of brain-lesioned patients while increasing it mirrors cognitive development during adolescence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/RIP3QH3R/Binz and Schulz - 2022 - Modeling Human Exploration Through Resource-Ration.pdf}
}

@article{blackmanTargetcellspecificShorttermPlasticity2013,
  title = {Target-Cell-Specific Short-Term Plasticity in Local Circuits},
  author = {Blackman, Arne V. and Abrahamsson, Therese and Costa, Rui Ponte and Lalanne, Txomin and Sj{\"o}str{\"o}m, P. Jesper},
  year = {2013},
  month = dec,
  journal = {Frontiers in Synaptic Neuroscience},
  volume = {5},
  pages = {11},
  issn = {1663-3563},
  doi = {10.3389/fnsyn.2013.00011},
  abstract = {Short-term plasticity (STP) denotes changes in synaptic strength that last up to tens of seconds. It is generally thought that STP impacts information transfer across synaptic connections and may thereby provide neurons with, for example, the ability to detect input coherence, to maintain stability and to promote synchronization. STP is due to a combination of mechanisms, including vesicle depletion and calcium accumulation in synaptic terminals. Different forms of STP exist, depending on many factors, including synapse type. Recent evidence shows that synapse dependence holds true even for connections that originate from a single presynaptic cell, which implies that postsynaptic target cell type can determine synaptic short-term dynamics. This arrangement is surprising, since STP itself is chiefly due to presynaptic mechanisms. Target-specific synaptic dynamics in addition imply that STP is not a bug resulting from synapses fatiguing when driven too hard, but rather a feature that is selectively implemented in the brain for specific functional purposes. As an example, target-specific STP results in sequential somatic and dendritic inhibition in neocortical and hippocampal excitatory cells during high-frequency firing. Recent studies also show that the Elfn1 gene specifically controls STP at some synapse types. In addition, presynaptic NMDA receptors have been implicated in synapse-specific control of synaptic dynamics during high-frequency activity. We argue that synapse-specific STP deserves considerable further study, both experimentally and theoretically, since its function is not well known. We propose that synapse-specific STP has to be understood in the context of the local circuit, which requires combining different scientific disciplines ranging from molecular biology through electrophysiology to computer modeling.},
  langid = {english},
  pmcid = {PMC3854841},
  pmid = {24367330},
  keywords = {development,network models,short-term plasticity,synapse formation,synapse specificity,synaptic disease},
  file = {/Users/daniekru/Zotero/storage/R3YRB8CR/Blackman et al. - 2013 - Target-cell-specific short-term plasticity in local circuits.pdf}
}

@article{boerlinPredictiveCodingDynamical2013,
  title = {Predictive {{Coding}} of {{Dynamical Variables}} in {{Balanced Spiking Networks}}},
  author = {Boerlin, Martin and Machens, Christian K. and Den{\`e}ve, Sophie},
  year = {2013},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {11},
  pages = {e1003258},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003258},
  urldate = {2024-05-13},
  abstract = {Two observations about the cortex have puzzled neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks that represent information efficiently in their spikes. We illustrate this insight with spiking networks that represent dynamical variables. Our approach is based on two assumptions: We assume that information about dynamical variables can be read out linearly from neural spike trains, and we assume that neurons only fire a spike if that improves the representation of the dynamical variables. Based on these assumptions, we derive a network of leaky integrate-and-fire neurons that is able to implement arbitrary linear dynamical systems. We show that the membrane voltage of the neurons is equivalent to a prediction error about a common population-level signal. Among other things, our approach allows us to construct an integrator network of spiking neurons that is robust against many perturbations. Most importantly, neural variability in our networks cannot be equated to noise. Despite exhibiting the same single unit properties as widely used population code models (e.g. tuning curves, Poisson distributed spike trains), balanced networks are orders of magnitudes more reliable. Our approach suggests that spikes do matter when considering how the brain computes, and that the reliability of cortical representations could have been strongly underestimated.},
  langid = {english},
  keywords = {Action potentials,Dynamical systems,Membrane potential,Network analysis,Neural networks,Neuronal tuning,Neurons,Sensory perception},
  file = {/Users/daniekru/Zotero/storage/9FYLREQT/Boerlin et al. - 2013 - Predictive Coding of Dynamical Variables in Balanc.pdf}
}

@article{bouchacourtFlexibleModelWorking2019,
  title = {A {{Flexible Model}} of {{Working Memory}}},
  author = {Bouchacourt, Flora and Buschman, Timothy J.},
  year = {2019},
  month = jul,
  journal = {Neuron},
  volume = {103},
  number = {1},
  pages = {147-160.e8},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.04.020},
  urldate = {2024-05-13},
  abstract = {Working memory is fundamental to cognition, allowing one to hold information ``in mind.'' A defining characteristic of working memory is its flexibility: we can hold anything in mind. However, typical models of working memory rely on finely tuned, content-specific attractors to persistently maintain neural activity and therefore do not allow for the flexibility observed in behavior. Here, we present a flexible model of working memory that maintains representations through random recurrent connections between two layers of neurons: a structured ``sensory'' layer and a randomly connected, unstructured layer. As the interactions are untuned with respect to the content being stored, the network maintains any arbitrary input. However, in our model, this flexibility comes at a cost: the random connections overlap, leading to interference between representations and limiting the memory capacity of the network. Additionally, our model captures several other key behavioral and neurophysiological characteristics of working memory.},
  keywords = {capacity limitations,cognitive control,cognitive flexibility,computational model,excitation-inhibition balance,mixed selectivity,working memory},
  file = {/Users/daniekru/Zotero/storage/WIEJTLCA/Bouchacourt and Buschman - 2019 - A Flexible Model of Working Memory.pdf;/Users/daniekru/Zotero/storage/ZHUIKJM6/S0896627319303770.html}
}

@article{brunelEffectsNeuromodulationCortical2001,
  title = {Effects of {{Neuromodulation}} in a {{Cortical Network Model}} of {{Object Working Memory Dominated}} by {{Recurrent Inhibition}}},
  author = {Brunel, Nicolas and Wang, Xiao-Jing},
  year = {2001},
  month = jul,
  journal = {Journal of Computational Neuroscience},
  volume = {11},
  number = {1},
  pages = {63--85},
  issn = {1573-6873},
  doi = {10.1023/A:1011204814320},
  urldate = {2024-05-13},
  abstract = {Experimental evidence suggests that the maintenance of an item in working memory is achieved through persistent activity in selective neural assemblies of the cortex. To understand the mechanisms underlying this phenomenon, it is essential to investigate how persistent activity is affected by external inputs or neuromodulation. We have addressed these questions using a recurrent network model of object working memory. Recurrence is dominated by inhibition, although persistent activity is generated through recurrent excitation in small subsets of excitatory neurons.},
  langid = {english},
  keywords = {AMPA,dopamine,GABA,inferotemporal cortex,network model,NMDA,persistent activity,prefrontal cortex,spontaneous activity,working memory},
  file = {/Users/daniekru/Zotero/storage/KSR8CNL3/Brunel and Wang - 2001 - Effects of Neuromodulation in a Cortical Network M.pdf}
}

@book{burtiniImprovingOnlineMarketing2015,
  title = {￼{{Improving Online Marketing Experiments}} with {{Drifting Multi-Armed Bandits}}},
  author = {Burtini, Giuseppe and Loeppky, Jason and Lawrence, Ramon},
  year = {2015},
  month = apr,
  journal = {ICEIS 2015 - 17th International Conference on Enterprise Information Systems, Proceedings},
  volume = {1},
  publisher = {International Conference on Enterprise Information Systems},
  doi = {10.5220/0005458706300636},
  abstract = {Restless bandits model the exploration vs. exploitation trade-off in a changing (non-stationary) world. Restless bandits have been studied in both the context of continuously-changing (drifting) and change-point (sudden) restlessness. In this work, we study specific classes of drifting restless bandits selected for their relevance to modelling an online website optimization process. The contribution in this work is a simple, feasible weighted least squares technique capable of utilizing contextual arm parameters while considering the parameter space drifting non-stationary within reasonable bounds. We produce a reference implementation, then evaluate and compare its performance in several different true world states, finding experimentally that performance is robust to time drifting factors similar to those seen in many real world cases.},
  file = {/Users/daniekru/Zotero/storage/RV3P2QBX/Burtini et al. - 2015 - ￼Improving Online Marketing Experiments with Drift.pdf}
}

@article{carrollEncodingCertaintyBump2014,
  title = {Encoding Certainty in Bump Attractors},
  author = {Carroll, Sam and Josi{\'c}, Kre{\v s}imir and Kilpatrick, Zachary P.},
  year = {2014},
  month = aug,
  journal = {Journal of Computational Neuroscience},
  volume = {37},
  number = {1},
  pages = {29--48},
  issn = {1573-6873},
  doi = {10.1007/s10827-013-0486-0},
  urldate = {2024-05-15},
  abstract = {Persistent activity in neuronal populations has been shown to represent the spatial position of remembered stimuli. Networks that support bump attractors are often used to model such persistent activity. Such models usually exhibit translational symmetry. Thus activity bumps are neutrally stable, and perturbations in position do not decay away. We extend previous work on bump attractors by constructing model networks capable of encoding the certainty or salience of a stimulus stored in memory. Such networks support bumps that are not only neutrally stable to perturbations in position, but also perturbations in amplitude. Possible bump solutions then lie on a two-dimensional attractor, determined by a continuum of positions and amplitudes. Such an attractor requires precisely balancing the strength of recurrent synaptic connections. The amplitude of activity bumps represents certainty, and is determined by the initial input to the system. Moreover, bumps with larger amplitudes are more robust to noise, and over time provide a more faithful representation of the stored stimulus. In networks with separate excitatory and inhibitory populations, generating bumps with a continuum of possible amplitudes, requires tuning the strength of inhibition to precisely cancel background excitation.},
  langid = {english},
  keywords = {Bump attractor,Excitation-inhibition balance,Neural field,Spatial working memory},
  file = {/Users/daniekru/Zotero/storage/W494N8BD/Carroll et al. - 2014 - Encoding certainty in bump attractors.pdf}
}

@article{cavenaghiNonStationaryMultiArmed2021,
  title = {Non {{Stationary Multi-Armed Bandit}}: {{Empirical Evaluation}} of a {{New Concept Drift-Aware Algorithm}}},
  shorttitle = {Non {{Stationary Multi-Armed Bandit}}},
  author = {Cavenaghi, Emanuele and Sottocornola, Gabriele and Stella, Fabio and Zanker, Markus},
  year = {2021},
  month = mar,
  journal = {Entropy},
  volume = {23},
  number = {3},
  pages = {380},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23030380},
  urldate = {2024-03-25},
  abstract = {The Multi-Armed Bandit (MAB) problem has been extensively studied in order to address real-world challenges related to sequential decision making. In this setting, an agent selects the best action to be performed at time-step t, based on the past rewards received by the environment. This formulation implicitly assumes that the expected payoff for each action is kept stationary by the environment through time. Nevertheless, in many real-world applications this assumption does not hold and the agent has to face a non-stationary environment, that is, with a changing reward distribution. Thus, we present a new MAB algorithm, named f-Discounted-Sliding-Window Thompson Sampling (f-dsw TS), for non-stationary environments, that is, when the data streaming is affected by concept drift. The f-dsw TS algorithm is based on Thompson Sampling (TS) and exploits a discount factor on the reward history and an arm-related sliding window to contrast concept drift in non-stationary environments. We investigate how to combine these two sources of information, namely the discount factor and the sliding window, by means of an aggregation function f(.). In particular, we proposed a pessimistic (f=min), an optimistic (f=max), as well as an averaged (f=mean) version of the f-dsw TS algorithm. A rich set of numerical experiments is performed to evaluate the f-dsw TS algorithm compared to both stationary and non-stationary state-of-the-art TS baselines. We exploited synthetic environments (both randomly-generated and controlled) to test the MAB algorithms under different types of drift, that is, sudden/abrupt, incremental, gradual and increasing/decreasing drift. Furthermore, we adapt four real-world active learning tasks to our framework---a prediction task on crimes in the city of Baltimore, a classification task on insects species, a recommendation task on local web-news, and a time-series analysis on microbial organisms in the tropical air ecosystem. The f-dsw TS approach emerges as the best performing MAB algorithm. At least one of the versions of f-dsw TS performs better than the baselines in synthetic environments, proving the robustness of f-dsw TS under different concept drift types. Moreover, the pessimistic version (f=min) results as the most effective in all real-world tasks.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {concept drift,machine learning,multi-armed bandit,non-stationary multi-armed bandit,Thompson Sampling,time-series analysis},
  file = {/Users/daniekru/Zotero/storage/IE86HVJ3/Cavenaghi et al. - 2021 - Non Stationary Multi-Armed Bandit Empirical Evalu.pdf}
}

@article{chenSpikingNeuralNetwork2023,
  title = {Spiking Neural Network with Working Memory Can Integrate and Rectify Spatiotemporal Features},
  author = {Chen, Yi and Liu, Hanwen and Shi, Kexin and Zhang, Malu and Qu, Hong},
  year = {2023},
  month = jun,
  journal = {Frontiers in Neuroscience},
  volume = {17},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2023.1167134},
  urldate = {2024-05-03},
  abstract = {{$<$}p{$>$}In the real world, information is often correlated with each other in the time domain. Whether it can effectively make a decision according to the global information is the key indicator of information processing ability. Due to the discrete characteristics of spike trains and unique temporal dynamics, spiking neural networks (SNNs) show great potential in applications in ultra-low-power platforms and various temporal-related real-life tasks. However, the current SNNs can only focus on the information a short time before the current moment, its sensitivity in the time domain is limited. This problem affects the processing ability of SNN in different kinds of data, including static data and time-variant data, and reduces the application scenarios and scalability of SNN. In this work, we analyze the impact of such information loss and then integrate SNN with working memory inspired by recent neuroscience research. Specifically, we propose Spiking Neural Networks with Working Memory (SNNWM) to handle input spike trains segment by segment. On the one hand, this model can effectively increase SNN's ability to obtain global information. On the other hand, it can effectively reduce the information redundancy between adjacent time steps. Then, we provide simple methods to implement the proposed network architecture from the perspectives of biological plausibility and neuromorphic hardware friendly. Finally, we test the proposed method on static and sequential data sets, and the experimental results show that the proposed model can better process the whole spike train, and achieve state-of-the-art results in short time steps. This work investigates the contribution of introducing biologically inspired mechanisms, e.g., working memory, and multiple delayed synapses to SNNs, and provides a new perspective to design future SNNs.{$<$}/p{$>$}},
  langid = {english},
  keywords = {CIFAR10,Convolutional Neural Network,Multi-dendrite,Spiking Neural network,working memory},
  file = {/Users/daniekru/Zotero/storage/F87L27R5/Chen et al. - 2023 - Spiking neural network with working memory can int.pdf}
}

@article{citriSynapticPlasticityMultiple2008,
  title = {Synaptic {{Plasticity}}: {{Multiple Forms}}, {{Functions}}, and {{Mechanisms}}},
  shorttitle = {Synaptic {{Plasticity}}},
  author = {Citri, Ami and Malenka, Robert C.},
  year = {2008},
  month = jan,
  journal = {Neuropsychopharmacology},
  volume = {33},
  number = {1},
  pages = {18--41},
  publisher = {Nature Publishing Group},
  issn = {1740-634X},
  doi = {10.1038/sj.npp.1301559},
  urldate = {2024-12-09},
  abstract = {Experiences, whether they be learning in a classroom, a stressful event, or ingestion of a psychoactive substance, impact the brain by modifying the activity and organization of specific neural circuitry. A major mechanism by which the neural activity generated by an experience modifies brain function is via modifications of synaptic transmission; that is, synaptic plasticity. Here, we review current understanding of the mechanisms of the major forms of synaptic plasticity at excitatory synapses in the mammalian brain. We also provide examples of the possible developmental and behavioral functions of synaptic plasticity and how maladaptive synaptic plasticity may contribute to neuropsychiatric disorders.},
  copyright = {2008 American College of Neuropsychopharmacology},
  langid = {english},
  keywords = {Behavioral Sciences,Biological Psychology,general,Medicine/Public Health,Neurosciences,Pharmacotherapy,Psychiatry},
  file = {/Users/daniekru/Zotero/storage/QVUS3WTU/Citri and Malenka - 2008 - Synaptic Plasticity Multiple Forms, Functions, and Mechanisms.pdf}
}

@article{clairisValueConfidenceDeliberation2022,
  title = {Value, {{Confidence}}, {{Deliberation}}: {{A Functional Partition}} of the {{Medial Prefrontal Cortex Demonstrated}} across {{Rating}} and {{Choice Tasks}}},
  shorttitle = {Value, {{Confidence}}, {{Deliberation}}},
  author = {Clairis, Nicolas and Pessiglione, Mathias},
  year = {2022},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {42},
  number = {28},
  pages = {5580--5592},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1795-21.2022},
  urldate = {2024-11-27},
  abstract = {Deciding about courses of action involves minimizing costs and maximizing benefits. Decision neuroscience studies have implicated both the ventral and dorsal medial PFC (vmPFC and dmPFC) in signaling goal value and action cost, but the precise functional role of these regions is still a matter of debate. Here, we suggest a more general functional partition that applies not only to decisions but also to judgments about goal value (expected reward) and action cost (expected effort). In this conceptual framework, cognitive representations related to options (reward value and effort cost) are dissociated from metacognitive representations (confidence and deliberation) related to solving the task (providing a judgment or making a choice). We used an original approach aimed at identifying consistencies across several preference tasks, from likeability ratings to binary decisions involving both attribute integration and option comparison. fMRI results in human male and female participants confirmed the vmPFC as a generic valuation system, its activity increasing with reward value and decreasing with effort cost. In contrast, more dorsal regions were not concerned with the valuation of options but with metacognitive variables, confidence being reflected in mPFC activity and deliberation time in dmPFC activity. Thus, there was a dissociation between the effort attached to choice options (represented in the vmPFC) and the effort invested in deliberation (represented in the dmPFC), the latter being expressed in pupil dilation. More generally, assessing commonalities across preference tasks might help in reaching a unified view of the neural mechanisms underlying the cost/benefit tradeoffs that drive human behavior. SIGNIFICANCE STATEMENT Decision neuroscience studies have implicated the medial PFC in forming the cognitive representations that drive human choice behavior. However, different studies using different tasks have suggested somewhat inconsistent links between precise computational variables and specific brain regions. Here, we use fMRI to demonstrate a robust functional partition of the medial PFC that generalizes across tasks involving an estimation of goal value and/or action cost to provide a judgment or make a choice. This general functional partition makes a critical dissociation between neural representations of decisional factors (the expected costs and benefits attached to a given option) and metacognitive estimates (confidence in the judgment or choice, and effort invested in the deliberation process).},
  chapter = {Research Articles},
  copyright = {Copyright {\copyright} 2022 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {35654606},
  keywords = {decision-making,effort,fMRI,metacognition,pupillometry,reward},
  file = {/Users/daniekru/Zotero/storage/E4JEQH55/Clairis and Pessiglione - 2022 - Value, Confidence, Deliberation A Functional Partition of the Medial Prefrontal Cortex Demonstrated.pdf}
}

@article{cohenShouldStayShould2007,
  title = {Should {{I}} Stay or Should {{I}} Go? {{How}} the Human Brain Manages the Trade-off between Exploitation and Exploration [{{Proceedings}} Paper]},
  shorttitle = {Should {{I}} Stay or Should {{I}} Go?},
  author = {Cohen, Jonathan and McClure, Samuel M. and Yu, Angela},
  year = {2007},
  month = mar,
  journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
  volume = {362},
  pages = {933--42},
  doi = {10.1098/rstb.2007.2098},
  abstract = {Many large and small decisions we make in our daily lives-which ice cream to choose, what research projects to pursue, which partner to marry-require an exploration of alternatives before committing to and exploiting the benefits of a particular choice. Furthermore, many decisions require re-evaluation, and further exploration of alternatives, in the face of changing needs or circumstances. That is, often our decisions depend on a higher level choice: whether to exploit well known but possibly suboptimal alternatives or to explore risky but potentially more profitable ones. How adaptive agents choose between exploitation and exploration remains an important and open question that has received relatively limited attention in the behavioural and brain sciences. The choice could depend on a number of factors, including the familiarity of the environment, how quickly the environment is likely to change and the relative value of exploiting known sources of reward versus the cost of reducing uncertainty through exploration. There is no known generally optimal solution to the exploration versus exploitation problem, and a solution to the general case may indeed not be possible. However, there have been formal analyses of the optimal policy under constrained circumstances. There have also been specific suggestions of how humans and animals may respond to this problem under particular experimental conditions as well as proposals about the brain mechanisms involved. Here, we provide a brief review of this work, discuss how exploration and exploitation may be mediated in the brain and highlight some promising future directions for research.},
  file = {/Users/daniekru/Zotero/storage/RYJ3REI3/Cohen et al. - 2007 - Should I stay or should I go How the human brain .pdf}
}

@article{cohenTemporalDynamicsBrain1997,
  title = {Temporal Dynamics of Brain Activation during a Working Memory Task},
  author = {Cohen, Jonathan D. and Perlstein, William M. and Braver, Todd S. and Nystrom, Leigh E. and Noll, Douglas C. and Jonides, John and Smith, Edward E.},
  year = {1997},
  month = apr,
  journal = {Nature},
  volume = {386},
  number = {6625},
  pages = {604--608},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/386604a0},
  urldate = {2024-05-11},
  abstract = {Working memory is responsible for the short-term storage and online manipulation of information necessary for higher cognitive functions, such as language, planning and problem-solving1,2. Traditionally, working memory has been divided into two types of processes: executive control (governing the encoding manipulation and retrieval of information in working memory) and active maintenance (keeping information available 'online'). It has also been proposed that these two types of processes may be subserved by distinct cortical structures, with the prefrontal cortex housing the executive control processes, and more posterior regions housing the content-specific buffers (for example verbal versus visuospatial) responsible for active maintenance3,4. However, studies in non-human primates suggest that dorsolateral regions of the prefrontal cortex may also be involved in active maintenance5--8. We have used functional magnetic resonance imaging to examine brain activation in human subjects during performance of a working memory task. We used the temporal resolution of this technique to examine the dynamics of regional activation, and to show that prefrontal cortex along with parietal cortex appears to play a role in active maintenance.},
  copyright = {1997 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/daniekru/Zotero/storage/SWCQXSZ6/Cohen et al. - 1997 - Temporal dynamics of brain activation during a wor.pdf}
}

@article{constantinidisPersistentSpikingActivity2018,
  title = {Persistent {{Spiking Activity Underlies Working Memory}}},
  author = {Constantinidis, Christos and Funahashi, Shintaro and Lee, Daeyeol and Murray, John D. and Qi, Xue-Lian and Wang, Min and Arnsten, Amy F. T.},
  year = {2018},
  month = aug,
  journal = {Journal of Neuroscience},
  volume = {38},
  number = {32},
  pages = {7020--7028},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2486-17.2018},
  urldate = {2024-05-13},
  abstract = {Persistent activity generated in the PFC during the delay period of working memory tasks represents information about stimuli held in memory and determines working memory performance. Alternative models of working memory, depending on the rhythmicity of discharges or exclusively on short-term synaptic plasticity, are inconsistent with the neurophysiological data. Dual Perspectives Companion Paper:Working Memory: Delay Activity, Yes! Persistent Activity? Maybe Not, by Mikael Lundqvist, Pawel Herman, and Earl K. Miller},
  chapter = {Dual Perspectives},
  copyright = {Copyright {\copyright} 2018 the authors 0270-6474/18/387020-09\$15.00/0},
  langid = {english},
  pmid = {30089641},
  keywords = {delay period,monkey,neurophysiology,prefrontal cortex,working memory},
  file = {/Users/daniekru/Zotero/storage/KBDE6QXC/Constantinidis et al. - 2018 - Persistent Spiking Activity Underlies Working Memo.pdf}
}

@article{coolsChemistryAdaptiveMind2019,
  title = {Chemistry of the {{Adaptive Mind}}: {{Lessons}} from {{Dopamine}}},
  shorttitle = {Chemistry of the {{Adaptive Mind}}},
  author = {Cools, Roshan},
  year = {2019},
  month = oct,
  journal = {Neuron},
  volume = {104},
  number = {1},
  pages = {113--131},
  issn = {1097-4199},
  doi = {10.1016/j.neuron.2019.09.035},
  abstract = {The brain faces various computational tradeoffs, such as the stability-flexibility dilemma. The major ascending neuromodulatory systems are well suited to dynamically regulate these tradeoffs depending on changing task demands. This follows from various general principles of chemical neuromodulation, which are illustrated with evidence from pharmacological neuroimaging studies on striatal dopamine's role in output gating and cost-benefit choice of cognitive tasks. The work raises open questions, including those regarding the top-down cortical control of the midbrain dopamine system, and begins to elucidate the mechanisms underlying the variability in catecholaminergic drug effects. Such drug effects depend on the baseline state of distinct target brain regions, reflecting, in part, the systems' self-regulatory capacity to maintain equilibrium. It is hypothesized that the basal tone of different dopaminergic projection systems reflects the perceived statistics of the environment computed in frontal cortex. By normalizing dopamine levels, dopaminergic drugs might counteract the bias elicited by the perceived environment.},
  langid = {english},
  pmid = {31600509},
  keywords = {Brain,Cognition,Dopamine,Dopamine Agents,Frontal Lobe,Homeostasis,Humans,Neostriatum,Neuroimaging},
  file = {/Users/daniekru/Zotero/storage/6LJFH3CC/Cools - 2019 - Chemistry of the Adaptive Mind Lessons from Dopam.pdf}
}

@article{dardenneRolePrefrontalCortex2012,
  title = {Role of Prefrontal Cortex and the Midbrain Dopamine System in Working Memory Updating},
  author = {D'Ardenne, Kimberlee and Eshel, Neir and Luka, Joseph and Lenartowicz, Agatha and Nystrom, Leigh E. and Cohen, Jonathan D.},
  year = {2012},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {49},
  pages = {19900--19909},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1116727109},
  urldate = {2024-04-26},
  abstract = {Humans are adept at switching between goal-directed behaviors quickly and effectively. The prefrontal cortex (PFC) is thought to play a critical role by encoding, updating, and maintaining internal representations of task context in working memory. It has also been hypothesized that the encoding of context representations in PFC is regulated by phasic dopamine gating signals. Here we use multimodal methods to test these hypotheses. First we used functional MRI (fMRI) to identify regions of PFC associated with the representation of context in a working memory task. Next we used single-pulse transcranial magnetic stimulation (TMS), guided spatially by our fMRI findings and temporally by previous event-related EEG recordings, to disrupt context encoding while participants performed the same working memory task. We found that TMS pulses to the right dorsolateral PFC (DLPFC) immediately after context presentation, and well in advance of the response, adversely impacted context-dependent relative to context-independent responses. This finding causally implicates right DLPFC function in context encoding. Finally, using the same paradigm, we conducted high-resolution fMRI measurements in brainstem dopaminergic nuclei (ventral tegmental area and substantia nigra) and found phasic responses after presentation of context stimuli relative to other stimuli, consistent with the timing of a gating signal that regulates the encoding of representations in PFC. Furthermore, these responses were positively correlated with behavior, as well as with responses in the same region of right DLPFC targeted in the TMS experiment, lending support to the hypothesis that dopamine phasic signals regulate encoding, and thereby the updating, of context representations in PFC.},
  file = {/Users/daniekru/Zotero/storage/J3WVZM2K/D’Ardenne et al. - 2012 - Role of prefrontal cortex and the midbrain dopamin.pdf}
}

@article{dawCorticalSubstratesExploratory2006,
  title = {Cortical Substrates for Exploratory Decisions in Humans},
  author = {Daw, Nathaniel D. and O'Doherty, John P. and Dayan, Peter and Seymour, Ben and Dolan, Raymond J.},
  year = {2006},
  month = jun,
  journal = {Nature},
  volume = {441},
  number = {7095},
  pages = {876--879},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature04766},
  urldate = {2024-05-07},
  abstract = {Humans are remarkably curious, and that is useful in helping us to learn about new environments and possibilities. But curiosity killed the cat, they say, and it also carries with it substantial potential risks and costs for us. Statisticians, engineers and economists have long considered ways of balancing the costs and benefits of exploration. Tests involving a gambling task and an fMRI brain scanner now show that humans appear to obey similar principles when considering their options. The players had to balance the desire to select the richest option based on accumulated experience against the desire to choose a less familiar option that might have a larger payoff. The frontopolar cortex, a brain area known to be involved in cognitive control, was preferentially active during exploratory decisions. The results suggest a neurobiological account of human exploration and point to a new area for behavioural and neural investigations.},
  copyright = {2006 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/daniekru/Zotero/storage/MY468LJ7/Daw et al. - 2006 - Cortical substrates for exploratory decisions in h.pdf}
}

@article{dayanDecisionTheoryReinforcement2008,
  title = {Decision Theory, Reinforcement Learning, and the Brain},
  author = {Dayan, Peter and Daw, Nathaniel D.},
  year = {2008},
  month = dec,
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  volume = {8},
  number = {4},
  pages = {429--453},
  issn = {1531-135X},
  doi = {10.3758/CABN.8.4.429},
  urldate = {2024-05-07},
  abstract = {Decision making is a core competence for animals and humans acting and surviving in environments they only partially comprehend, gaining rewards and punishments for their troubles. Decision-theoretic concepts permeate experiments and computational models in ethology, psychology, and neuroscience. Here, we review a well-known, coherent Bayesian approach to decision making, showing how it unifies issues in Markovian decision problems, signal detection psychophysics, sequential sampling, and optimal exploration and discuss paradigmatic psychological and neural examples of each problem. We discuss computational issues concerning what subjects know about their task and how ambitious they are in seeking optimal solutions; we address algorithmic topics concerning model-based and model-free methods for making choices; and we highlight key aspects of the neural implementation of decision making.},
  langid = {english},
  keywords = {Belief State,Ective State,Lateral Intraparietal Area,Markov Decision Problem,Temporal Difference Model},
  file = {/Users/daniekru/Zotero/storage/NDKXYHS4/Dayan and Daw - 2008 - Decision theory, reinforcement learning, and the b.pdf}
}

@article{didomenicoDopaminergicModulationPrefrontal2023,
  title = {Dopaminergic {{Modulation}} of {{Prefrontal Cortex Inhibition}}},
  author = {Di Domenico, Danila and Mapelli, Lisa},
  year = {2023},
  month = may,
  journal = {Biomedicines},
  volume = {11},
  number = {5},
  pages = {1276},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-9059},
  doi = {10.3390/biomedicines11051276},
  urldate = {2024-04-26},
  abstract = {The prefrontal cortex is the highest stage of integration in the mammalian brain. Its functions vary greatly, from working memory to decision-making, and are primarily related to higher cognitive functions. This explains the considerable effort devoted to investigating this area, revealing the complex molecular, cellular, and network organization, and the essential role of various regulatory controls. In particular, the dopaminergic modulation and the impact of local interneurons activity are critical for prefrontal cortex functioning, controlling the excitatory/inhibitory balance and the overall network processing. Though often studied separately, the dopaminergic and GABAergic systems are deeply intertwined in influencing prefrontal network processing. This mini review will focus on the dopaminergic modulation of GABAergic inhibition, which plays a significant role in shaping prefrontal cortex activity.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {dopaminergic system,GABAergic system,prefrontal cortex},
  file = {/Users/daniekru/Zotero/storage/6J7N6B8T/Di Domenico and Mapelli - 2023 - Dopaminergic Modulation of Prefrontal Cortex Inhib.pdf}
}

@article{duszkiewiczNoveltyDopaminergicModulation2019,
  title = {Novelty and {{Dopaminergic Modulation}} of {{Memory Persistence}}: {{A Tale}} of {{Two Systems}}},
  shorttitle = {Novelty and {{Dopaminergic Modulation}} of {{Memory Persistence}}},
  author = {Duszkiewicz, Adrian J. and McNamara, Colin G. and Takeuchi, Tomonori and Genzel, Lisa},
  year = {2019},
  month = feb,
  journal = {Trends in Neurosciences},
  volume = {42},
  number = {2},
  pages = {102--114},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2018.10.002},
  urldate = {2023-04-12},
  abstract = {Adaptation to the ever-changing world is critical for survival, and our brains are particularly tuned to remember events that differ from previous experiences. Novel experiences induce dopamine release in the hippocampus, a process which promotes memory persistence. While axons from the ventral tegmental area (VTA) were generally thought to be the exclusive source of hippocampal dopamine, recent studies have demonstrated that noradrenergic neurons in the locus coeruleus (LC) corelease noradrenaline and dopamine in the hippocampus and that their dopamine release boosts memory retention as well. In this opinion article, we propose that the projections originating from the VTA and the LC belong to two distinct systems that enhance memory of novel events. Novel experiences that share some commonality with past ones (`common novelty') activate the VTA and promote semantic memory formation via systems memory consolidation. By contrast, experiences that bear only a minimal relationship to past experiences (`distinct novelty') activate the LC to trigger strong initial memory consolidation in the hippocampus, resulting in vivid and long-lasting episodic memories.},
  langid = {english},
  keywords = {dopamine,episodic memory,hippocampus,memory consolidation,novelty,semantic memory},
  file = {/Users/daniekru/Zotero/storage/CAPKKDIT/Duszkiewicz et al. - 2019 - Novelty and Dopaminergic Modulation of Memory Pers.pdf;/Users/daniekru/Zotero/storage/FY7FGYP3/S016622361830273X.html}
}

@article{elliottDissociableFunctionsMedial2000,
  title = {Dissociable {{Functions}} in the {{Medial}} and {{Lateral Orbitofrontal Cortex}}: {{Evidence}} from {{Human Neuroimaging Studies}}},
  shorttitle = {Dissociable {{Functions}} in the {{Medial}} and {{Lateral Orbitofrontal Cortex}}},
  author = {Elliott, Rebecca and Dolan, Raymond J. and Frith, Chris D.},
  year = {2000},
  month = mar,
  journal = {Cerebral Cortex},
  volume = {10},
  number = {3},
  pages = {308--317},
  issn = {1047-3211},
  doi = {10.1093/cercor/10.3.308},
  urldate = {2024-05-14},
  abstract = {Recent imaging studies show that the orbitofrontal cortex (OFC) is~activated during a wide variety of paradigms, including guessing tasks, simple delayed matching tasks and sentence completion. We suggest that, as with other regions of the prefrontal cortex, activity in the OFC is most likely to be observed when there is insufficient information available to determine the appropriate course of action. In these circumstances the OFC, rather than other prefrontal regions, is more likely to be activated when the problem of what to do next is best solved by taking into account the likely reward value of stimuli and responses, rather than their identity or location. We suggest that selection of stimuli on the basis of their familiarity and responses on the basis of a feeling of `rightness' are also examples of selection on the basis of reward value. Within the OFC, the lateral regions are more likely to be involved when the action selected requires the suppression of previously rewarded responses.},
  file = {/Users/daniekru/Zotero/storage/92CFAQC4/Elliott et al. - 2000 - Dissociable Functions in the Medial and Lateral Or.pdf;/Users/daniekru/Zotero/storage/CCN3L9YY/449600.html}
}

@article{enelStableDynamicRepresentations2020,
  title = {Stable and Dynamic Representations of Value in the Prefrontal Cortex},
  author = {Enel, Pierre and Wallis, Joni D and Rich, Erin L},
  editor = {Schoenbaum, Geoffrey and Wassum, Kate M and Hunt, Laurence Tudor and Schoenbaum, Geoffrey},
  year = {2020},
  month = jul,
  journal = {eLife},
  volume = {9},
  pages = {e54313},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.54313},
  urldate = {2024-11-27},
  abstract = {Optimal decision-making requires that stimulus-value associations are kept up to date by constantly comparing the expected value of a stimulus with its experienced outcome. To do this, value information must be held in mind when a stimulus and outcome are separated in time. However, little is known about the neural mechanisms of working memory (WM) for value. Contradicting theories have suggested WM requires either persistent or transient neuronal activity, with stable or dynamic representations, respectively. To test these hypotheses, we recorded neuronal activity in the orbitofrontal and anterior cingulate cortex of two monkeys performing a valuation task. We found that features of all hypotheses were simultaneously present in prefrontal activity, and no single hypothesis was exclusively supported. Instead, mixed dynamics supported robust, time invariant value representations while also encoding the information in a temporally specific manner. We suggest that this hybrid coding is a critical mechanism supporting flexible cognitive abilities.},
  keywords = {decision-making,neural coding,orbitofrontal cortex,prefrontal cortex,value,working memory},
  file = {/Users/daniekru/Zotero/storage/VWNML2HH/Enel et al. - 2020 - Stable and dynamic representations of value in the prefrontal cortex.pdf}
}

@misc{esnaola-acebesBumpAttractorDynamics2021,
  title = {Bump Attractor Dynamics Underlying Stimulus Integration in Perceptual Estimation Tasks},
  author = {{Esnaola-Acebes}, Jose M. and Roxin, Alex and Wimmer, Klaus},
  year = {2021},
  month = mar,
  doi = {10.1101/2021.03.15.434192},
  urldate = {2024-05-15},
  abstract = {Perceptual decision and continuous stimulus estimation tasks involve making judgments based on accumulated sensory evidence. Network models of evidence integration usually rely on competition between neural populations each encoding a discrete categorical choice. By design, these models do not maintain information of the integrated stimulus (e.g. the average stimulus direction in degrees) that is necessary for a continuous perceptual judgement. Here, we show that the continuous ring attractor network can integrate a stimulus feature such as orientation and track the stimulus average in the phase of its activity bump. We reduced the network dynamics of the ring model to a two-dimensional equation for the amplitude and the phase of the bump. Interestingly, these reduced equations are nearly identical to an optimal integration process for computing the running average of the stimulus orientation. They differ only in the intrinsic dynamics of the amplitude, which affects the temporal weighting of the sensory evidence. Whether the network shows early (primacy), uniform or late (recency) weighting depends on the relative strength of sensory stimuli compared to the amplitude of the bump and on the initial state of the network. The specific relation between the internal network dynamics and the sensory inputs can be modulated by changing a single parameter of the model, the global excitatory drive. We show that this can account for the heterogeneity of temporal weighting profiles observed in humans integrating a stream of oriented stimulus frames. Our findings point to continuous attractor dynamics as a plausible mechanism underlying stimulus integration in perceptual estimation tasks.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/4WLPW3CH/Esnaola-Acebes et al. - 2021 - Bump attractor dynamics underlying stimulus integr.pdf}
}

@incollection{faisalNoiseNeuronsOther2012,
  title = {Noise in {{Neurons}} and {{Other Constraints}}},
  booktitle = {Computational {{Systems Neurobiology}}},
  author = {Faisal, A. Aldo},
  editor = {Le Nov{\`e}re, N.},
  year = {2012},
  pages = {227--257},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-007-3858-4_8},
  urldate = {2024-12-10},
  abstract = {How do the properties of signalling molecules constrain the structure and function biological networks such as those of our brain? Here we focus on the action potential, the fundamental electrical signal of the brain, because malfunction of the action potential causes many neurological conditions. The action potential is mediated by the concerted action of voltagegated ion channels and relating the properties of these signalling molecules to the properties of neurons at the systems level is essential for biomedical brain research, as minor variations in properties of a neurons individual component, can have large, pathological effects on the physiology of the whole nervous system and the behaviour it generates. This approach is very complex and requires us to discuss computational methods that can span across many levels of biological organization, from single signalling proteins to the organization of the entire nervous system, and encompassing time scales from milliseconds to hours.Within this methodical framework, we will focus on how the properties of voltagegated ion channels relate to the functional and structural requirements of axonal signalling and the engineering design principles of neurons and their axons (nerve fibres). This is important, not only because axons are the essential wires that allow information transmission between neurons, but also because they play a crucial in neural computation itself.},
  isbn = {978-94-007-3858-4},
  langid = {english},
  keywords = {Axon Diameter,Channel Noise,Squid Giant Axon,Stochastic Simulation,Unmyelinated Axon},
  file = {/Users/daniekru/Zotero/storage/UNC8ZGRI/Faisal - 2012 - Noise in Neurons and Other Constraints.pdf}
}

@inproceedings{fetteShortTermMemory2005,
  title = {Short {{Term Memory}} and {{Pattern Matching}} with {{Simple Echo State Networks}}},
  booktitle = {Artificial {{Neural Networks}}: {{Biological Inspirations}} -- {{ICANN}} 2005},
  author = {Fette, Georg and Eggert, Julian},
  editor = {Duch, W{\l}odzis{\l}aw and Kacprzyk, Janusz and Oja, Erkki and Zadro{\.z}ny, S{\l}awomir},
  year = {2005},
  pages = {13--18},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11550822_3},
  abstract = {Two recently proposed approaches to recognize temporal patterns have been proposed by J{\"a}ger with the so called Echo State Network (ESN) and by Maass with the so called Liquid State Machine (LSM). The ESN approach assumes a sort of ``black-box'' operability of the networks and claims a broad applicability to several different problems using the same principle. Here we propose a simplified version of ESNs which we call Simple Echo State Network (SESN) which exhibits good results in memory capacity and pattern matching tasks and which allows a better understanding of the capabilities and restrictions of ESNs.},
  isbn = {978-3-540-28754-4},
  langid = {english},
  keywords = {Hide Layer,Hide Unit,Input Weight,Memory Capacity,Output Weight},
  file = {/Users/daniekru/Zotero/storage/TZPPW836/Fette and Eggert - 2005 - Short Term Memory and Pattern Matching with Simple.pdf}
}

@article{frankAnatomyDecisionStriatoorbitofrontal2006,
  title = {Anatomy of a Decision: {{Striato-orbitofrontal}} Interactions in Reinforcement Learning, Decision Making, and Reversal.},
  shorttitle = {Anatomy of a Decision},
  author = {Frank, Michael J. and Claus, Eric D.},
  year = {2006},
  month = apr,
  journal = {Psychological Review},
  volume = {113},
  number = {2},
  pages = {300--326},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.113.2.300},
  urldate = {2024-05-14},
  abstract = {The authors explore the division of labor between the basal ganglia-- dopamine (BG-DA) system and the orbitofrontal cortex (OFC) in decision making. They show that a primitive neural network model of the BG-DA system slowly learns to make decisions on the basis of the relative probability of rewards but is not as sensitive to (a) recency or (b) the value of specific rewards. An augmented model that explores BG-OFC interactions is more successful at estimating the true expected value of decisions and is faster at switching behavior when reinforcement contingencies change. In the augmented model, OFC areas exert top-down control on the BG and premotor areas by representing reinforcement magnitudes in working memory. The model successfully captures patterns of behavior resulting from OFC damage in decision making, reversal learning, and devaluation paradigms and makes additional predictions for the underlying source of these deficits.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/WNJC8TGP/Frank and Claus - 2006 - Anatomy of a decision Striato-orbitofrontal inter.pdf}
}

@article{frankInteractionsFrontalCortex2001,
  title = {Interactions between Frontal Cortex and Basal Ganglia in Working Memory: {{A}} Computational Model},
  shorttitle = {Interactions between Frontal Cortex and Basal Ganglia in Working Memory},
  author = {Frank, M. J. and Loughry, B. and O'Reilly, R. C.},
  year = {2001},
  month = jun,
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  volume = {1},
  number = {2},
  pages = {137--160},
  issn = {1530-7026, 1531-135X},
  doi = {10.3758/CABN.1.2.137},
  urldate = {2024-05-14},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/V5R9YYYX/Frank et al. - 2001 - Interactions between frontal cortex and basal gang.pdf}
}

@article{fremauxReinforcementLearningUsing2013,
  title = {Reinforcement {{Learning Using}} a {{Continuous Time Actor-Critic Framework}} with {{Spiking Neurons}}},
  author = {Fr{\'e}maux, Nicolas and Sprekeler, Henning and Gerstner, Wulfram},
  year = {2013},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {4},
  pages = {e1003024},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003024},
  urldate = {2024-05-14},
  abstract = {Animals repeat rewarded behaviors, but the physiological basis of reward-based learning has only been partially elucidated. On one hand, experimental evidence shows that the neuromodulator dopamine carries information about rewards and affects synaptic plasticity. On the other hand, the theory of reinforcement learning provides a framework for reward-based learning. Recent models of reward-modulated spike-timing-dependent plasticity have made first steps towards bridging the gap between the two approaches, but faced two problems. First, reinforcement learning is typically formulated in a discrete framework, ill-adapted to the description of natural situations. Second, biologically plausible models of reward-modulated spike-timing-dependent plasticity require precise calculation of the reward prediction error, yet it remains to be shown how this can be computed by neurons. Here we propose a solution to these problems by extending the continuous temporal difference (TD) learning of Doya (2000) to the case of spiking neurons in an actor-critic network operating in continuous time, and with continuous state and action representations. In our model, the critic learns to predict expected future rewards in real time. Its activity, together with actual rewards, conditions the delivery of a neuromodulatory TD signal to itself and to the actor, which is responsible for action choice. In simulations, we show that such an architecture can solve a Morris water-maze-like navigation task, in a number of trials consistent with reported animal performance. We also use our model to solve the acrobot and the cartpole problems, two complex motor control tasks. Our model provides a plausible way of computing reward prediction error in the brain. Moreover, the analytically derived learning rule is consistent with experimental evidence for dopamine-modulated spike-timing-dependent plasticity.},
  langid = {english},
  keywords = {Action potentials,Coding mechanisms,Decision making,Dopamine,Learning,Neuronal tuning,Neurons,Synapses},
  file = {/Users/daniekru/Zotero/storage/DIKWLDBA/Frémaux et al. - 2013 - Reinforcement Learning Using a Continuous Time Act.pdf}
}

@article{funahashiPrefrontalContributionDecisionMaking2017,
  title = {Prefrontal {{Contribution}} to {{Decision-Making}} under {{Free-Choice Conditions}}},
  author = {Funahashi, Shintaro},
  year = {2017},
  month = jul,
  journal = {Frontiers in Neuroscience},
  volume = {11},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2017.00431},
  urldate = {2024-11-27},
  abstract = {{$<$}p{$>$}Executive function is thought to be the coordinated operation of multiple neural processes and allows to accomplish a current goal flexibly. The most important function of the prefrontal cortex is the executive function. Among a variety of executive functions in which the prefrontal cortex participates, decision-making is one of the most important. Although the prefrontal contribution to decision-making has been examined using a variety of behavioral tasks, recent studies using fMRI have shown that the prefrontal cortex participates in decision-making under free-choice conditions. Since decision-making under free-choice conditions represents the very first stage for any kind of decision-making process, it is important that we understand its neural mechanism. Although few studies have examined this issue while a monkey performed a free-choice task, those studies showed that, when the monkey made a decision to subsequently choose one particular option, prefrontal neurons showing selectivity to that option exhibited transient activation just {$<$}italic{$>$}before{$<$}/italic{$>$} presentation of the imperative cue. Further studies have suggested that this transient increase is caused by the irregular fluctuation of spontaneous firing just {$<$}italic{$>$}before{$<$}/italic{$>$} cue presentation, which enhances the response to the cue and biases the strength of the neuron's selectivity to the option. In addition, this biasing effect was observed only in neurons that exhibited sustained delay-period activity, indicating that this biasing effect not only influences the animal's decision for an upcoming choice, but also is linked to working memory mechanisms in the prefrontal cortex.{$<$}/p{$>$}},
  langid = {english},
  keywords = {choice-predictive activity,decision-making,free-choice,Prefrontal Cortex,Spontaneous fluctuation},
  file = {/Users/daniekru/Zotero/storage/T8HE6XRW/Funahashi - 2017 - Prefrontal Contribution to Decision-Making under Free-Choice Conditions.pdf}
}

@article{gallinaroSynapticWeightsThat2023,
  title = {Synaptic Weights That Correlate with Presynaptic Selectivity Increase Decoding Performance},
  author = {Gallinaro, J{\'u}lia V. and Scholl, Benjamin and Clopath, Claudia},
  year = {2023},
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {19},
  number = {8},
  pages = {e1011362},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1011362},
  urldate = {2024-12-09},
  abstract = {The activity of neurons in the visual cortex is often characterized by tuning curves, which are thought to be shaped by Hebbian plasticity during development and sensory experience. This leads to the prediction that neural circuits should be organized such that neurons with similar functional preference are connected with stronger weights. In support of this idea, previous experimental and theoretical work have provided evidence for a model of the visual cortex characterized by such functional subnetworks. A recent experimental study, however, have found that the postsynaptic preferred stimulus was defined by the total number of spines activated by a given stimulus and independent of their individual strength. While this result might seem to contradict previous literature, there are many factors that define how a given synaptic input influences postsynaptic selectivity. Here, we designed a computational model in which postsynaptic functional preference is defined by the number of inputs activated by a given stimulus. Using a plasticity rule where synaptic weights tend to correlate with presynaptic selectivity, and is independent of functional-similarity between pre- and postsynaptic activity, we find that this model can be used to decode presented stimuli in a manner that is comparable to maximum likelihood inference., Brains are composed of complex networks, with communication taking place along synaptic connections between neurons. These connections can change and adapt, a process we call ``plasticity''. However, the specific rules that dictate these changes remain largely unknown. In our visual system, which is responsible for vision and perception, it is primarily thought that connections get stronger between neurons exhibiting similar activity, also known as `Hebbian plasticity'. A recent study revealed results that seemed to contradict this idea, showing a strength in numbers of synapses rather than strength. Prompted by these findings, we developed a computational model with a hypothesis about how these changes could occur. We discovered that this new model, with a plasticity mechanism based on presynaptic activity, could capture experimental findings and lead to benefits in population decoding. Our model doesn't necessarily contradict a Hebbian model, but rather, likely co-exists with it.},
  pmcid = {PMC10434873},
  pmid = {37549193},
  file = {/Users/daniekru/Zotero/storage/Z3CGADG6/Gallinaro et al. - 2023 - Synaptic weights that correlate with presynaptic selectivity increase decoding performance.pdf}
}

@misc{garivierUpperConfidenceBoundPolicies2008,
  title = {On {{Upper-Confidence Bound Policies}} for {{Non-Stationary Bandit Problems}}},
  author = {Garivier, Aur{\'e}lien and Moulines, Eric},
  year = {2008},
  month = may,
  number = {arXiv:0805.3415},
  eprint = {0805.3415},
  primaryclass = {math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.0805.3415},
  urldate = {2024-03-25},
  abstract = {Multi-armed bandit problems are considered as a paradigm of the trade-off between exploring the environment to find profitable actions and exploiting what is already known. In the stationary case, the distributions of the rewards do not change in time, Upper-Confidence Bound (UCB) policies have been shown to be rate optimal. A challenging variant of the MABP is the non-stationary bandit problem where the gambler must decide which arm to play while facing the possibility of a changing environment. In this paper, we consider the situation where the distributions of rewards remain constant over epochs and change at unknown time instants. We analyze two algorithms: the discounted UCB and the sliding-window UCB. We establish for these two algorithms an upper-bound for the expected regret by upper-bounding the expectation of the number of times a suboptimal arm is played. For that purpose, we derive a Hoeffding type inequality for self normalized deviations with a random number of summands. We establish a lower-bound for the regret in presence of abrupt changes in the arms reward distributions. We show that the discounted UCB and the sliding-window UCB both match the lower-bound up to a logarithmic factor.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/daniekru/Zotero/storage/XPLJKI2D/Garivier and Moulines - 2008 - On Upper-Confidence Bound Policies for Non-Station.pdf;/Users/daniekru/Zotero/storage/65CD88JS/0805.html}
}

@incollection{gerstnerChapter12Framework2001,
  title = {Chapter 12 {{A}} Framework for Spiking Neuron Models: {{The}} Spike Response Model},
  shorttitle = {Chapter 12 {{A}} Framework for Spiking Neuron Models},
  booktitle = {Handbook of {{Biological Physics}}},
  author = {Gerstner, W.},
  editor = {Moss, F. and Gielen, S.},
  year = {2001},
  month = jan,
  series = {Neuro-{{Informatics}} and {{Neural Modelling}}},
  volume = {4},
  pages = {469--516},
  publisher = {North-Holland},
  doi = {10.1016/S1383-8121(01)80015-4},
  urldate = {2024-05-13},
  abstract = {This chapter addresses both issues from the systematic point of view of a response kernel expansion. Spike generation in the Hodgkin--Huxley model can be reproduced to a high degree of accuracy by a single-variable threshold model [19]. The problem of spatial structure is studied for a multicompartmental integrate-and-fire model with a passive dendritic tree and active currents at the soma. In this case, the model dynamics can be solved and systematically reduced to a single-variable model with response kernels. The chapter reviews the standard Hodgkin--Huxley model. The four differential equations of Hodgkin and Huxley give an accurate description of neuronal spiking in the giant axon of the squid. The chapter proposes a method based on spike response kernels and provides a biologically transparent description of the essential effects during spiking. The chapter discusses the spike response model (SRM), derived from the Hodgkin--Huxley model by the SRM, can reproduce up to 90\% of the spike times of the Hodkgin--Huxley model correctly.},
  file = {/Users/daniekru/Zotero/storage/3N3QDFLA/Gerstner - 2001 - Chapter 12 A framework for spiking neuron models .pdf;/Users/daniekru/Zotero/storage/HQVYDAW9/S1383812101800154.html}
}

@article{gittinsBanditProcessesDynamic1979,
  title = {Bandit {{Processes}} and {{Dynamic Allocation Indices}}},
  author = {Gittins, J. C.},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {41},
  number = {2},
  eprint = {2985029},
  eprinttype = {jstor},
  pages = {148--177},
  abstract = {The paperaimsto givea unifiedaccountofthecentracl onceptsin recentworkon banditprocessesand dynamicallocationindices;to showhow thesereducesome previouslyintractablperoblemsto theproblemof calculatingsuchindices;and to describehow thesecalculationsmay be carriedout. Applicationsto stochastic schedulings,equentialclinicaltrialsand a class of searchproblemsare discussed.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FYGDZ4DT/Gittins - 1979 - Bandit Processes and Dynamic Allocation Indices.pdf}
}

@article{gordleevaModelingWorkingMemory2021,
  title = {Modeling {{Working Memory}} in a {{Spiking Neuron Network Accompanied}} by {{Astrocytes}}},
  author = {Gordleeva, Susanna Yu and Tsybina, Yuliya A. and Krivonosov, Mikhail I. and Ivanchenko, Mikhail V. and Zaikin, Alexey A. and Kazantsev, Victor B. and Gorban, Alexander N.},
  year = {2021},
  month = mar,
  journal = {Frontiers in Cellular Neuroscience},
  volume = {15},
  publisher = {Frontiers},
  issn = {1662-5102},
  doi = {10.3389/fncel.2021.631485},
  urldate = {2024-05-03},
  abstract = {We propose a novel biologically plausible computational model of working memory (WM) implemented by the spiking neuron network (SNN) interacting with a network of astrocytes. SNN is modelled by the synaptically coupled Izhikevich neurons with a non-specific architecture connection topology. Astrocytes generating calcium signals are connected by local gap junction diffusive couplings and interact with neurons by chemicals diffused in the extracellular space. Calcium elevations occur in response to the increased concentration of the neurotransmitter released by spiking neurons when a group of them fire coherently. In turn, gliotransmitters are released by activated astrocytes modulating the strengths of synaptic connections in the corresponding neuronal group. Input information is encoded as two-dimensional patterns of short applied current pulses stimulating neurons. The output is taken from frequencies of transient discharges of corresponding neurons. We show how a set of information patterns with quite significant overlapping areas can be uploaded into the neuron-astrocyte network and stored for several seconds. Information retrieval is organised by the application of a cue pattern representing the one from the memory set distorted by noise. We found that successful retrieval with the level of the correlation between the recalled pattern and ideal pattern exceeding 90\% is possible for multi-item WM task. Having analysed the dynamical mechanism of WM formation, we discovered that astrocytes operating at a time scale of a dozen of seconds can successfully store traces of neuronal activations corresponding to information patterns. In the retrieval stage, the astrocytic network selectively modulates synaptic connections in SNN leading to the successful recall. Information and dynamical characteristics of the proposed WM model agrees with classical concepts and other WM models.},
  langid = {english},
  keywords = {astrocyte,delayed activity,neuron-astrocyte interaction,Spiking Neural network,working memory},
  file = {/Users/daniekru/Zotero/storage/CLGHGHEZ/Gordleeva et al. - 2021 - Modeling Working Memory in a Spiking Neuron Networ.pdf}
}

@incollection{gotoPrefrontalCorticalSynaptic2007,
  title = {Prefrontal {{Cortical Synaptic Plasticity}}: {{The Roles}} of {{Dopamine}} and {{Implication}} for {{Schizophrenia}}},
  shorttitle = {Prefrontal {{Cortical Synaptic Plasticity}}},
  booktitle = {Monoaminergic {{Modulation}} of {{Cortical Excitability}}},
  author = {Goto, Yukiori and Otani, Satoru},
  editor = {Tseng, Kuei-Yuan and Atzori, Marco},
  year = {2007},
  pages = {165--174},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-0-387-72256-6_10},
  urldate = {2024-04-26},
  abstract = {The prefrontal cortex (PFC) is central in mediating executive functions in goaldirected behavior, for which proper dopamine (DA) actions of information processing modulation is essential in this area. It is now evident that, as in the case of the hippocampus, the PFC undergoes neuronal adaptation processes in its networks with induction of synaptic plasticity such as long-term potentiation (LTP) and short-term potentiation (STP). A prominent characteristic of synaptic plasticity in the PFC is that its induction mechanisms involve DA as an essential modulatory molecule. As such, DA-dependent plastic changes occurring in PFC network have important roles for PFC-mediated cognitive functions. Nevertheless, little attempt has been made to characterize the nature of PFC neuronal adaptation by synaptic plasticity, given that the PFC is thought to be the area of temporary storage and manipulation of information, known as working memory. However, accumulating evidences now indicate that the functions of the PFC cannot be fully explained just as the region of an online representation and handling of information. Importance of DA-dependent synaptic plasticity is further encouraged by possible disruption of synaptic plasticity mechanism in the PFC in psychiatric disorders such as schizophrenia, drug addiction, and depression.},
  isbn = {978-0-387-72256-6},
  langid = {english},
  keywords = {Prefrontal Cortex,Synaptic Plasticity,Tetanic Stimulation,Trace Fear Conditioning,Ventral Tegmental Area},
  file = {/Users/daniekru/Zotero/storage/5PNVFBP5/Goto and Otani - 2007 - Prefrontal Cortical Synaptic Plasticity The Roles.pdf}
}

@article{gurneyNewFrameworkCorticoStriatal2015,
  title = {A {{New Framework}} for {{Cortico-Striatal Plasticity}}: {{Behavioural Theory Meets In Vitro Data}} at the {{Reinforcement-Action Interface}}},
  shorttitle = {A {{New Framework}} for {{Cortico-Striatal Plasticity}}},
  author = {Gurney, Kevin N. and Humphries, Mark D. and Redgrave, Peter},
  editor = {Dayan, Peter},
  year = {2015},
  month = jan,
  journal = {PLoS Biology},
  volume = {13},
  number = {1},
  pages = {e1002034},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002034},
  urldate = {2024-05-06},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/Y7GF8RN2/Gurney et al. - 2015 - A New Framework for Cortico-Striatal Plasticity B.pdf}
}

@article{harleAlteredStatisticalLearning2015,
  title = {Altered {{Statistical Learning}} and {{Decision-Making}} in {{Methamphetamine Dependence}}: {{Evidence}} from a {{Two-Armed Bandit Task}}},
  shorttitle = {Altered {{Statistical Learning}} and {{Decision-Making}} in {{Methamphetamine Dependence}}},
  author = {Harl{\'e}, Katia M. and Zhang, Shunan and Schiff, Max and Mackey, Scott and Paulus, Martin P. and Yu, Angela J.},
  year = {2015},
  month = dec,
  journal = {Frontiers in Psychology},
  volume = {6},
  pages = {1910},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.01910},
  urldate = {2024-04-29},
  abstract = {Understanding how humans weigh long-term and short-term goals is important for both basic cognitive science and clinical neuroscience, as substance users need to balance the appeal of an immediate high vs. the long-term goal of sobriety. We use a computational model to identify learning and decision-making abnormalities in methamphetamine-dependent individuals (MDI, n = 16) vs. healthy control subjects (HCS, n = 16), in a two-armed bandit task. In this task, subjects repeatedly choose between two arms with fixed but unknown reward rates. Each choice not only yields potential immediate reward but also information useful for long-term reward accumulation, thus pitting exploration against exploitation. We formalize the task as comprising a learning component, the updating of estimated reward rates based on ongoing observations, and a decision-making component, the choice among options based on current beliefs and uncertainties about reward rates. We model the learning component as iterative Bayesian inference (the Dynamic Belief Model), and the decision component using five competing decision policies: Win-stay/Lose-shift (WSLS), {$\varepsilon$}-Greedy, {$\tau$}-Switch, Softmax, Knowledge Gradient. HCS and MDI significantly differ in how they learn about reward rates and use them to make decisions. HCS learn from past observations but weigh recent data more, and their decision policy is best fit as Softmax. MDI are more likely to follow the simple learning-independent policy of WSLS, and among MDI best fit by Softmax, they have more pessimistic prior beliefs about reward rates and are less likely to choose the option estimated to be most rewarding. Neurally, MDI's tendency to avoid the most rewarding option is associated with a lower gray matter volume of the thalamic dorsal lateral nucleus. More broadly, our work illustrates the ability of our computational framework to help reveal subtle learning and decision-making abnormalities in substance use.},
  pmcid = {PMC4683191},
  pmid = {26733906},
  file = {/Users/daniekru/Zotero/storage/WH6J7IYU/Harlé et al. - 2015 - Altered Statistical Learning and Decision-Making i.pdf}
}

@article{herdNeuralNetworkModel2014,
  title = {A Neural Network Model of Individual Differences in Task Switching Abilities},
  author = {Herd, Seth A. and O'Reilly, Randall C. and Hazy, Tom E. and Chatham, Christopher H. and Brant, Angela M. and Friedman, Naomi P.},
  year = {2014},
  month = sep,
  journal = {Neuropsychologia},
  volume = {62},
  pages = {375--389},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2014.04.014},
  urldate = {2024-05-14},
  abstract = {We use a biologically grounded neural network model to investigate the brain mechanisms underlying individual differences specific to the selection and instantiation of representations that exert cognitive control in task switching. Existing computational models of task switching do not focus on individual differences and so cannot explain why task switching abilities are separable from other executive function (EF) abilities (such as response inhibition). We explore hypotheses regarding neural mechanisms underlying the ``Shifting-Specific'' and ``Common EF'' components of EF proposed in the Unity/Diversity model (Miyake \& Friedman, 2012) and similar components in related theoretical frameworks. We do so by adapting a well-developed neural network model of working memory (Prefrontal cortex, Basal ganglia Working Memory or PBWM; Hazy, Frank, \& O'Reilly, 2007) to task switching and the Stroop task, and comparing its behavior on those tasks under a variety of individual difference manipulations. Results are consistent with the hypotheses that variation specific to task switching (i.e., Shifting-Specific) may be related to uncontrolled, automatic persistence of goal representations, whereas variation general to multiple EFs (i.e., Common EF) may be related to the strength of PFC representations and their effect on processing in the remainder of the cognitive system. Moreover, increasing signal to noise ratio in PFC, theoretically tied to levels of tonic dopamine and a genetic polymorphism in the COMT gene, reduced Stroop interference but increased switch costs. This stability--flexibility tradeoff provides an explanation for why these two EF components sometimes show opposing correlations with other variables such as attention problems and self-restraint.},
  keywords = {Computational model,Executive control,Genetics,Set shifting},
  file = {/Users/daniekru/Zotero/storage/R7P3GEDX/Herd et al. - 2014 - A neural network model of individual differences i.pdf;/Users/daniekru/Zotero/storage/XR5Q2HMI/S0028393214001365.html}
}

@article{houstonMatchingBehavioursRewards2021,
  title = {Matching {{Behaviours}} and {{Rewards}}},
  author = {Houston, Alasdair I. and Trimmer, Pete C. and McNamara, John M.},
  year = {2021},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {25},
  number = {5},
  pages = {403--415},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2021.01.011},
  urldate = {2024-05-07},
  langid = {english},
  pmid = {33612384},
  keywords = {bout structure,input matching,matching law,optimal behaviour,probability matching,switching},
  file = {/Users/daniekru/Zotero/storage/YS788STY/Houston et al. - 2021 - Matching Behaviours and Rewards.pdf}
}

@article{hulmeEmergingRolesMetaplasticity2013,
  title = {Emerging Roles of Metaplasticity in Behaviour and Disease},
  author = {Hulme, Sarah R. and Jones, Owen D. and Abraham, Wickliffe C.},
  year = {2013},
  month = jun,
  journal = {Trends in Neurosciences},
  volume = {36},
  number = {6},
  pages = {353--362},
  publisher = {Elsevier},
  issn = {0166-2236, 1878-108X},
  doi = {10.1016/j.tins.2013.03.007},
  urldate = {2024-12-12},
  langid = {english},
  pmid = {23602195},
  keywords = {fEPSP,field excitatory postsynaptic potentials.}
}

@article{igelCovarianceMatrixAdaptation2007,
  title = {Covariance {{Matrix Adaptation}} for {{Multi-objective Optimization}}},
  author = {Igel, Christian and Hansen, Nikolaus and Roth, Stefan},
  year = {2007},
  month = mar,
  journal = {Evolutionary Computation},
  volume = {15},
  number = {1},
  pages = {1--28},
  issn = {1063-6560},
  doi = {10.1162/evco.2007.15.1.1},
  urldate = {2024-12-05},
  abstract = {The covariancematrix adaptation evolution strategy (CMA-ES) is one of themost powerful evolutionary algorithms for real-valued single-objective optimization. In this paper, we develop a variant of the CMA-ES for multi-objective optimization (MOO). We first introduce a single-objective, elitist CMA-ES using plus-selection and step size control based on a success rule. This algorithm is compared to the standard CMA-ES. The elitist CMA-ES turns out to be slightly faster on unimodal functions, but is more prone to getting stuck in sub-optimal local minima. In the new multi-objective CMAES (MO-CMA-ES) a population of individuals that adapt their search strategy as in the elitist CMA-ES is maintained. These are subject to multi-objective selection. The selection is based on non-dominated sorting using either the crowding-distance or the contributing hypervolume as second sorting criterion. Both the elitist single-objective CMA-ES and the MO-CMA-ES inherit important invariance properties, in particular invariance against rotation of the search space, from the original CMA-ES. The benefits of the new MO-CMA-ES in comparison to the well-known NSGA-II and to NSDE, a multi-objective differential evolution algorithm, are experimentally shown.},
  file = {/Users/daniekru/Zotero/storage/AY6BZSM6/Covariance-Matrix-Adaptation-for-Multi-objective.html}
}

@article{iigayaAdaptiveLearningDecisionmaking2016,
  title = {Adaptive Learning and Decision-Making under Uncertainty by Metaplastic Synapses Guided by a Surprise Detection System},
  author = {Iigaya, Kiyohito},
  editor = {Uchida, Naoshige},
  year = {2016},
  month = aug,
  journal = {eLife},
  volume = {5},
  pages = {e18073},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.18073},
  urldate = {2024-12-02},
  abstract = {Recent experiments have shown that animals and humans have a remarkable ability to adapt their learning rate according to the volatility of the environment. Yet the neural mechanism responsible for such adaptive learning has remained unclear. To fill this gap, we investigated a biophysically inspired, metaplastic synaptic model within the context of a well-studied decision-making network, in which synapses can change their rate of plasticity in addition to their efficacy according to a reward-based learning rule. We found that our model, which assumes that synaptic plasticity is guided by a novel surprise detection system, captures a wide range of key experimental findings and performs as well as a Bayes optimal model, with remarkably little parameter tuning. Our results further demonstrate the computational power of synaptic plasticity, and provide insights into the circuit-level computation which underlies adaptive decision-making.},
  keywords = {decision-making,learning,memory consolidation,plasticity,surprise,synapse},
  file = {/Users/daniekru/Zotero/storage/XSMBK22P/Iigaya - 2016 - Adaptive learning and decision-making under uncertainty by metaplastic synapses guided by a surprise.pdf}
}

@article{inglisModulationDopamineAdaptive2021,
  title = {Modulation of {{Dopamine}} for {{Adaptive Learning}}: {{A Neurocomputational Model}}},
  shorttitle = {Modulation of {{Dopamine}} for {{Adaptive Learning}}},
  author = {Inglis, Jeffrey B. and Valentin, Vivian V. and Ashby, F. Gregory},
  year = {2021},
  month = mar,
  journal = {Computational brain \& behavior},
  volume = {4},
  number = {1},
  pages = {34--52},
  issn = {2522-087X},
  doi = {10.1007/s42113-020-00083-x},
  urldate = {2024-12-02},
  abstract = {There have been many proposals that learning rates in the brain are adaptive, in the sense that they increase or decrease depending on environmental conditions. The majority of these models are abstract and make no attempt to describe the neural circuitry that implements the proposed computations. This article describes a biologically detailed computational model that overcomes this shortcoming. Specifically, we propose a neural circuit that implements adaptive learning rates by modulating the gain on the dopamine response to reward prediction errors, and we model activity within this circuit at the level of spiking neurons. The model generates a dopamine signal that depends on the size of the tonically active dopamine neuron population and the phasic spike rate. The model was tested successfully against results from two single-neuron recording studies and a fast-scan cyclic voltammetry study. We conclude by discussing the general applicability of the model to dopamine mediated tasks that transcend the experimental phenomena it was initially designed to address.},
  pmcid = {PMC8210637},
  pmid = {34151186},
  file = {/Users/daniekru/Zotero/storage/9ZNF7VY3/Inglis et al. - 2021 - Modulation of Dopamine for Adaptive Learning A Neurocomputational Model.pdf}
}

@article{jayPlasticityHippocampalPrefrontal2004,
  title = {Plasticity at Hippocampal to Prefrontal Cortex Synapses Is Impaired by Loss of Dopamine and Stress: Importance for Psychiatric Diseases},
  shorttitle = {Plasticity at Hippocampal to Prefrontal Cortex Synapses Is Impaired by Loss of Dopamine and Stress},
  author = {Jay, Th{\'e}r{\`e}se M. and Rocher, Cyril and Hotte, Ma{\"i}te and Naudon, Laurent and Gurden, Hirac and Spedding, Michael},
  year = {2004},
  journal = {Neurotoxicity Research},
  volume = {6},
  number = {3},
  pages = {233--244},
  issn = {1029-8428},
  doi = {10.1007/BF03033225},
  abstract = {The direct hippocampal to prefrontal cortex pathway and its changes in synaptic plasticity is a useful framework for investigating the functional operations of hippocampal-prefrontal cortex communication in cognitive functions. Synapses on this pathway are modifiable and synaptic strength can be turned up or down depending on specific patterns of activity in the pathway. The objective of this review will be to summarize the different studies carried out on this topic including very recent data and to underline the importance of animal models for the development of new and effective medications in psychiatric diseases. We have shown that long-term potentiation (LTP) of hippocampal-prefrontal synapses is driven by the level of mesocortical dopaminergic (DA) activity and more recently that stress is also an environmental determinant of LTP at these cortical synapses. Stimulation of the ventral tegmental area at a frequency known to evoke DA overflow in the prefrontal cortex produces a long-lasting enhancement of the magnitude of hippocampal-prefrontal cortex LTP whereas a depletion of cortical DA levels generates a dramatic decrease in this LTP. Moreover, hippocampal stimulation induces a transient but significant increase in DA release in the prefrontal cortex and an optimal level of D1 receptor activation is essential for LTP expression. We recently investigated the impact of stress on hippocampal-prefrontal LTP and demonstrated that exposure to an acute stress causes a remarkable and long-lasting inhibition of LTP. Furthermore, we demonstrated that tianeptine, an antidepressant which has a unique mode of action, and clozapine an atypical antipsychotic when administered at doses normally used in human testing are able to reverse the impairment in LTP. Stressful life events have a substantial causal association with psychiatric disorders like schizophrenia and depression and recent imaging studies have shown an important role of the limbic-cortical circuit in the pathophysiology of these illnesses. Therefore, we proposed that agents capable of reversing the impairment of plasticity at hippocampal to prefrontal cortex synapses have the potential of becoming new therapeutic classes of antidepressant or antipsychotic drugs.},
  langid = {english},
  pmid = {15325962},
  keywords = {Animals,Dopamine,Hippocampus,Humans,Neuronal Plasticity,Prefrontal Cortex,Psychotropic Drugs,Receptors Dopamine D1,Stress Physiological,Synapses,Synaptic Transmission}
}

@article{jepkoechEffectAdaptiveLearning2021,
  title = {The {{Effect}} of {{Adaptive Learning Rate}} on the {{Accuracy}} of {{Neural Networks}}},
  author = {Jepkoech, Jennifer and Mugo, David Muchangi and Kenduiywo, Benson K. and Too, Edna Chebet},
  year = {2021},
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {12},
  number = {8},
  publisher = {{Science and Information (SAI) Organization Limited}},
  address = {West Yorkshire, United Kingdom},
  issn = {2158107X},
  doi = {10.14569/IJACSA.2021.0120885},
  urldate = {2024-12-09},
  abstract = {Learning rates in gradient descent algorithms have significant effects especially on the accuracy of a Capsule Neural Network (CNN). Choosing an appropriate learning rate is still an issue to date. Many developers still have a problem in selecting a learning rate for CNN leading to low accuracies in classification. This gap motivated this study to assess the effect of learning rate on the accuracy of a developed (CNN). There are no predefined learning rates in CNN and therefore it is hard for researchers to know what learning rate will give good results. This work, therefore, focused on assessing the effect of learning rate on the accuracy of a CNN by using different learning rates and observing the best performance. The contribution of this work is to give an appropriate learning rate for CNNs to improve accuracy during classification. This work has assessed the effect of different learning rates and came up with the most appropriate learning rate for CNN plant leaf disease classification. Part of the images used in this work was from the PlantVillage dataset while others were from the Nepal database. The images were pre-processed then subjected to the original CNN model for classification. When the learning rate was 0.0001, the best performance was 99.4\% on testing and 100\% on training. When the learning rate was 0.00001, the highest performance was 97\% on testing and 99.9\% on training. The lowest performance observed was 81\% accuracy on testing and 99\% on training when the learning rate was 0.001. This work observed that CNN was able to achieve the highest accuracy with a learning rate of 0.0001. The best Convolutional Neural Network accuracy observed was 98\% on testing and 100\% on training when the learning rate was 0.0001.},
  copyright = {{\copyright} 2021. This work is licensed under https://creativecommons.org/licenses/by/4.0/  (the ``License'').  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  keywords = {Accuracy,Accuracy and precision,Adaptive learning,Algorithms,Artificial neural networks,Capsule neural network,Classification,CNN,ConvNet,Convolutional neural network,Digital image processing,gradient descent,Gradient descent,Image classification,learning rate,Learning rate,Machine learning,Nepal,Nervous system,Neural network,Neural networks,Plant diseases,Training},
  file = {/Users/daniekru/Zotero/storage/PDIFPEPC/Jepkoech et al. - 2021 - The Effect of Adaptive Learning Rate on the Accuracy of Neural Networks.pdf}
}

@misc{kannanUnsupervisedSpikingNeural2023,
  title = {Unsupervised {{Spiking Neural Network Model}} of {{Prefrontal Cortex}} to Study {{Task Switching}} with {{Synaptic}} Deficiency},
  author = {Kannan, Ashwin Viswanathan and Mylavarapu, Goutam and Thomas, Johnson P.},
  year = {2023},
  month = may,
  number = {arXiv:2305.14394},
  eprint = {2305.14394},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14394},
  urldate = {2024-05-03},
  abstract = {In this study, we build a computational model of Prefrontal Cortex (PFC) using Spiking Neural Networks (SNN) to understand how neurons adapt and respond to tasks switched under short and longer duration of stimulus changes. We also explore behavioral deficits arising out of the PFC lesions by simulating lesioned states in our Spiking architecture model. Although there are some computational models of the PFC, SNN's have not been used to model them. In this study, we use SNN's having parameters close to biologically plausible values and train the model using unsupervised Spike Timing Dependent Plasticity (STDP) learning rule. Our model is based on connectionist architectures and exhibits neural phenomena like sustained activity which helps in generating short-term or working memory. We use these features to simulate lesions by deactivating synaptic pathways and record the weight adjustments of learned patterns and capture the accuracy of learning tasks in such conditions. All our experiments are trained and recorded using a real-world Fashion MNIST (FMNIST) dataset and through this work, we bridge the gap between bio-realistic models and those that perform well in pattern recognition tasks},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/daniekru/Zotero/storage/DJTHQ7S7/Kannan et al. - 2023 - Unsupervised Spiking Neural Network Model of Prefr.pdf;/Users/daniekru/Zotero/storage/MH6K7SL2/2305.html}
}

@misc{kaufmannThompsonSamplingAsymptotically2012,
  title = {Thompson {{Sampling}}: {{An Asymptotically Optimal Finite Time Analysis}}},
  shorttitle = {Thompson {{Sampling}}},
  author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'e}mi},
  year = {2012},
  month = jul,
  number = {arXiv:1205.4217},
  eprint = {1205.4217},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-23},
  abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/ZL83R67T/Kaufmann et al. - 2012 - Thompson Sampling An Asymptotically Optimal Finit.pdf}
}

@article{kennedySynapticSignalingLearning2016,
  title = {Synaptic {{Signaling}} in {{Learning}} and {{Memory}}},
  author = {Kennedy, Mary B.},
  year = {2016},
  month = feb,
  journal = {Cold Spring Harbor Perspectives in Biology},
  volume = {8},
  number = {2},
  pages = {a016824},
  issn = {1943-0264},
  doi = {10.1101/cshperspect.a016824},
  urldate = {2024-12-12},
  abstract = {Learning and memory require the formation of new neural networks in the brain. A key mechanism underlying this process is synaptic plasticity at excitatory synapses, which connect neurons into networks. Excitatory synaptic transmission happens when glutamate, the excitatory neurotransmitter, activates receptors on the postsynaptic neuron. Synaptic plasticity is a higher-level process in which the strength of excitatory synapses is altered in response to the pattern of activity at the synapse. It is initiated in the postsynaptic compartment, where the precise pattern of influx of calcium through activated glutamate receptors leads either to the addition of new receptors and enlargement of the synapse (long-term potentiation) or the removal of receptors and shrinkage of the synapse (long-term depression). Calcium/calmodulin-regulated enzymes and small GTPases collaborate to control this highly tuned mechanism., The {$\sim$}86 billion neurons of the human brain make trillions of synaptic connections. The unique plasticity of excitatory glutamatergic synapses is an essential mechanism of memory formation.},
  pmcid = {PMC4743082},
  pmid = {24379319},
  file = {/Users/daniekru/Zotero/storage/64R6AGRR/Kennedy - 2016 - Synaptic Signaling in Learning and Memory.pdf}
}

@article{kennerleyDecisionMakingReward2011a,
  title = {Decision {{Making}} and {{Reward}} in {{Frontal Cortex}}},
  author = {Kennerley, Steven W. and Walton, Mark E.},
  year = {2011},
  month = jun,
  journal = {Behavioral Neuroscience},
  volume = {125},
  number = {3},
  pages = {297--317},
  issn = {0735-7044},
  doi = {10.1037/a0023575},
  urldate = {2024-11-27},
  abstract = {Patients with damage to the prefrontal cortex (PFC)---especially the ventral and medial parts of PFC---often show a marked inability to make choices that meet their needs and goals. These decision-making impairments often reflect both a deficit in learning concerning the consequences of a choice, as well as deficits in the ability to adapt future choices based on experienced value of the current choice. Thus, areas of PFC must support some value computations that are necessary for optimal choice. However, recent frameworks of decision making have highlighted that optimal and adaptive decision making does not simply rest on a single computation, but a number of different value computations may be necessary. Using this framework as a guide, we summarize evidence from both lesion studies and single-neuron physiology for the representation of different value computations across PFC areas.},
  pmcid = {PMC3129331},
  pmid = {21534649},
  file = {/Users/daniekru/Zotero/storage/WW4F669B/Kennerley and Walton - 2011 - Decision Making and Reward in Frontal Cortex.pdf}
}

@incollection{khamassiChapter22Medial2013,
  title = {Chapter 22 - {{Medial}} Prefrontal Cortex and the Adaptive Regulation of Reinforcement Learning Parameters},
  booktitle = {Progress in {{Brain Research}}},
  author = {Khamassi, Mehdi and Enel, Pierre and Dominey, Peter Ford and Procyk, Emmanuel},
  editor = {Pammi, V. S. Chandrasekhar and Srinivasan, Narayanan},
  year = {2013},
  month = jan,
  series = {Decision {{Making}}},
  volume = {202},
  pages = {441--464},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-444-62604-2.00022-8},
  urldate = {2024-12-10},
  abstract = {Converging evidence suggest that the medial prefrontal cortex (MPFC) is involved in feedback categorization, performance monitoring, and task monitoring, and may contribute to the online regulation of reinforcement learning (RL) parameters that would affect decision-making processes in the lateral prefrontal cortex (LPFC). Previous neurophysiological experiments have shown MPFC activities encoding error likelihood, uncertainty, reward volatility, as well as neural responses categorizing different types of feedback, for instance, distinguishing between choice errors and execution errors. Rushworth and colleagues have proposed that the involvement of MPFC in tracking the volatility of the task could contribute to the regulation of one of RL parameters called the learning rate. We extend this hypothesis by proposing that MPFC could contribute to the regulation of other RL parameters such as the exploration rate and default action values in case of task shifts. Here, we analyze the sensitivity to RL parameters of behavioral performance in two monkey decision-making tasks, one with a deterministic reward schedule and the other with a stochastic one. We show that there exist optimal parameter values specific to each of these tasks, that need to be found for optimal performance and that are usually hand-tuned in computational models. In contrast, automatic online regulation of these parameters using some heuristics can help producing a good, although non-optimal, behavioral performance in each task. We finally describe our computational model of MPFC--LPFC interaction used for online regulation of the exploration rate and its application to a human--robot interaction scenario. There, unexpected uncertainties are produced by the human introducing cued task changes or by cheating. The model enables the robot to autonomously learn to reset exploration in response to such uncertain cues and events. The combined results provide concrete evidence specifying how prefrontal cortical subregions may cooperate to regulate RL parameters. It also shows how such neurophysiologically inspired mechanisms can control advanced robots in the real world. Finally, the model's learning mechanisms that were challenged in the last robotic scenario provide testable predictions on the way monkeys may learn the structure of the task during the pretraining phase of the previous laboratory experiments.},
  keywords = {computational modeling,decision making,medial prefrontal cortex,metalearning,neurorobotics,reinforcement learning},
  file = {/Users/daniekru/Zotero/storage/6G7K6A66/Khamassi et al. - 2013 - Chapter 22 - Medial prefrontal cortex and the adaptive regulation of reinforcement learning paramete.pdf;/Users/daniekru/Zotero/storage/KRWI3AVB/B9780444626042000228.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2024-12-09},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/2JZF4MHY/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/daniekru/Zotero/storage/KT9KGGYT/1412.html}
}

@article{laraRolePrefrontalCortex2015,
  title = {The {{Role}} of {{Prefrontal Cortex}} in {{Working Memory}}: {{A Mini Review}}},
  shorttitle = {The {{Role}} of {{Prefrontal Cortex}} in {{Working Memory}}},
  author = {Lara, Antonio H. and Wallis, Jonathan D.},
  year = {2015},
  month = dec,
  journal = {Frontiers in Systems Neuroscience},
  volume = {9},
  publisher = {Frontiers},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2015.00173},
  urldate = {2024-05-11},
  abstract = {A prominent account of prefrontal cortex (PFC) function is that single neurons within the PFC maintain representations of task-relevant stimuli in working memory. Evidence for this view comes from studies in which subjects hold a stimulus across a delay lasting up to several seconds. Persistent elevated activity in the PFC has been observed in animal models as well as in humans performing these tasks. This persistent activity has been interpreted as evidence for the encoding of the stimulus itself in working memory. However, recent findings have posed a challenge to this notion. A number of recent studies have examined neural data from the PFC and posterior sensory areas, both at the single neuron level in primates, and at a larger scale in humans, and have failed to find encoding of stimulus information in the PFC during tasks with a substantial working memory component. Strong stimulus related information, however, was seen in posterior sensory areas. These results suggest that delay period activity in the PFC might be better understood not as a signature of memory storage per se, but as a top down signal that influences posterior sensory areas where the actual working memory representations are maintained.},
  langid = {english},
  keywords = {Attention,Executive Function,frontoparietal network,Prefrontal Cortex,working memory},
  file = {/Users/daniekru/Zotero/storage/PKTASAWQ/Lara and Wallis - 2015 - The Role of Prefrontal Cortex in Working Memory A.pdf}
}

@article{larsenSynapsetypespecificPlasticityLocal2015,
  title = {Synapse-Type-Specific Plasticity in Local Circuits},
  author = {Larsen, Rylan S and Sj{\"o}str{\"o}m, P Jesper},
  year = {2015},
  month = dec,
  journal = {Current opinion in neurobiology},
  volume = {35},
  pages = {127--135},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2015.08.001},
  urldate = {2024-12-10},
  abstract = {Neuroscientists spent decades debating whether synaptic plasticity was presynaptically or postsynaptically expressed. It was eventually concluded that plasticity depends on many factors, including cell type. More recently, it has become increasingly clear that plasticity is regulated at an even finer grained level; it is specific to the synapse type, a concept we denote synapse-type-specific plasticity (STSP). Here, we review recent developments in the field of STSP, discussing both long-term and short-term variants and with particular emphasis on neocortical function. As there are dozens of neocortical cell types, there is a multiplicity of forms of STSP, the vast majority of which have never been explored. We argue that to understand the brain and synaptic diseases, we have to grapple with STSP.},
  pmcid = {PMC5280068},
  pmid = {26310110},
  file = {/Users/daniekru/Zotero/storage/TBKZ7FZ5/Larsen and Sjöström - 2015 - Synapse-type-specific plasticity in local circuits.pdf}
}

@article{laskowskiRoleMedialPrefrontal2016,
  title = {The Role of the Medial Prefrontal Cortex in Updating Reward Value and Avoiding Perseveration},
  author = {Laskowski, C. S. and Williams, R. J. and Martens, K. M. and Gruber, A. J. and Fisher, K. G. and Euston, D. R.},
  year = {2016},
  month = jun,
  journal = {Behavioural Brain Research},
  volume = {306},
  pages = {52--63},
  issn = {0166-4328},
  doi = {10.1016/j.bbr.2016.03.007},
  urldate = {2024-04-29},
  abstract = {The medial prefrontal cortex (mPFC) plays a major role in goal-directed behaviours, but it is unclear whether it plays a role in breaking away from a high-value reward in order to explore for better options. To address this question, we designed a novel 3-arm Bandit Task in which rats were required to choose one of three potential reward arms, each of which was associated with a different amount of food reward and time-out punishment. After a variable number of choice trials the reward locations were shuffled and animals had to disengage from the now devalued arm and explore the other options in order to optimise payout. Lesion and control groups' behaviours on the task were then analysed by fitting data with a reinforcement learning model. As expected, lesioned animals obtained less reward overall due to an inability to flexibly adapt their behaviours after a change in reward location. However, modelling results showed that lesioned animals were no more likely to explore than control animals. We also discovered that all animals showed a strong preference for certain maze arms, at the expense of reward. This tendency was exacerbated in the lesioned animals, with the strongest effects seen in a subset of animals with damage to dorsal mPFC. The results confirm a role for mPFC in goal-directed behaviours but suggest that rats rely on other areas to resolve the explore-exploit dilemma.},
  keywords = {Decision making,Exploration,Prefrontal cortex,Reinforcement learning,Reward,Value},
  file = {/Users/daniekru/Zotero/storage/X52KZDDB/S0166432816301322.html}
}

@inproceedings{linerImprovingNeuralNetwork2021,
  title = {Improving {{Neural Network Learning Through Dual Variable Learning Rates}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Liner, Elizabeth and Miikkulainen, Risto},
  year = {2021},
  month = jul,
  pages = {1--7},
  issn = {2161-4407},
  doi = {10.1109/IJCNN52387.2021.9533487},
  urldate = {2024-12-02},
  abstract = {This paper introduces and evaluates a novel training method for neural networks: Dual Variable Learning Rates (DVLR). Building on insights from behavioral psychology, the dual learning rates are used to emphasize correct and incorrect responses differently, thereby making the feedback to the network more specific. Further, the learning rates are varied as a function of the network's performance, thereby making it more efficient. DVLR was implemented on three types of networks: feedforward, convolutional, and residual, and two domains: MNIST and CIFAR-10. The results suggest a consistently improved accuracy, demonstrating that DVLR is a promising, psychologically motivated technique for training neural network models.},
  keywords = {Buildings,Machine learning,Neural networks,Psychology,Residual neural networks,Schedules,Training},
  file = {/Users/daniekru/Zotero/storage/H3JIEI3K/Liner and Miikkulainen - 2021 - Improving Neural Network Learning Through Dual Variable Learning Rates.pdf;/Users/daniekru/Zotero/storage/AXR9KM29/9533487.html}
}

@article{lohaniDopamineModulationPrefrontal2019,
  title = {Dopamine {{Modulation}} of {{Prefrontal Cortex Activity Is Manifold}} and {{Operates}} at {{Multiple Temporal}} and {{Spatial Scales}}},
  author = {Lohani, Sweyta and Martig, Adria K. and Deisseroth, Karl and Witten, Ilana B. and Moghaddam, Bita},
  year = {2019},
  month = apr,
  journal = {Cell Reports},
  volume = {27},
  number = {1},
  pages = {99-114.e6},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2019.03.012},
  urldate = {2024-05-07},
  abstract = {Although the function of dopamine in subcortical structures is largely limited to reward and movement, dopamine neurotransmission in the prefrontal cortex (PFC) is critical to a multitude of temporally and functionally diverse processes, such as attention, working memory, behavioral flexibility, action planning, and sustained motivational and affective states. How does dopamine influence computation of these temporally complex functions? We find causative links between sustained and burst patterns of phasic dopamine neuron activation and modulation of medial PFC neuronal activity at multiple spatiotemporal scales. These include a multidirectional and weak impact on individual neuron rate~activity but a robust influence on coordinated ensemble activity, gamma oscillations, and gamma-theta coupling that persisted for minutes. In addition, PFC network responses to burst pattern of dopamine firing were~selectively strengthened in behaviorally active states. This multiplex mode of modulation by dopamine input may enable PFC to compute and generate spatiotemporally diverse and specialized outputs.},
  keywords = {attention,cognition,ensemble activity,gamma oscillation,ventral tegmental area,working memory},
  file = {/Users/daniekru/Zotero/storage/YSAY9638/Lohani et al. - 2019 - Dopamine Modulation of Prefrontal Cortex Activity .pdf;/Users/daniekru/Zotero/storage/2KNBMVK4/S2211124719303195.html}
}

@article{lukChoiceCodingFrontal2013,
  title = {Choice {{Coding}} in {{Frontal Cortex}} during {{Stimulus-Guided}} or {{Action-Guided Decision-Making}}},
  author = {Luk, Chung-Hay and Wallis, Jonathan D.},
  year = {2013},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {33},
  number = {5},
  pages = {1864--1871},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4920-12.2013},
  urldate = {2024-11-27},
  abstract = {To optimally obtain desirable outcomes, organisms must track outcomes predicted by stimuli in the environment (stimulus--outcome or SO associations) and outcomes predicted by their own actions (action--outcome or AO associations). Anterior cingulate cortex (ACC) and orbitofrontal cortex (OFC) are implicated in tracking outcomes, but anatomical and functional studies suggest a dissociation, with ACC and OFC responsible for encoding AO and SO associations, respectively. To examine whether this dissociation held at the single neuron level, we trained two subjects to perform choice tasks that required using AO or SO associations. OFC and ACC neurons encoded the action that the subject used to indicate its choice, but this encoding was stronger in OFC during the SO task and stronger in ACC during the AO task. These results are consistent with a division of labor between the two areas in terms of using rewards associated with either stimuli or actions to guide decision-making.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2013 the authors 0270-6474/13/331864-08\$15.00/0},
  langid = {english},
  pmid = {23365226},
  file = {/Users/daniekru/Zotero/storage/UX5ID2F5/Luk and Wallis - 2013 - Choice Coding in Frontal Cortex during Stimulus-Guided or Action-Guided Decision-Making.pdf}
}

@article{madadiaslDopaminergicModulationSynaptic2019,
  title = {Dopaminergic {{Modulation}} of {{Synaptic Plasticity}}, {{Its Role}} in {{Neuropsychiatric Disorders}}, and {{Its Computational Modeling}}},
  author = {Madadi Asl, Mojtaba and Vahabie, Abdol-Hossein and Valizadeh, Alireza},
  year = {2019},
  journal = {Basic and Clinical Neuroscience},
  volume = {10},
  number = {1},
  pages = {1--12},
  issn = {2008-126X},
  doi = {10.32598/bcn.9.10.125},
  urldate = {2024-04-26},
  abstract = {Neuromodulators modify intrinsic characteristics of the nervous system in order to reconfigure the functional properties of neural circuits. This reconfiguration is crucial for the flexibility of the nervous system to respond on an input-modulated basis. Such a functional rearrangement is realized by modification of intrinsic properties of the neural circuits including synaptic interactions. Dopamine is an important neuromodulator involved in motivation and stimulus-reward learning process, and adjusts synaptic dynamics in multiple time scales through different pathways. The modification of synaptic plasticity by dopamine underlies the change in synaptic transmission and integration mechanisms, which affects intrinsic properties of the neural system including membrane excitability, probability of neurotransmitters release, receptors' response to neurotransmitters, protein trafficking, and gene transcription. Dopamine also plays a central role in behavioral control, whereas its malfunction can cause cognitive disorders. Impaired dopamine signaling is implicated in several neuropsychiatric disorders such as Parkinson's disease, drug addiction, schizophrenia, attention-deficit/hyperactivity disorder, obsessive-compulsive disorder and Tourette's syndrome. Therefore, dopamine plays a crucial role in the nervous system, where its proper modulation of neural circuits may enhance plasticity-related procedures, but disturbances in dopamine signaling might be involved in numerous neuropsychiatric disorders. In recent years, several computational models are proposed to formulate the involvement of dopamine in synaptic plasticity or neuropsychiatric disorders and address their connection based on the experimental findings.},
  pmcid = {PMC6484184},
  pmid = {31031889},
  file = {/Users/daniekru/Zotero/storage/S4YBA7FY/Madadi Asl et al. - 2019 - Dopaminergic Modulation of Synaptic Plasticity, It.pdf}
}

@article{malakasisSynapticTurnoverPromotes2023,
  title = {Synaptic Turnover Promotes Efficient Learning in Bio-Realistic Spiking Neural Networks},
  author = {Malakasis, Nikos and Chavlis, Spyridon and Poirazi, Panayiota},
  year = {2023},
  month = may,
  journal = {bioRxiv},
  pages = {2023.05.22.541722},
  doi = {10.1101/2023.05.22.541722},
  urldate = {2024-12-10},
  abstract = {While artificial machine learning systems achieve superhuman performance in specific tasks such as language processing, image and video recognition, they do so use extremely large datasets and huge amounts of power. On the other hand, the brain remains superior in several cognitively challenging tasks while operating with the energy of a small lightbulb. We use a biologically constrained spiking neural network model to explore how the neural tissue achieves such high efficiency and assess its learning capacity on discrimination tasks. We found that synaptic turnover, a form of structural plasticity, which is the ability of the brain to form and eliminate synapses continuously, increases both the speed and the performance of our network on all tasks tested. Moreover, it allows accurate learning using a smaller number of examples. Importantly, these improvements are most significant under conditions of resource scarcity, such as when the number of trainable parameters is halved and when the task difficulty is increased. Our findings provide new insights into the mechanisms that underlie efficient learning in the brain and can inspire the development of more efficient and flexible machine learning algorithms.},
  pmcid = {PMC10245885},
  pmid = {37292929},
  file = {/Users/daniekru/Zotero/storage/NH5LIBTA/Malakasis et al. - 2023 - Synaptic turnover promotes efficient learning in bio-realistic spiking neural networks.pdf}
}

@article{marcosDeterminingMonkeyFree2016,
  title = {Determining {{Monkey Free Choice Long}} before the {{Choice Is Made}}: {{The Principal Role}} of {{Prefrontal Neurons Involved}} in {{Both Decision}} and {{Motor Processes}}},
  shorttitle = {Determining {{Monkey Free Choice Long}} before the {{Choice Is Made}}},
  author = {Marcos, Encarni and Genovesio, Aldo},
  year = {2016},
  month = sep,
  journal = {Frontiers in Neural Circuits},
  volume = {10},
  publisher = {Frontiers},
  issn = {1662-5110},
  doi = {10.3389/fncir.2016.00075},
  urldate = {2024-11-28},
  abstract = {{$<$}p{$>$}When choices are made freely, they might emerge from pre-existing neural activity. However, whether neurons in the prefrontal cortex (PF) show this anticipatory effect and, if so, in which part of the process they are involved is still debated. To answer this question, we studied PF activity in monkeys while they performed a strategy task. In this task when the stimulus changed from the previous trial, the monkeys had to shift their response to one of two spatial goals, excluding the one that had been previously selected. Under this free-choice condition, the prestimulus activity of the same neurons that are involved in decision and motor processes predicted future choices. These neurons developed the same goal preferences during the prestimulus presentation as they did later in the decision phase. In contrast, the same effect was not observed in motor-only neurons and it was present but weaker in decision-only neurons. Overall, our results suggest that the PF neuronal activity predicts upcoming actions mainly through the decision-making network that integrate in time decision and motor task aspects.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Bias,Decision Making,monkeys,Neurophysiology,prefrontal,strategy},
  file = {/Users/daniekru/Zotero/storage/R52M755Q/Marcos and Genovesio - 2016 - Determining Monkey Free Choice Long before the Choice Is Made The Principal Role of Prefrontal Neur.pdf}
}

@article{martinRepresentationObjectConcepts2007a,
  title = {The {{Representation}} of {{Object Concepts}} in the {{Brain}}},
  author = {Martin, Alex},
  year = {2007},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {58},
  number = {1},
  pages = {25--45},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.57.102904.190143},
  urldate = {2024-12-19},
  abstract = {Evidence from functional neuroimaging of the human brain indicates that information about salient properties of an object---such as what it looks like, how it moves, and how it is used---is stored in sensory and motor systems active when that information was acquired. As a result, object concepts belonging to different categories like animals and tools are represented in partially distinct, sensory- and motor property--based neural networks. This suggests that object concepts are not explicitly represented, but rather emerge from weighted activity within property-based brain regions. However, some property-based regions seem to show a categorical organization, thus providing evidence consistent with category-based, domain-specific formulations as well.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/2BZ6XLAW/Martin - 2007 - The Representation of Object Concepts in the Brain.pdf}
}

@article{millerCombinedMechanismsNeural2019,
  title = {Combined Mechanisms of Neural Firing Rate Homeostasis},
  author = {Miller, Paul and Cannon, Jonathan},
  year = {2019},
  journal = {Biological Cybernetics},
  volume = {113},
  number = {1},
  pages = {47--59},
  issn = {0340-1200},
  doi = {10.1007/s00422-018-0768-8},
  urldate = {2024-12-12},
  abstract = {Spikes in the membrane potential of neurons comprise the currency of information processing in the brain. The ability of neurons to convert any information present across their multiple inputs into a significant modification to the pattern of their emitted spikes depends on the rate at which they emit spikes. If the mean rate is near the neuron's maximum, or if the rate is near zero, then changes in the inputs have minimal impact on the neuron's firing rate. Therefore, a neuron needs to control its mean rate. Protocols that either dramatically increase or decrease a neuron's firing rate lead to multiple compensatory changes that return the neuron's mean rate toward its prior value. In this primer, first as a summary of our previous work (Cannon and Miller in J Neurophysiol 116(5):2004--2022, ; Cannon and Miller in J Math Neurosci 7(1):1, ), we describe the advantages and disadvantages of having more than one such control mechanism responding to the neuron's firing rate. We suggest how problems of two, coexisting, potentially competing mechanisms can be overcome. Key requirements are: (1) the control be of a distribution of values, which the controlled variable achieves over a fast timescale compared to the timescale of the control system; (2) at least one of the control mechanisms be nonlinear; and (3) the two control systems are satisfied by a stable distribution or range of values that can be achieved by the variable. We show examples of functional control systems, including the previously studied integral feedback controller and new simulations of a ``bang--bang'' controller, that allow for compensation when inputs to the system change. Finally, we present new results describing how the underlying signal processing pathways would produce mechanisms of dual control, as opposed to a single mechanism with two outputs, and compare the responses of these systems to changes of input statistics.},
  pmcid = {PMC6510813},
  pmid = {29955960},
  file = {/Users/daniekru/Zotero/storage/K5Z9JTU5/Miller and Cannon - 2019 - Combined mechanisms of neural firing rate homeostasis.pdf}
}

@article{millerIntegrativeTheoryPrefrontal2001a,
  title = {An {{Integrative Theory}} of {{Prefrontal Cortex Function}}},
  author = {Miller, Earl K. and Cohen, Jonathan D.},
  year = {2001},
  month = mar,
  journal = {Annual Review of Neuroscience},
  volume = {24},
  number = {1},
  pages = {167--202},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.24.1.167},
  urldate = {2024-05-06},
  abstract = {▪ Abstract{\enspace} The prefrontal cortex has long been suspected to play an important role in cognitive control, in the ability to orchestrate thought and action in accordance with internal goals. Its neural basis, however, has remained a mystery. Here, we propose that cognitive control stems from the active maintenance of patterns of activity in the prefrontal cortex that represent goals and the means to achieve them. They provide bias signals to other brain structures whose net effect is to guide the flow of activity along neural pathways that establish the proper mappings between inputs, internal states, and outputs needed to perform a given task. We review neurophysiological, neurobiological, neuroimaging, and computational studies that support this theory and discuss its implications as well as further issues to be addressed},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/K5SC3U6J/Miller and Cohen - 2001 - An Integrative Theory of Prefrontal Cortex Functio.pdf}
}

@article{montagueFrameworkMesencephalicDopamine1996,
  title = {A Framework for Mesencephalic Dopamine Systems Based on Predictive {{Hebbian}} Learning},
  author = {Montague, P. R. and Dayan, P. and Sejnowski, T. J.},
  year = {1996},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {16},
  number = {5},
  pages = {1936--1947},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.16-05-01936.1996},
  urldate = {2024-04-26},
  abstract = {We develop a theoretical framework that shows how mesencephalic dopamine systems could distribute to their targets a signal that represents information about future expectations. In particular, we show how activity in the cerebral cortex can make predictions about future receipt of reward and how fluctuations in the activity levels of neurons in diffuse dopamine systems above and below baseline levels would represent errors in these predictions that are delivered to cortical and subcortical targets. We present a model for how such errors could be constructed in a real brain that is consistent with physiological results for a subset of dopaminergic neurons located in the ventral tegmental area and surrounding dopaminergic neurons. The theory also makes testable predictions about human choice behavior on a simple decision-making task. Furthermore, we show that, through a simple influence on synaptic plasticity, fluctuations in dopamine release can act to change the predictions in an appropriate manner.},
  chapter = {Articles},
  copyright = {{\copyright} 1996 by Society for Neuroscience},
  langid = {english},
  pmid = {8774460},
  file = {/Users/daniekru/Zotero/storage/P9HGS9Z3/Montague et al. - 1996 - A framework for mesencephalic dopamine systems bas.pdf}
}

@misc{murakamiDistinctSourcesDeterministic2016,
  title = {Distinct Sources of Deterministic and Stochastic Components of Action Timing Decisions in Rodent Frontal Cortex},
  author = {Murakami, Masayoshi and Shteingart, Hanan and Loewenstein, Yonatan and Mainen, Zachary F.},
  year = {2016},
  month = nov,
  doi = {10.1101/088963},
  urldate = {2024-05-07},
  abstract = {The selection and timing of actions are subject to determinate influences such as sensory cues and internal state as well as to effectively stochastic variability. Although stochastic choice mechanisms are assumed by many theoretical models, their origin and mechanisms remain poorly understood. Here we investigated this issue by studying how neural circuits in the frontal cortex determine action timing in rats performing a waiting task. Electrophysiological recordings from two regions necessary for this behavior, medial prefrontal cortex (mPFC) and secondary motor cortex (M2), revealed an unexpected functional dissociation. Both areas encoded deterministic biases in action timing, but only M2 neurons reflected stochastic trial-bytrial fluctuations. This differential coding was reflected in distinct timescales of neural dynamics in the two frontal cortical areas. These results suggest a two-stage model in which stochastic components of action timing decisions are injected by circuits downstream of those carrying deterministic bias signals.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/INEJV6FR/Murakami et al. - 2016 - Distinct sources of deterministic and stochastic c.pdf}
}

@article{nivEvolutionReinforcementLearning2002,
  title = {Evolution of {{Reinforcement Learning}} in {{Uncertain Environments}}: {{A Simple Explanation}} for {{Complex Foraging Behaviors}}},
  author = {Niv, Yael and Joel, Daphna and Meilijson, Isaac and Ruppin, Eytan},
  year = {2002},
  journal = {International Society for Adaptive Behavior},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/5ZAAEAZT/Niv et al. - Evolution of Reinforcement Learning in Uncertain E.pdf}
}

@article{nunesSpikingNeuralNetworks2022,
  title = {Spiking {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Spiking {{Neural Networks}}},
  author = {Nunes, Jo{\~a}o D. and Carvalho, Marcelo and Carneiro, Diogo and Cardoso, Jaime S.},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {60738--60764},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3179968},
  urldate = {2024-12-10},
  abstract = {The field of Deep Learning (DL) has seen a remarkable series of developments with increasingly accurate and robust algorithms. However, the increase in performance has been accompanied by an increase in the parameters, complexity, and training and inference time of the models, which means that we are rapidly reaching a point where DL may no longer be feasible. On the other hand, some specific applications need to be carefully considered when developing DL models due to hardware limitations or power requirements. In this context, there is a growing interest in efficient DL algorithms, with Spiking Neural Networks (SNNs) being one of the most promising paradigms. Due to the inherent asynchrony and sparseness of spike trains, these types of networks have the potential to reduce power consumption while maintaining relatively good performance. This is attractive for efficient DL and, if successful, could replace traditional Artificial Neural Networks (ANNs) in many applications. However, despite significant progress, the performance of SNNs on benchmark datasets is often lower than that of traditional ANNs. Moreover, due to the non-differentiable nature of their activation functions, it is difficult to train SNNs with direct backpropagation, so appropriate training strategies must be found. Nevertheless, significant efforts have been made to develop competitive models. This survey covers the main ideas behind SNNs and reviews recent trends in learning rules and network architectures, with a particular focus on biologically inspired strategies. It also provides some practical considerations of state-of-the-art SNNs and discusses relevant research opportunities.},
  keywords = {Artificial neural networks,Biological neural networks,Biological system modeling,Biology,Computational modeling,computer vision,efficient deep learning,event-driven,machine learning,Micromechanical devices,neuromorphic computing,neuromorphic hardware,Neurons,spiking neural networks,Training},
  file = {/Users/daniekru/Zotero/storage/YIN799P8/Nunes et al. - 2022 - Spiking Neural Networks A Survey.pdf;/Users/daniekru/Zotero/storage/8L7L48WK/9787485.html}
}

@misc{ockerFlexibleNeuralConnectivity2020,
  title = {Flexible Neural Connectivity under Constraints on Total Connection Strength},
  author = {Ocker, Gabriel Koch and Buice, Michael A.},
  year = {2020},
  month = jan,
  primaryclass = {New Results},
  pages = {603027},
  publisher = {bioRxiv},
  doi = {10.1101/603027},
  urldate = {2024-12-12},
  abstract = {Neural computation is determined by neuron dynamics and circuit connectivity. Uncertain and dynamic environments may require neural hardware to adapt to different computational tasks, each requiring different connectivity configurations. At the same time, connectivity is subject to a variety of constraints, placing limits on the possible computations a given neural circuit can perform. Here we examine the hypothesis that the organization of neural circuitry favors computational flexibility: that it makes many computational solutions available, given physiological constraints. From this hypothesis, we develop models of the degree distributions of connectivity based on constraints on a neuron's total synaptic weight. To test these models, we examine reconstructions of the mushroom bodies from the first instar larva and the adult Drosophila melanogaster. We perform a Bayesian model comparison for two constraint models and a random wiring null model. Overall, we find that flexibility under a homeostatically fixed total synaptic weight describes Kenyon cell connectivity better than other models, suggesting a principle shaping the apparently random structure of Kenyon cell wiring. Furthermore, we find evidence that larval Kenyon cells are more flexible earlier in development, suggesting a mechanism whereby neural circuits begin as flexible systems that develop into specialized computational circuits. Author summary High-throughput electron microscopic anatomical experiments have begun to yield detailed maps of neural circuit connectivity. Uncovering the principles that govern these circuit structures is a major challenge for systems neuroscience. Healthy neural circuits must be able to perform computational tasks while satisfying physiological constraints. Those constraints can restrict a neuron's possible connectivity, and thus potentially restrict its computation. Here we examine simple models of constraints on total synaptic weights, and calculate the number of circuit configurations they allow: their computational flexibility. We propose probabilistic models of connectivity that weight the number of synaptic partners according to computational flexibility under a constraint and test them using recent wiring diagrams from a learning center, the mushroom body, in the fly brain. We compare constraints that fix or bound a neuron's total connection strength to a simple random wiring null model. Of these models, the fixed total connection strength matched the overall connectivity best in mushroom bodies from both larval and adult flies. We also provide evidence suggesting that neural circuits are more flexible in early stages of development and lose this flexibility as they grow towards specialized function.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/E2GTUF6R/Ocker and Buice - 2020 - Flexible neural connectivity under constraints on total connection strength.pdf}
}

@article{odohertyAbstractRewardPunishment2001,
  title = {Abstract Reward and Punishment Representations in the Human Orbitofrontal Cortex},
  author = {O'Doherty, J. and Kringelbach, M. L. and Rolls, E. T. and Hornak, J. and Andrews, C.},
  year = {2001},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {4},
  number = {1},
  pages = {95--102},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/82959},
  urldate = {2024-05-06},
  abstract = {The orbitofrontal cortex (OFC) is implicated in emotion and emotion-related learning. Using event-related functional magnetic resonance imaging (fMRI), we measured brain activation in human subjects doing an emotion-related visual reversal-learning task in which choice of the correct stimulus led to a probabilistically determined 'monetary' reward and choice of the incorrect stimulus led to a monetary loss. Distinct areas of the OFC were activated by monetary rewards and punishments. Moreover, in these areas, we found a correlation between the magnitude of the brain activation and the magnitude of the rewards and punishments received. These findings indicate that one emotional involvement of the human orbitofrontal cortex is its representation of the magnitudes of abstract rewards and punishments, such as receiving or losing money.},
  copyright = {2001 Nature America Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/HYX78HJT/O'Doherty et al. - 2001 - Abstract reward and punishment representations in .pdf}
}

@article{ojaOjaLearningRule2008,
  title = {Oja Learning Rule},
  author = {Oja, Erkki},
  year = {2008},
  month = mar,
  journal = {Scholarpedia},
  volume = {3},
  number = {3},
  pages = {3612},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.3612},
  urldate = {2024-12-09},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/UYLQVD39/Oja_learning_rule.html}
}

@article{ortnerNeuromorphicHardwareLearns2019,
  title = {Neuromorphic {{Hardware Learns}} to {{Learn}}},
  author = {Ortner, Thomas and Scherr, Franz and Pehle, Christian and Meier, Kyle and Maass, Wolfgang},
  year = {2019},
  month = may,
  journal = {Frontiers in Neuroscience},
  volume = {13},
  doi = {10.3389/fnins.2019.00483},
  abstract = {Hyperparameters and learning algorithms for neuromorphic hardware are usually chosen by hand to suit a particular task. In contrast, networks of neurons in the brain were optimized through extensive evolutionary and developmental processes to work well on a range of computing and learning tasks. Occasionally this process has been emulated through genetic algorithms, but these require themselves hand-design of their details and tend to provide a limited range of improvements. We employ instead other powerful gradient-free optimization tools, such as cross-entropy methods and evolutionary strategies, in order to port the function of biological optimization processes to neuromorphic hardware. As an example, we show these optimization algorithms enable neuromorphic agents to learn very efficiently from rewards. In particular, meta-plasticity, i.e., the optimization of the learning rule which they use, substantially enhances reward-based learning capability of the hardware. In addition, we demonstrate for the first time Learning-to-Learn benefits from such hardware, in particular, the capability to extract abstract knowledge from prior learning experiences that speeds up the learning of new but related tasks. Learning-to-Learn is especially suited for accelerated neuromorphic hardware, since it makes it feasible to carry out the required very large number of network computations.},
  file = {/Users/daniekru/Zotero/storage/BRUCLEYK/Ortner et al. - 2019 - Neuromorphic Hardware Learns to Learn.pdf}
}

@article{parkSymmetryLearningRate2017,
  title = {Symmetry of Learning Rate in Synaptic Plasticity Modulates Formation of Flexible and Stable Memories},
  author = {Park, Youngjin and Choi, Woochul and Paik, Se-Bum},
  year = {2017},
  month = jul,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {5671},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-05929-2},
  urldate = {2024-12-09},
  abstract = {Spike-timing-dependent plasticity (STDP) is considered critical to learning and memory functions in the human brain. Across various types of synapse, STDP is observed as different profiles of Hebbian and anti-Hebbian learning rules. However, the specific roles of diverse STDP profiles in memory formation still remain elusive. Here, we show that the symmetry of the learning rate profile in STDP is crucial to determining the character of stored memory. Using computer simulations, we found that an asymmetric learning rate generates flexible memory that is volatile and easily overwritten by newly appended information. Moreover, a symmetric learning rate generates stable memory that can coexist with newly appended information. In addition, by combining these two conditions, we could realize a hybrid memory type that operates in a way intermediate between stable and flexible memory. Our results demonstrate that various attributes of memory functions may originate from differences in the synaptic stability.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Learning algorithms,Neural circuits,Spike-timing-dependent plasticity},
  file = {/Users/daniekru/Zotero/storage/SPH68UTU/Park et al. - 2017 - Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories.pdf}
}

@article{parkSymmetryLearningRate2017a,
  title = {Symmetry of Learning Rate in Synaptic Plasticity Modulates Formation of Flexible and Stable Memories},
  author = {Park, Youngjin and Choi, Woochul and Paik, Se-Bum},
  year = {2017},
  month = jul,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {5671},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-05929-2},
  urldate = {2024-12-09},
  abstract = {Spike-timing-dependent plasticity (STDP) is considered critical to learning and memory functions in the human brain. Across various types of synapse, STDP is observed as different profiles of Hebbian and anti-Hebbian learning rules. However, the specific roles of diverse STDP profiles in memory formation still remain elusive. Here, we show that the symmetry of the learning rate profile in STDP is crucial to determining the character of stored memory. Using computer simulations, we found that an asymmetric learning rate generates flexible memory that is volatile and easily overwritten by newly appended information. Moreover, a symmetric learning rate generates stable memory that can coexist with newly appended information. In addition, by combining these two conditions, we could realize a hybrid memory type that operates in a way intermediate between stable and flexible memory. Our results demonstrate that various attributes of memory functions may originate from differences in the synaptic stability.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Learning algorithms,Neural circuits,Spike-timing-dependent plasticity},
  file = {/Users/daniekru/Zotero/storage/L26S9AVW/Park et al. - 2017 - Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories.pdf}
}

@article{pascanuNeurodynamicalModelWorking2011,
  title = {A Neurodynamical Model for Working Memory},
  author = {Pascanu, Razvan and Jaeger, Herbert},
  year = {2011},
  month = mar,
  journal = {Neural Networks},
  volume = {24},
  number = {2},
  pages = {199--207},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2010.10.003},
  urldate = {2024-05-13},
  abstract = {Neurodynamical models of working memory (WM) should provide mechanisms for storing, maintaining, retrieving, and deleting information. Many models address only a subset of these aspects. Here we present a rather simple WM model in which all of these performance modes are trained into a recurrent neural network (RNN) of the echo state network (ESN) type. The model is demonstrated on a bracket level parsing task with a stream of rich and noisy graphical script input. In terms of nonlinear dynamics, memory states correspond, intuitively, to attractors in an input-driven system. As a supplementary contribution, the article proposes a rigorous formal framework to describe such attractors, generalizing from the standard definition of attractors in autonomous (input-free) dynamical systems.},
  keywords = {Attractor,Echo state networks,Recurrent neural networks,Working memory},
  file = {/Users/daniekru/Zotero/storage/3WI7NL5V/S0893608010001899.html}
}

@article{pilarskiOptimalPolicyBernoulli2021,
  title = {Optimal {{Policy}} for {{Bernoulli Bandits}}: {{Computation}} and {{Algorithm Gauge}}},
  shorttitle = {Optimal {{Policy}} for {{Bernoulli Bandits}}},
  author = {Pilarski, Sebastian and Pilarski, Slawomir and Varro, Daniel},
  year = {2021},
  month = feb,
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {2},
  number = {1},
  pages = {2--17},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3074122},
  urldate = {2024-03-25},
  abstract = {Bernoulli multi-armed bandits are a reinforcement learning model used to study a variety of choice optimization problems. Often such optimizations concern a finite-time horizon. In principle, statistically optimal policies can be computed via dynamic programming, but doing so is considered infeasible due to prohibitive computational requirements and implementation complexity. Hence, suboptimal algorithms are applied in practice, despite their unknown level of suboptimality. In this article, we demonstrate that optimal policies can be efficiently computed for large time horizons or number of arms thanks to a novel memory organization and indexing scheme. We use optimal policies to gauge the suboptimality of several well-known finite- and infinitetime horizon algorithms including Whittle and Gittins indices, epsilon-greedy, Thompson sampling, and upper-confidence bound (UCB) algorithms. Our simulation study shows that all but one evaluated algorithm perform significantly worse than the optimal policy. The Whittle index offers a nearly optimal strategy for multiarmed Bernoulli bandits despite its suboptimal decisions---up to 10\%---compared to an optimal policy table. Lastly, we discuss optimizations of known algorithms. We derive a novel solution from UCB1-tuned. It outperforms other infinite-time horizon algorithms when dealing with many arms.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FRRR75ZZ/Pilarski et al. - 2021 - Optimal Policy for Bernoulli Bandits Computation .pdf}
}

@article{puigDopamineModulationLearning2014,
  title = {Dopamine Modulation of Learning and Memory in the Prefrontal Cortex: Insights from Studies in Primates, Rodents, and Birds},
  shorttitle = {Dopamine Modulation of Learning and Memory in the Prefrontal Cortex},
  author = {Puig, M. Victoria and Rose, Jonas and Schmidt, Robert and Freund, Nadja},
  year = {2014},
  month = aug,
  journal = {Frontiers in Neural Circuits},
  volume = {8},
  publisher = {Frontiers},
  issn = {1662-5110},
  doi = {10.3389/fncir.2014.00093},
  urldate = {2024-04-26},
  abstract = {In this review, we provide a brief overview over the current knowledge about the role of dopamine transmission in the prefrontal cortex during learning and memory. We discuss work in humans, monkeys, rats, and birds in order to provide a basis for comparison across species that might help identify crucial features and constraints of the dopaminergic system in executive function. Computational models of dopamine function are introduced to provide a framework for such a comparison. We also provide a brief evolutionary perspective showing that the dopaminergic system is highly preserved across mammals. Even birds, following a largely independent evolution of higher cognitive abilities, have evolved a comparable dopaminergic system. Finally, we discuss the unique advantages and challenges of using different animal models for advancing our understanding of dopamine function in the healthy and diseased brain.},
  langid = {english},
  keywords = {dopamine receptors,evolution,Executive Function,Learning and Memory (Neurosciences),Neuromodulation,prefrontal cortex (PFC),working memory},
  file = {/Users/daniekru/Zotero/storage/T58MVUWQ/Puig et al. - 2014 - Dopamine modulation of learning and memory in the .pdf}
}

@misc{qiForcedExplorationBandit2023,
  title = {Forced {{Exploration}} in {{Bandit Problems}}},
  author = {Qi, Han and Guo, Fei and Zhu, Li},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07285},
  eprint = {2312.07285},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.07285},
  urldate = {2024-12-11},
  abstract = {The multi-armed bandit(MAB) is a classical sequential decision problem. Most work requires assumptions about the reward distribution (e.g., bounded), while practitioners may have difficulty obtaining information about these distributions to design models for their problems, especially in non-stationary MAB problems. This paper aims to design a multi-armed bandit algorithm that can be implemented without using information about the reward distribution while still achieving substantial regret upper bounds. To this end, we propose a novel algorithm alternating between greedy rule and forced exploration. Our method can be applied to Gaussian, Bernoulli and other subgaussian distributions, and its implementation does not require additional information. We employ a unified analysis method for different forced exploration strategies and provide problem-dependent regret upper bounds for stationary and piecewise-stationary settings. Furthermore, we compare our algorithm with popular bandit algorithms on different reward distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/daniekru/Zotero/storage/NL68RNHP/Qi et al. - 2023 - Forced Exploration in Bandit Problems.pdf;/Users/daniekru/Zotero/storage/LKMRC3GV/2312.html}
}

@article{ratteImpactNeuronalProperties2013,
  title = {Impact of {{Neuronal Properties}} on {{Network Coding}}: {{Roles}} of {{Spike Initiation Dynamics}} and {{Robust Synchrony Transfer}}},
  shorttitle = {Impact of {{Neuronal Properties}} on {{Network Coding}}},
  author = {Ratt{\'e}, St{\'e}phanie and Hong, Sungho and De Schutter, Erik and Prescott, Steven A.},
  year = {2013},
  month = jun,
  journal = {Neuron},
  volume = {78},
  number = {5},
  pages = {758--772},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.05.030},
  urldate = {2024-12-12},
  abstract = {Neural networks are more than the sum of their parts, but the properties of those parts are nonetheless important. For instance, neuronal properties affect the degree to which neurons receiving common input will spike synchronously, and whether that synchrony will propagate through the network. Stimulus-evoked synchrony can help or hinder network coding depending on the type of code. In this Perspective, we describe how spike initiation dynamics influence neuronal input-output properties, how those properties affect synchronization, and how synchronization affects network coding. We propose that synchronous and asynchronous spiking can be used to multiplex temporal (synchrony) and rate coding and discuss how pyramidal neurons would be well suited for that task.},
  file = {/Users/daniekru/Zotero/storage/LZLP6GCM/Ratté et al. - 2013 - Impact of Neuronal Properties on Network Coding Roles of Spike Initiation Dynamics and Robust Synch.pdf;/Users/daniekru/Zotero/storage/2IV4M2Z9/S0896627313004509.html}
}

@article{reynoldsDopaminedependentPlasticityCorticostriatal2002,
  title = {Dopamine-Dependent Plasticity of Corticostriatal Synapses},
  author = {Reynolds, John N.J and Wickens, Jeffery R},
  year = {2002},
  month = jun,
  journal = {Neural Networks},
  volume = {15},
  number = {4-6},
  pages = {507--521},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00045-X},
  urldate = {2024-05-07},
  abstract = {Knowledge of the effect of dopamine on corticostriatal synaptic plasticity has advanced rapidly over the last 5 years. We consider this new knowledge in relation to three factors proposed earlier to describe the rules for synaptic plasticity in the corticostriatal pathway. These factors are a phasic increase in dopamine release, presynaptic activity and postsynaptic depolarisation. A function is proposed which relates the amount of dopamine release in the striatum to the modulation of corticostriatal synaptic efficacy. It is argued that this function, and the experimental data from which it arises, are compatible with existing models which associate the reward-related firing of dopamine neurons with changes in corticostriatal synaptic efficacy. q 2002 Elsevier Science Ltd. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/68B6VBQA/Reynolds and Wickens - 2002 - Dopamine-dependent plasticity of corticostriatal s.pdf}
}

@article{ricebergRewardStabilityDetermines2012,
  title = {Reward {{Stability Determines}} the {{Contribution}} of {{Orbitofrontal Cortex}} to {{Adaptive Behavior}}},
  author = {Riceberg, Justin S. and Shapiro, Matthew L.},
  year = {2012},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {32},
  number = {46},
  pages = {16402--16409},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0776-12.2012},
  urldate = {2024-05-06},
  abstract = {Animals respond to changing contingencies to maximize reward. The orbitofrontal cortex (OFC) is important for flexible responding when established contingencies change, but the underlying cognitive mechanisms are debated. We tested rats with sham or OFC lesions in radial maze tasks that varied the frequency of contingency changes and measured both perseverative and non-perseverative errors. When contingencies were changed rarely, rats with sham lesions learned quickly and performed better than rats with OFC lesions. Rats with sham lesions made fewer non-perseverative errors, rarely entering non-rewarded arms, and more win--stay responses by returning to recently rewarded arms compared with rats with OFC lesions. When contingencies were changed rapidly, however, rats with sham lesions learned slower, made more non-perseverative errors and fewer lose--shift responses, and returned more often to non-rewarded arms than rats with OFC lesions. The results support the view that the OFC integrates reward history and suggest that the availability of outcome expectancy signals can either improve or impair adaptive responding depending on reward stability.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2012 the authors 0270-6474/12/3216402-08\$15.00/0},
  langid = {english},
  pmid = {23152622},
  file = {/Users/daniekru/Zotero/storage/QNUCGLPW/Riceberg and Shapiro - 2012 - Reward Stability Determines the Contribution of Or.pdf}
}

@article{roeschDopamineNeuronsEncode2007,
  title = {Dopamine Neurons Encode the Better Option in Rats Deciding between Differently Delayed or Sized Rewards},
  author = {Roesch, Matthew R. and Calu, Donna J. and Schoenbaum, Geoffrey},
  year = {2007},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {12},
  pages = {1615--1624},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn2013},
  urldate = {2024-04-26},
  abstract = {The dopamine system is thought to be involved in making decisions about reward. Here we recorded from the ventral tegmental area in rats learning to choose between differently delayed and sized rewards. As expected, the activity of many putative dopamine neurons reflected reward prediction errors, changing when the value of the reward increased or decreased unexpectedly. During learning, neural responses to reward in these neurons waned and responses to cues that predicted reward emerged. Notably, this cue-evoked activity varied with size and delay. Moreover, when rats were given a choice between two differently valued outcomes, the activity of the neurons initially reflected the more valuable option, even when it was not subsequently selected.},
  copyright = {2007 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/82Y9XINE/Roesch et al. - 2007 - Dopamine neurons encode the better option in rats .pdf}
}

@article{rosenbloomFunctionalNeuroanatomyDecisionMaking2012,
  title = {The {{Functional Neuroanatomy}} of {{Decision-Making}}},
  author = {Rosenbloom, Michael H. and Schmahmann, Jeremy D. and Price, Bruce H.},
  year = {2012},
  month = jul,
  journal = {The Journal of Neuropsychiatry and Clinical Neurosciences},
  volume = {24},
  number = {3},
  pages = {266--277},
  publisher = {American Psychiatric Publishing},
  issn = {0895-0172},
  doi = {10.1176/appi.neuropsych.11060139},
  urldate = {2024-11-27},
  abstract = {Decision-making is a complex executive function that draws on past experience, present goals, and anticipation of outcome, and which is influenced by prevailing and predicted emotional tone and cultural context. Functional imaging investigations and focal lesion studies identify the orbitofrontal, anterior cingulate, and dorsolateral prefrontal cortices as critical to decision-making. The authors review the connections of these prefrontal regions with the neocortex, limbic system, basal ganglia, and cerebellum, highlight current ideas regarding the cognitive processes of decision-making that these networks subserve, and present a novel integrated neuroanatomical model for decision-making. Finally, clinical relevance of this circuitry is illustrated through a discussion of frontotemporal dementia, traumatic brain injury, and sociopathy.},
  file = {/Users/daniekru/Zotero/storage/6A6EJ78H/Rosenbloom et al. - 2012 - The Functional Neuroanatomy of Decision-Making.pdf}
}

@article{rouaultPrefrontalMechanismsCombining2019,
  title = {Prefrontal Mechanisms Combining Rewards and Beliefs in Human Decision-Making},
  author = {Rouault, Marion and Drugowitsch, Jan and Koechlin, Etienne},
  year = {2019},
  month = jan,
  journal = {Nature Communications},
  volume = {10},
  pages = {301},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08121-w},
  urldate = {2024-04-29},
  abstract = {In uncertain and changing environments, optimal decision-making requires integrating reward expectations with probabilistic beliefs about reward contingencies. Little is known, however, about how the prefrontal cortex (PFC), which subserves decision-making, combines these quantities. Here, using computational modelling and neuroimaging, we show that the ventromedial PFC encodes both reward expectations and proper beliefs about reward contingencies, while the dorsomedial PFC combines these quantities and guides choices that are at variance with those predicted by optimal decision theory: instead of integrating reward expectations with beliefs, the dorsomedial PFC built context-dependent reward expectations commensurable to beliefs and used these quantities as two concurrent appetitive components, driving choices. This neural mechanism accounts for well-known risk aversion effects in human decision-making. The results reveal that the irrationality of human choices commonly theorized as deriving from optimal computations over false beliefs, actually stems from suboptimal neural heuristics over rational beliefs about reward contingencies., Optimal decision-making requires integrating expectations about rewards with beliefs about reward contingencies. Here, the authors show that these aspects of reward are encoded in the ventromedial prefrontal cortex then combined in the dorsomedial prefrontal cortex, a process that guides choice biases characteristic of human decision-making.},
  pmcid = {PMC6336816},
  pmid = {30655534},
  file = {/Users/daniekru/Zotero/storage/N4FCVGQW/Rouault et al. - 2019 - Prefrontal mechanisms combining rewards and belief.pdf}
}

@article{rouaultPrefrontalMechanismsCombining2019a,
  title = {Prefrontal Mechanisms Combining Rewards and Beliefs in Human Decision-Making},
  author = {Rouault, Marion and Drugowitsch, Jan and Koechlin, Etienne},
  year = {2019},
  month = jan,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {301},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08121-w},
  urldate = {2024-11-27},
  abstract = {In uncertain and changing environments, optimal decision-making requires integrating reward expectations with probabilistic beliefs about reward contingencies. Little is known, however, about how the prefrontal cortex (PFC), which subserves decision-making, combines these quantities. Here, using computational modelling and neuroimaging, we show that the ventromedial PFC encodes both reward expectations and proper beliefs about reward contingencies, while the dorsomedial PFC combines these quantities and guides choices that are at variance with those predicted by optimal decision theory: instead of integrating reward expectations with beliefs, the dorsomedial PFC built context-dependent reward expectations commensurable to beliefs and used these quantities as two concurrent appetitive components, driving choices. This neural mechanism accounts for well-known risk aversion effects in human decision-making. The results reveal that the irrationality of human choices commonly theorized as deriving from optimal computations over false beliefs, actually stems from suboptimal neural heuristics over rational beliefs about reward contingencies.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Decision,Human behaviour},
  file = {/Users/daniekru/Zotero/storage/B2FRLAGM/Rouault et al. - 2019 - Prefrontal mechanisms combining rewards and beliefs in human decision-making.pdf}
}

@article{samavatSynapticInformationStorage2024,
  title = {Synaptic {{Information Storage Capacity Measured With Information Theory}}},
  author = {Samavat, Mohammad and Bartol, Thomas M. and Harris, Kristen M. and Sejnowski, Terrence J.},
  year = {2024},
  month = apr,
  journal = {Neural Computation},
  volume = {36},
  number = {5},
  pages = {781--802},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01659},
  urldate = {2024-12-12},
  abstract = {Variation in the strength of synapses can be quantified by measuring the anatomical properties of synapses. Quantifying precision of synaptic plasticity is fundamental to understanding information storage and retrieval in neural circuits. Synapses from the same axon onto the same dendrite have a common history of coactivation, making them ideal candidates for determining the precision of synaptic plasticity based on the similarity of their physical dimensions. Here, the precision and amount of information stored in synapse dimensions were quantified with Shannon information theory, expanding prior analysis that used signal detection theory (Bartol et~al., 2015). The two methods were compared using dendritic spine head volumes in the middle of the stratum radiatum of hippocampal area CA1 as well-defined measures of synaptic strength. Information theory delineated the number of distinguishable synaptic strengths based on nonoverlapping bins of dendritic spine head volumes. Shannon entropy was applied to measure synaptic information storage capacity (SISC) and resulted in a lower bound of 4.1 bits and upper bound of 4.59 bits of information based on 24 distinguishable sizes. We further compared the distribution of distinguishable sizes and a uniform distribution using Kullback-Leibler divergence and discovered that there was a nearly uniform distribution of spine head volumes across the sizes, suggesting optimal use of the distinguishable values. Thus, SISC provides a new analytical measure that can be generalized to probe synaptic strengths and capacity for plasticity in different brain regions of different species and among animals raised in different conditions or during learning. How brain diseases and disorders affect the precision of synaptic plasticity can also be probed.},
  file = {/Users/daniekru/Zotero/storage/ZPHWN89A/Samavat et al. - 2024 - Synaptic Information Storage Capacity Measured With Information Theory.pdf;/Users/daniekru/Zotero/storage/RT6RSMG6/Synaptic-Information-Storage-Capacity-Measured.html}
}

@article{schultzNeuralSubstratePrediction1997,
  title = {A {{Neural Substrate}} of {{Prediction}} and {{Reward}}},
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
  year = {1997},
  month = mar,
  journal = {Science},
  volume = {275},
  number = {5306},
  pages = {1593--1599},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.275.5306.1593},
  urldate = {2024-04-26},
  abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
  file = {/Users/daniekru/Zotero/storage/M99A4WN2/Schultz et al. - 1997 - A Neural Substrate of Prediction and Reward.pdf}
}

@article{schulzFindingStructureMultiarmed2020,
  title = {Finding Structure in Multi-Armed Bandits},
  author = {Schulz, Eric and Franklin, Nicholas T. and Gershman, Samuel J.},
  year = {2020},
  month = jun,
  journal = {Cognitive Psychology},
  volume = {119},
  pages = {101261},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2019.101261},
  urldate = {2024-05-07},
  abstract = {How do humans search for rewards? This question is commonly studied using multi-armed bandit tasks, which require participants to trade off exploration and exploitation. Standard multi-armed bandits assume that each option has an independent reward distribution. However, learning about options independently is unrealistic, since in the real world options often share an underlying structure. We study a class of structured bandit tasks, which we use to probe how generalization guides exploration. In a structured multi-armed bandit, options have a correlation structure dictated by a latent function. We focus on bandits in which rewards are linear functions of an option's spatial position. Across 5 experiments, we find evidence that participants utilize functional structure to guide their exploration, and also exhibit a learning-to-learn effect across rounds, becoming progressively faster at identifying the latent function. Our experiments rule out several heuristic explanations and show that the same findings obtain with non-linear functions. Comparing several models of learning and decision making, we find that the best model of human behavior in our tasks combines three computational mechanisms: (1) function learning, (2) clustering of reward distributions across rounds, and (3) uncertainty-guided exploration. Our results suggest that human reinforcement learning can utilize latent structure in sophisticated ways to improve efficiency.},
  keywords = {Decision making,Exploration-exploitation,Function learning,Gaussian process,Generalization,Latent structure,Learning,Learning-to-learn,Reinforcement learning,Structure learning},
  file = {/Users/daniekru/Zotero/storage/RC35QTAA/Schulz et al. - 2020 - Finding structure in multi-armed bandits.pdf;/Users/daniekru/Zotero/storage/WGWCDN48/S0010028519302518.html}
}

@article{sheynikhovichLongtermMemorySynaptic2023,
  title = {Long-Term Memory, Synaptic Plasticity and Dopamine in Rodent Medial Prefrontal Cortex: {{Role}} in Executive Functions},
  shorttitle = {Long-Term Memory, Synaptic Plasticity and Dopamine in Rodent Medial Prefrontal Cortex},
  author = {Sheynikhovich, Denis and Otani, Satoru and Bai, Jing and Arleo, Angelo},
  year = {2023},
  month = jan,
  journal = {Frontiers in Behavioral Neuroscience},
  volume = {16},
  publisher = {Frontiers},
  issn = {1662-5153},
  doi = {10.3389/fnbeh.2022.1068271},
  urldate = {2024-04-26},
  abstract = {Mnemonic functions, supporting rodent behavior in complex tasks, include both long-term and (short-term) working memory components. While working memory is thought to rely on persistent activity states in an active neural network, long-term memory and synaptic plasticity contribute to the formation of the underlying synaptic structure, determining the range of possible states. Whereas the implication of working memory in executive functions, mediated by the prefrontal cortex (PFC) in primates and rodents, has been extensively studied, the contribution of long-term memory component to these tasks received little attention. This review summarizes available experimental data and theoretical work concerning cellular mechanisms of synaptic plasticity in the medial region of rodent PFC and the link between plasticity, memory and behavior in PFC-dependent tasks. A special attention is devoted to unique properties of dopaminergic modulation of prefrontal synaptic plasticity and its contribution to executive functions.},
  langid = {english},
  keywords = {behavioral flexibility,Computational models,Dopamine,executive functions,Long-term memory,Neuromodulation,Prefrontal Cortex,synaptic plasticity},
  file = {/Users/daniekru/Zotero/storage/M7NJNB4Y/Sheynikhovich et al. - 2023 - Long-term memory, synaptic plasticity and dopamine.pdf}
}

@article{steyversBayesianAnalysisHuman2009a,
  title = {A {{Bayesian}} Analysis of Human Decision-Making on Bandit Problems},
  author = {Steyvers, Mark and Lee, Michael D. and Wagenmakers, Eric-Jan},
  year = {2009},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  volume = {53},
  number = {3},
  pages = {168--179},
  issn = {00222496},
  doi = {10.1016/j.jmp.2008.11.002},
  urldate = {2024-05-07},
  abstract = {The bandit problem is a dynamic decision-making task that is simply described, well-suited to controlled laboratory study, and representative of a broad class of real-world problems. In bandit problems, people must choose between a set of alternatives, each with different unknown reward rates, to maximize the total reward they receive over a fixed number of trials. A key feature of the task is that it challenges people to balance the exploration of unfamiliar choices with the exploitation of familiar ones. We use a Bayesian model of optimal decision-making on the task, in which how people balance exploration with exploitation depends on their assumptions about the distribution of reward rates. We also use Bayesian model selection measures that assess how well people adhere to an optimal decision process, compared to simpler heuristic decision strategies. Using these models, we make inferences about the decision-making of 451 participants who completed a set of bandit problems, and relate various measures of their performance to other psychological variables, including psychometric assessments of cognitive abilities and personality traits. We find clear evidence of individual differences in the way the participants made decisions on the bandit problems, and some interesting correlations with measures of general intelligence.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/RS9CLBYS/Steyvers et al. - 2009 - A Bayesian analysis of human decision-making on ba.pdf}
}

@article{suttonReinforcementLearningProblem,
  title = {The {{Reinforcement Learning Problem}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/FHVHMCSC/Sutton and Barto - The Reinforcement Learning Problem.pdf}
}

@incollection{suttonReinforcementLearningProblem1998,
  title = {The {{Reinforcement Learning Problem}}},
  booktitle = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {1998},
  pages = {51--85},
  publisher = {MIT Press},
  urldate = {2024-05-07},
  abstract = {This chapter contains sections titled: The Agent-Environment Interface, Goals and Rewards, Returns, Unified Notation for Episodic and Continuing Tasks, The Markov Property, Markov Decision Processes, Value Functions, Optimal Value Functions, Optimality and Approximation, Summary, Bibliographical and Historical Remarks},
  isbn = {978-0-262-25705-3},
  file = {/Users/daniekru/Zotero/storage/IWF6EIEU/6282968.html}
}

@article{thompsonLikelihoodThatOne1933,
  title = {On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples.},
  author = {Thompson, William R.},
  year = {1933},
  file = {/Users/daniekru/Zotero/storage/SWQQZ4P3/25-3-4-285.pdf}
}

@article{toblerAdaptiveCodingReward2005,
  title = {Adaptive {{Coding}} of {{Reward Value}} by {{Dopamine Neurons}}},
  author = {Tobler, Philippe N. and Fiorillo, Christopher D. and Schultz, Wolfram},
  year = {2005},
  month = mar,
  journal = {Science},
  volume = {307},
  number = {5715},
  pages = {1642--1645},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1105370},
  urldate = {2024-05-02},
  abstract = {It is important for animals to estimate the value of rewards as accurately as possible. Because the number of potential reward values is very large, it is necessary that the brain's limited resources be allocated so as to discriminate better among more likely reward outcomes at the expense of less likely outcomes. We found that midbrain dopamine neurons rapidly adapted to the information provided by reward-predicting stimuli. Responses shifted relative to the expected reward value, and the gain adjusted to the variance of reward value. In this way, dopamine neurons maintained their reward sensitivity over a large range of reward values.},
  file = {/Users/daniekru/Zotero/storage/CBJ4X3LU/Tobler et al. - 2005 - Adaptive Coding of Reward Value by Dopamine Neuron.pdf}
}

@incollection{tokicAdaptiveEGreedyExploration2010,
  title = {Adaptive {$\varepsilon$}-{{Greedy Exploration}} in {{Reinforcement Learning Based}} on {{Value Differences}}},
  booktitle = {{{KI}} 2010: {{Advances}} in {{Artificial Intelligence}}},
  author = {Tokic, Michel},
  editor = {Dillmann, R{\"u}diger and Beyerer, J{\"u}rgen and Hanebeck, Uwe D. and Schultz, Tanja},
  year = {2010},
  volume = {6359},
  pages = {203--210},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-16111-7_23},
  urldate = {2024-03-23},
  abstract = {This paper presents ``Value-Difference Based Exploration'' (VDBE), a method for balancing the exploration/exploitation dilemma inherent to reinforcement learning. The proposed method adapts the exploration parameter of {$\varepsilon$}-greedy in dependence of the temporal-difference error observed from value-function backups, which is considered as a measure of the agent's uncertainty about the environment. VDBE is evaluated on a multi-armed bandit task, which allows for insight into the behavior of the method. Preliminary results indicate that VDBE seems to be more parameter robust than commonly used ad hoc approaches such as {$\varepsilon$}-greedy or softmax.},
  isbn = {978-3-642-16110-0 978-3-642-16111-7},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/CR7BH65S/Tokic - 2010 - Adaptive ε-Greedy Exploration in Reinforcement Lea.pdf}
}

@incollection{tokicValueDifferenceBasedExploration2011,
  title = {Value-{{Difference Based Exploration}}: {{Adaptive Control}} between {{Epsilon-Greedy}} and {{Softmax}}},
  shorttitle = {Value-{{Difference Based Exploration}}},
  booktitle = {{{KI}} 2011: {{Advances}} in {{Artificial Intelligence}}},
  author = {Tokic, Michel and Palm, G{\"u}nther},
  editor = {Bach, Joscha and Edelkamp, Stefan},
  year = {2011},
  volume = {7006},
  pages = {335--346},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-24455-1_33},
  urldate = {2024-03-23},
  abstract = {This paper proposes ``Value-Difference Based Exploration combined with Softmax action selection'' (VDBE-Softmax) as an adaptive exploration/exploitation policy for temporal-difference learning. The advantage of the proposed approach is that exploration actions are only selected in situations when the knowledge about the environment is uncertain, which is indicated by fluctuating values during learning. The method is evaluated in experiments having deterministic rewards and a mixture of both deterministic and stochastic rewards. The results show that a VDBE-Softmax policy can outperform {$\varepsilon$}-greedy, Softmax and VDBE policies in combination with on- and off-policy learning algorithms such as Q-learning and Sarsa. Furthermore, it is also shown that VDBE-Softmax is more reliable in case of value-function oscillations.},
  isbn = {978-3-642-24454-4 978-3-642-24455-1},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/ZM5YNNWB/Tokic and Palm - 2011 - Value-Difference Based Exploration Adaptive Contr.pdf}
}

@article{tremblayRelativeRewardPreference1999,
  title = {Relative Reward Preference in Primate Orbitofrontal Cortex},
  author = {Tremblay, L{\'e}on and Schultz, Wolfram},
  year = {1999},
  month = apr,
  journal = {Nature},
  volume = {398},
  number = {6729},
  pages = {704--708},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/19525},
  urldate = {2024-05-14},
  abstract = {The orbital part of prefrontal cortex appears to be crucially involved in the motivational control of goal-directed behaviour1,2. Patients with lesions of orbitofrontal cortex show impairments in making decisions about the expected outcome of actions3. Monkeys with orbitofrontal lesions respond abnormally to changes in reward expectations4,5 and show altered reward preferences6. As rewards constitute basic goals of behaviour7, we investigated here how neurons in the orbitofrontal cortex of monkeys process information about liquid and food rewards in a typical frontal task, spatial delayed responding8. The activity of orbitofrontal neurons increases in response to reward-predicting signals, during the expectation of rewards, and after the receipt of rewards. Neurons discriminate between different rewards, mainly irrespective of the spatial and visual features of reward-predicting stimuli and behavioural reactions. Most reward discriminations reflect the animals' relative preference among the available rewards, as expressed by their choice behaviour, rather than physical reward properties. Thus, neurons in the orbitofrontal cortex appear to process the motivational value of rewarding outcomes of voluntary action.},
  copyright = {1999 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/daniekru/Zotero/storage/PJ7CWN2D/Tremblay and Schultz - 1999 - Relative reward preference in primate orbitofronta.pdf}
}

@article{vogelsGatingMultipleSignals2009,
  title = {Gating Multiple Signals through Detailed Balance of Excitation and Inhibition in Spiking Networks},
  author = {Vogels, Tim P. and Abbott, L. F.},
  year = {2009},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {12},
  number = {4},
  pages = {483--491},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn.2276},
  urldate = {2024-05-13},
  abstract = {The balance of excitation and inhibition across large populations of spiking neurons has been suggested to be important. Here the authors model the effects of a more detailed balance between incoming excitation and local inhibition on the transmission of signals through a neural network.},
  copyright = {2009 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/daniekru/Zotero/storage/TXJZRQDQ/Vogels and Abbott - 2009 - Gating multiple signals through detailed balance o.pdf}
}

@article{waltonADAPTIVEDECISIONMAKING2007,
  title = {{{ADAPTIVE DECISION MAKING AND VALUE IN THE ANTERIOR CINGULATE CORTEX}}},
  author = {Walton, Mark E. and Croxson, Paula L. and Behrens, Timothy E. J. and Kennerley, Steven W. and Rushworth, Matthew F. S.},
  year = {2007},
  journal = {NeuroImage},
  volume = {36},
  number = {Suppl 2},
  pages = {T142-T154},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2007.03.029},
  urldate = {2024-11-27},
  abstract = {Choosing an appropriate response in an uncertain and varying world is central to adaptive behaviour. The frequent activation of the anterior cingulate cortex (ACC) in a diverse range of tasks has lead to intense interest in and debate over its role in the guidance and control of performance. Here, we consider how this issue can be informed by a series of studies considering the ACC's role in more naturalistic situations where there is no single certain correct response and the relationships between choices and their consequences vary. A neuroimaging study of response switching demonstrates that dorsal ACC is not simply concerned with self-generated responses or error monitoring in isolation, but is instead involved in evaluating the outcome of choices, positive or negative, that have been voluntarily chosen. By contrast, an interconnected part of the orbitofrontal cortex is shown to be more active when attending to consequences of actions instructed by the experimenter. This dissociation is explained with reference to the anatomy of these regions in humans as demonstrated by diffusion weighted imaging. Lesions to a corresponding ACC region in monkeys has no effect on animals' ability to detect or immediately correct errors when response contingencies reverse, but renders them unable to sustain appropriate behaviour due to an impairment in the ability to integrate over time their recent history of choices and outcomes. Taken together, this implies a prominent role for the ACC within a distributed network of regions that determine the dynamic value of actions and guide decision making appropriately.},
  pmcid = {PMC2954047},
  pmid = {17499161},
  file = {/Users/daniekru/Zotero/storage/H6CXCMHN/Walton et al. - 2007 - ADAPTIVE DECISION MAKING AND VALUE IN THE ANTERIOR CINGULATE CORTEX.pdf}
}

@article{wangMetalearningNaturalArtificial2021,
  title = {Meta-Learning in Natural and Artificial Intelligence},
  author = {Wang, Jane X},
  year = {2021},
  month = apr,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Computational Cognitive Neuroscience},
  volume = {38},
  pages = {90--95},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2021.01.002},
  urldate = {2024-12-09},
  abstract = {Meta-learning, or learning to learn, has gained renewed interest in recent years within the artificial intelligence community. However, meta-learning is incredibly prevalent within nature, has deep roots in cognitive science and psychology, and is currently studied in various forms within neuroscience. The aim of this review is to recast previous lines of research in the study of biological intelligence within the lens of meta-learning, placing these works into a common framework. More recent points of interaction between AI and neuroscience will be discussed, as well as interesting new directions that arise under this perspective.},
  file = {/Users/daniekru/Zotero/storage/WL9WEEJ4/Wang - 2021 - Meta-learning in natural and artificial intelligence.pdf;/Users/daniekru/Zotero/storage/BAWSBHRE/S2352154621000024.html}
}

@article{zhangCheapCleverHuman,
  title = {Cheap but {{Clever}}: {{Human Active Learning}} in a {{Bandit Setting}}},
  author = {Zhang, Shunan and Yu, Angela J},
  abstract = {How people achieve long-term goals in an imperfectly known environment, via repeated tries and noisy outcomes, is an important problem in cognitive science. There are two interrelated questions: how humans represent information, both what has been learned and what can still be learned, and how they choose actions, in particular how they negotiate the tension between exploration and exploitation. In this work, we examine human behavioral data in a multi-armed bandit setting, in which the subject choose one of four ``arms'' to pull on each trial and receives a binary outcome (win/lose). We implement both the Bayes-optimal policy, which maximizes the expected cumulative reward in this finite-horizon bandit environment, as well as a variety of heuristic policies that vary in their complexity of information representation and decision policy. We find that the knowledge gradient algorithm, which combines exact Bayesian learning with a decision policy that maximizes a combination of immediate reward gain and longterm knowledge gain, captures subjects' trial-by-trial choice best among all the models considered; it also provides the best approximation to the computationally intense optimal policy among all the heuristic policies.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/I5NDI744/Zhang and Yu - Cheap but Clever Human Active Learning in a Bandi.pdf}
}

@inproceedings{zhangForgetfulBayesMyopic2013,
  title = {Forgetful {{Bayes}} and Myopic Planning: {{Human}} Learning and Decision-Making in a Bandit Setting},
  shorttitle = {Forgetful {{Bayes}} and Myopic Planning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Shunan and Yu, Angela J},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-02},
  abstract = {How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects' choices, on a trial-to-trial basis, are best captured by a forgetful" Bayesian iterative learning model in combination with a partially myopic decision policy known as Knowledge Gradient. This model accounts for subjects' trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, epsilon-greedy and win-stay-lose-shift. It has the added benefit of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are significantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment."},
  file = {/Users/daniekru/Zotero/storage/V336BASA/Zhang and Yu - 2013 - Forgetful Bayes and myopic planning Human learnin.pdf}
}

@article{zhangHippocampalSpatialRepresentations2023,
  title = {Hippocampal Spatial Representations Exhibit a Hyperbolic Geometry That Expands with Experience},
  author = {Zhang, Huanqiu and Rich, P. Dylan and Lee, Albert K. and Sharpee, Tatyana O.},
  year = {2023},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {26},
  number = {1},
  pages = {131--139},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01212-4},
  urldate = {2024-06-06},
  abstract = {Daily experience suggests that we perceive distances near us linearly. However, the actual geometry of spatial representation in the brain is unknown. Here we report that neurons in the CA1 region of rat hippocampus that mediate spatial perception represent space according to a non-linear hyperbolic geometry. This geometry uses an exponential scale and yields greater positional information than a linear scale. We found that the size of the representation matches the optimal predictions for the number of CA1 neurons. The representations also dynamically expanded proportional to the logarithm of time that the animal spent exploring the environment, in correspondence with the maximal mutual information that can be received. The dynamic changes tracked even small variations due to changes in the running speed of the animal. These results demonstrate how neural circuits achieve efficient representations using dynamic hyperbolic geometry.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {insightful,Learning and memory,Neural encoding,to study},
  file = {/Users/daniekru/Zotero/storage/VR3WIJLY/Zhang et al. - 2023 - Hippocampal spatial representations exhibit a hype.pdf}
}

@article{zhaoBrainInspiredDecisionMakingSpiking2018,
  title = {A {{Brain-Inspired Decision-Making Spiking Neural Network}} and {{Its Application}} in {{Unmanned Aerial Vehicle}}},
  author = {Zhao, Feifei and Zeng, Yi and Xu, Bo},
  year = {2018},
  month = sep,
  journal = {Frontiers in Neurorobotics},
  volume = {12},
  publisher = {Frontiers},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2018.00056},
  urldate = {2024-05-03},
  abstract = {Decision-making is a crucial cognitive function for various animal species surviving in nature, and it is also a fundamental ability for intelligent agents. To make a step forward in the understanding of the computational mechanism of human-like decision-making, this paper proposes a brain-inspired decision-making spiking neural network (BDM-SNN) and applies it to decision-making tasks on intelligent agents. This paper makes the following contributions: (1) A spiking neural network (SNN) is used to model human decision-making neural circuit from both connectome and functional perspectives. (2) The proposed model combines dopamine and spike-timing-dependent plasticity (STDP) mechanisms to modulate the network learning process, which indicates more biological inspiration. (3) The model considers the effects of interactions among sub-areas in PFC on accelerating the learning process. (4) The proposed model can be easily applied to decision-making tasks in intelligent agents, such as an unmanned aerial vehicle (UAV) flying through a window and a UAV avoiding an obstacle. The experimental results support the effectiveness of the model. Compared with traditional reinforcement learning and existing biologically inspired methods, our method contains more biologically-inspired mechanistic principles, has greater accuracy and is faster.},
  langid = {english},
  keywords = {brain-inspired decision-making,Dopamine Regulation,multiple brain areas coordination,reinforcement learning model,Spiking Neural network,UAV autonomous learning},
  file = {/Users/daniekru/Zotero/storage/NYWWHQ9Q/Zhao et al. - 2018 - A Brain-Inspired Decision-Making Spiking Neural Ne.pdf}
}

@article{zylberbergMechanismsPersistentActivity2017,
  title = {Mechanisms of {{Persistent Activity}} in {{Cortical Circuits}}: {{Possible Neural Substrates}} for {{Working Memory}}},
  shorttitle = {Mechanisms of {{Persistent Activity}} in {{Cortical Circuits}}},
  author = {Zylberberg, Joel and Strowbridge, Ben W.},
  year = {2017},
  month = jul,
  journal = {Annual Review of Neuroscience},
  volume = {40},
  number = {Volume 40, 2017},
  pages = {603--627},
  publisher = {Annual Reviews},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-070815-014006},
  urldate = {2024-05-13},
  abstract = {A commonly observed neural correlate of working memory is firing that persists after the triggering stimulus disappears. Substantial effort has been devoted to understanding the many potential mechanisms that may underlie memory-associated persistent activity. These rely either on the intrinsic properties of individual neurons or on the connectivity within neural circuits to maintain the persistent activity. Nevertheless, it remains unclear which mechanisms are at play in the many brain areas involved in working memory. Herein, we first summarize the palette of different mechanisms that can generate persistent activity. We then discuss recent work that asks which mechanisms underlie persistent activity in different brain areas. Finally, we discuss future studies that might tackle this question further. Our goal is to bridge between the communities of researchers who study either single-neuron biophysical, or neural circuit, mechanisms that can generate the persistent activity that underlies working memory.},
  langid = {english},
  file = {/Users/daniekru/Zotero/storage/MKX56IQL/Zylberberg and Strowbridge - 2017 - Mechanisms of Persistent Activity in Cortical Circ.pdf;/Users/daniekru/Zotero/storage/GDPP3LU5/annurev-neuro-070815-014006.html}
}
