
\section{Appendix}\label{sec:appendix}


\subsection{Activation function}
\noindent The function $\Phi_{\cdot}$ is defined by combining a generalized version of the sigmoid, namely with a gain $\beta \neq 1$ and offset $\alpha\neq 0$, and a Gaussian with mean $\mu$ and variance $\sigma$. Their contributions are weighted by as $r$ and $1-r$ ($r\in(0,1)$) respectively.

\begin{equation*}
    \Phi_v(x) = r\left(1 + \exp^{-\beta(x-\alpha)}\right)^{-1} + (1-r)\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation*}

\noindent The motivation behind this choice is to express a function that possesses a bounded region (depending on $\mu,\,\sigma$) at a high/low peak (depeding on the value of $\gamma_{2}$), and a continuous transition to a constant value (depending on the steepness of the sigmoid $\beta$, shift
$\alpha$, and intensity $\gamma_{1}$).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/gaussian_sigmoid.png}
    \caption{\textsc{Activation function $\Phi_{v}$} - \textit{Parameters $\beta=10$, $\alpha=1$, $\mu=1$, $\sigma=1$, and $r=0.5$.}}
    \label{fig:gau_sigm}
\end{figure}


\subsection{Reward distribution entropy}\label{sec:appendix_entropy}

\noindent The calculation of a set of $N$ reward probability distribution $\mathbf{p}_{i}\text{  for  } i\ldots N$ for $K$ values with a progressively decreasing levels of entropy $\mathbf{h}_{i}\text{  for  } i\ldots N$ has been obtained by the following algorithm:

\begin{algorithm}[ht]
\caption{Reward Probability Distribution Generation}
\label{alg:reward_distribution}
\SetAlgoLined
\KwIn{Number of distributions $N$, dimension $K$}
\KwOut{Set of probability distributions ${\mathbf{p}_i}$ with decreasing entropy}
\SetKwComment{Comment}{// }{ }
\textbf{Initial Setup:}
Define set $B = \{1.5^x \mid x = 1, \ldots, 7\}$; \\
\For{$i \gets 1$ to $N$}{
$\mathbf{z} \gets \text{RandomVector}(0,1)^K$;\\
$j \gets \text{RandomIndex}(K)$;\\
$\mathbf{z}_j \gets 1$;\\
$\beta_i \gets \text{Sample index=} i \text{ from }(B)$ \Comment*[r]{Sample temperature from $B$}

$\mathbf{p}_i \gets \frac{\exp(\beta_i \mathbf{z})}{\sum_j \exp(\beta_i \mathbf{z}_j)}$ \Comment*[r]{Softmax with temperature}
}
\Return ${\mathbf{p}_i}$
\end{algorithm}


\subsection{Table of results}

% --- table K.5
\begin{table}[H]
\centering
\caption{Performance comparison for $K=5$}
\label{tab:k5}
\begin{tabular}{l c c c}
\toprule
\textbf{Model} & \textbf{\textsc{KAB-0}} & \textbf{\textsc{KAB-$\epsilon$}} & \textbf{\textsc{KAB-$\sin$}}\\
\midrule
Optimal & $0.900$ & $0.881$ & $0.563$ \\
Random & $0.330$ & $0.337$ & $0.200$ \\
\midrule
Thompson & $0.905$ & $0.617$ & $0.317$ \\
$\epsilon$-Greedy & $0.797$ & $0.531$ & $0.315$ \\
UCB & $0.897$ & $0.656$ & $0.319$ \\
\textbf{Model} & $\mathbf{0.899}$ & $\mathbf{0.663}$ & $\mathbf{0.265}$ \\

\bottomrule
\end{tabular}
\end{table}

% --- table K.10
\begin{table}[H]
\centering
\caption{Performance comparison for $K=10$}
\label{tab:k10}
\begin{tabular}{l c c c}
\toprule
\textbf{Model} & \textbf{\textsc{KAB-0}} & \textbf{\textsc{KAB-$\epsilon$}} & \textbf{\textsc{KAB-$\sin$}} \\
\midrule
Optimal & $0.900$ & $0.885$ & $0.355$  \\
Random & $0.247$ & $0.250$ & $0.100$ \\
\midrule
Thompson & $0.896$ & $0.648$ & $0.339$ \\

$\epsilon$-Greedy & $0.611$ & $0.597$ & $0.343$ \\
UCB & $0.891$ & $0.655$ & $0.358$ \\
\textbf{Model} & $\mathbf{0.905}$ & $\mathbf{0.668}$ & $\mathbf{0.203}$  \\
\bottomrule
\end{tabular}
\end{table}

% --- table K.100
\begin{table}[H]
\centering
\caption{Performance comparison for $K=100$}
\label{tab:k100}
\begin{tabular}{l c c c}
\toprule
\textbf{Model} & \textbf{\textsc{KAB-0}} & \textbf{\textsc{KAB-$\epsilon$}} & \textbf{\textsc{KAB-$\sin$}} \\
\midrule
Optimal & $0.900$ & $0.883$ & $0.020$  \\
Random & $0.196$ & $0.201$ & $0.010$ \\
\midrule
Thompson & $0.894$ & $0.586$ & $0.013$ \\
$\epsilon$-Greedy & $0.519$ & $0.574$ & $0.018$ \\
UCB & $0.853$ & $0.572$ & $0.012$ \\
\textbf{Model} & $\mathbf{0.898}$ & $\mathbf{0.651}$ & $\mathbf{0.010}$  \\
\bottomrule
\end{tabular}
\end{table}

% --- table K.200
\begin{table}[H]
\centering
\caption{Performance comparison for $K=100$}
\label{tab:k200}
\begin{tabular}{l c c c}
\toprule
\textbf{Model} & \textbf{\textsc{KAB-0}} & \textbf{\textsc{KAB-$\epsilon$}} & \textbf{\textsc{KAB-$\sin$}} \\
\midrule
Optimal & $0.900$ & $0.885$ & $0.010$  \\
Random & $0.178$ & $0.176$ & $0.005$ \\
\midrule
Thompson & $0.875$ & $0.624$ & $0.006$ \\
$\epsilon$-Greedy & $0.679$ & $0.588$ & $0.010$ \\
UCB & $0.792$ & $0.510$ & $0.006$ \\
\textbf{Model} & $\mathbf{0.905}$ & $\mathbf{0.610}$ & $\mathbf{0.006}$  \\
\bottomrule
\end{tabular}
\end{table}

% --- table K.1000
\begin{table}[H]
\centering
\caption{Performance comparison for $K=100$}
\label{tab:k1000}
\begin{tabular}{l c c c}
\toprule
\textbf{Model} & \textbf{\textsc{KAB-0}} & \textbf{\textsc{KAB-$\epsilon$}} & \textbf{\textsc{KAB-$\sin$}} \\
\midrule
Optimal & $0.900$ & $0.880$ & $0.002$  \\
Random & $0.177$ & $0.178$ & $0.001$ \\
\midrule
Thompson & $0.779$ & $0.445$ & $0.001$ \\
$\epsilon$-Greedy & $0.386$ & $0.478$ & $0.002$ \\
UCB & $0.301$ & $0.185$ & $0.001$ \\
\textbf{Model} & $\mathbf{0.703}$ & $\mathbf{0.480}$ & $\mathbf{0.001}$  \\
\bottomrule
\end{tabular}
\end{table}


\newpage
