

\section{Methods}

% brief outline of the section
\noindent The following section is organized as follows. First, we introduce a formalization of general problem setting, together with the variants considered in this work. Then, we outline the architecture of our model and how it can be mapped to neurobiology. Finally, we describe the learning procedure,
and showcase its dynamics in a simple example.

% mathematical formulation of the k-armed bandit problem.
\subsection{Binomial K-armed bandit problem}
\hfill \break
\noindent The standard formulation of the task is structured as a set of $K$ arms (or levers) $\mathcal{A}_{K}=\{a_{1}\ldots a_{K}\}$, with an associated reward distribution $\mathbf{p}=\{p_{1}, \ldots p_{K}\}$.
At each iteration, the agent pulls an arm and collects a possible reward drawn as a Bernoulli variable $R\sim \mathcal{B}(\{0,1\},p_{k})$. The agent's objective is maximizing the total reward $\sum^{T}_{t} R_{t}$, after a certain number of rounds $T$, also called horizon.
Importantly, the agent is unaware of the true reward probabilities, and thus has to make its decisions following a certain policy, denoted as $\pi$.
In the reinforcement learning literature, the policy is often defined as a distribution over actions, here the arms $\mathcal{A}_{K}$, given the current state at time $t$. In the bandit problem, the state can be taken to correspond to the history $h_{t}$ of past actions and rewards in the period
$(0\ldots t]$, and the policy as a function that return a selected arm $\pi(h_{t})=a_{t}$ \cite{qiForcedExplorationBandit2023}.

Given the inherent stochasticity of the feedbacks from the environment, the policy is affected by the so-called exploration-exploitation trade-off, which here is phrased as the contrast between the option of the arm with the estimated highest expected reward versus the option to explore other arms, so to gather more information.
A common approach is the $\epsilon-$greedy policy, where the choice to explore is selected with a probability $\epsilon$.
Moreover, it is often preferable to have a more explorative behaviour early during the training, with the intent to have a good sample size for the empirical reward distribution, which can be later exploited for maximizing reward.

% regret
Another important concept in multi-armed bandit problems is \textit{regret}. Intuitively, it quantifies the loss of reward due to following a certain policy, and it is determined by the difference beween the collected reward and the theoretical optimal, obtained by choosing the best arm at each round.
Formally, given defined a function $r(\pi)$ which returns the expected reward while following policy $\pi$, the regret $\rho$ over an horizon $T$ can be formulated as:
\begin{equation}
    \rho = \frac{1}{T}\sum^{T}_{t} p^{*}_{t} -r(\pi(h_{t}))
\end{equation}

\noindent where $p^{*}_{t}$ is the expected reward of the optimal arm at time $t$, which correspond to its probability since it is a Bernoulli distribution.
\noindent The goal of the agent is to minimize the regret, and thus maximize the total reward.

% minimal model description
\input{minmodel}



