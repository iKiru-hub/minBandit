

\section{Methods}

% brief outline of the section
\noindent The section is organized as follows. First, we introduce a formalization of general problem setting, together with the variants considered in this work. Then, we outline the architecture of the our model and how it can be mapped to neurobiology. Finally, we describe the learning procedure,
and showcase its dynamics in a simple example.

% mathematical formulation of the k-armed bandit problem.
\subsection{Binomial K-armed bandit problem}
\hfill \break
\noindent The standard formulation of the task is structured as a set of $\{1\dots K\}$ levers (or arms), with an associated reward distribution $\mathbf{p}=\{p_{1}, \ldots p_{K}\}$. At each iteration, the agent pulls a lever and collect a possible reward drawn as a Bernoulli variable $R\sim
\mathcal{B}(\{0,1\},p_{k})$. The agent's objective is maximizing the total reward
$\sum^{T}_{t} R_{t}$, after a certain number $T$ of trials. Importantly, the agent is unaware of the true reward probability distribution, and thus has to make its decisions following a certain policy, usually denoted as $\pi$. In the reinforcement learning literature, the policy is often defined as
a distribution over the actions, here the levers $K$, given the current state, which in this case can be the history of past actions and rewards up to time $t\leq T$. Given the inherent stochasticity of the feedbacks from the environment, the definition of the policy is affected by the so-called
exploration-exploitation trade-off, which here is phrased as the contrast between the option of the lever with the known highest expected reward versus the option to explore other levers, so to gather more information. A common approach is the $\epsilon-$greedy policy, where the choice to explore is
selected with a probability $\epsilon$. Moreover, it is often preferable to have a more explorative behaviour early during the training, with the intent to have a good sample size for the empirical reward distribution, which can be later exploited for maximizing reward.\\
Another important concept in multi-armed bandit problems is the \textit{regret}. Intuitively, it is defined as the deviation of the total reward obtained by the agent from the optimal reward that could have been obtained by always choosing the lever with the highest expected reward. Formally, the regret is defined as:
\begin{equation}
    \rho=R^{*} - \sum^{T}_{t} R_{t}
\end{equation}

\noindent where $R^{*}$ is the reward obtained by always choosing the lever with the highest expected reward $R^{*}=T\max_{k}\{p_{k}\}$, and $R_{t}$ is the empirical reward obtained up to time $t$ by following policy $\pi$ as $R_{t}=\sum^{T}_{t=1}\pi_{\theta}(t)$.
The regret is a measure of the performance of the agent, and it is often used to compare different algorithms. The goal of the agent is to minimize the regret, and thus maximize the total reward.

% \hfill \break
\subsubsection{Non-stationary environment}
% \textbf{Goal} \\
\noindent The setting we consider in this work is a non-stationary environment with Binomial rewards. In particular, the agent is evaluated over a number $T_{\text{trials}}$ of \textit{trials}, each composed by an arbitrary number $T$ of \textit{rounds}; each \textit{trial} is characterized by a different
reward distribution $\mathbf{p}\sim\mathcal{U}(0,1)^{K}$ (although in practice the bounds have been set to $(0.1, 0.8)$ such that the distributions are less trivial). \\
\noindent Our goal in this work is to investigate the performance of the agent in a non-stationary environment with Binomial reward distributions, meaning that its underlying distribution changes over time.
We choose this setting as it resembles an ecological scenario in which an animal has to forage in a patchy environment, where the reward of a given patch can change over time.

\noindent In particular, we consider two different types of non-stationarity.\\
\textbf{zero-steps distribution shift}: the reward distribution changes immediately at the end of a trial $i$ to a new one $i+1$ as $\mathbf{\pi}_{i} \to \mathbf{\pi}_{i+1}$.

\noindent \textbf{epsilon-steps distribution shift}: the reward distribution $\pi_{t}$ at time $t$ changes gradually over time, and it tends to a target distribution $\hat{\pi}_{i}$ as $\tau_{\pi}\dot{\pi}_{t}=\hat{\pi}_{i}-\pi_{t}$. Once distance is below a threshold $\epsilon$ as $\vert \hat{\pi}_{i} - \pi_{t}\vert < \epsilon$, the target distribution is changed to a new one $\hat{\pi}_{i}\to\hat{\pi}_{i+1}$.

We will investigate the performance of the model in both settings, and compare it to the common benchmark algorithms.


