
\section{Introduction}
\hfill \break
\vspace {0.5cm}

% brief introduction to decision making and the brain.
The ability to make decisions for long-term reward maximization is a fundamental aspect of cognition. The brain has evolved a complex web of interconnected regions that work together to express this behaviour under the constraints of biology. The Pre-Frontal Cortex (PFC) is considered
a fundamental high-level region for the attention and cognitive control, in particular the medial PFC \cite{millerIntegrativeTheoryPrefrontal2001a, sheynikhovichLongtermMemorySynaptic2023}.
Further, the orbitofrontal cortex (OFC) is thought to be involved in motivation and representation of the expected value of the actions, either positive or negative  \cite{odohertyAbstractRewardPunishment2001, ricebergRewardStabilityDetermines2012, tremblayRelativeRewardPreference1999}, and action
selection in uncertain environments \cite{elliottDissociableFunctionsMedial2000}.
A relevant element for online executive functions is working memory, which is usually defined as the capacity to hold and manipulate information over short periods of time \cite{baddeleyWorkingMemory1974}; thus functionality 
has been associated with the dorsolateral PFC \cite{dardenneRolePrefrontalCortex2012, cohenTemporalDynamicsBrain1997, constantinidisPersistentSpikingActivity2018, zylberbergMechanismsPersistentActivity2017}.
However, it has also been suggested that the PFC is instead exercising a more top-down control over more sensory regions \cite{laraRolePrefrontalCortex2015}. These cortical projections have been proposed to target also the basal ganglia, which are thought to rely on first-order reward statistics,
while the OFC is able to capture more complex contextual dynamics \cite{frankAnatomyDecisionStriatoorbitofrontal2006}.

% bridge between decision making and the k-armed bandit problem.
Considering decision-making, simple and well-studied ecological settings are foraging tasks, such as food search. In this problems, the agent is usually asked to choose between different options to maximize the expected reward.
In nature, animals have been shown to exhibit different strategies depending on context.
\textit{matching behaviour} is a well-known phenomenon in which the animal's decision patterns are proportional to the reward probability of the available options.
This behaviour is thought to result from the trade-off between exploration and exploitation \cite{suttonReinforcementLearningProblem1998, nivEvolutionReinforcementLearning2002}.
This is a well known phenomenon in the reinforcement learning literature, in which the agent is faced with the dilemma of exploring new alternatives, potentially more rewarding, or exploiting known options, although possibly sup-optimal.
Other behaviours dependant on contexts are \textit{input matching}, where social cues are considered, and \textit{probability matching}, where the animal's choice behaviour is proportional to the reward probability of the options \cite{bariDynamicDecisionMaking2021, houstonMatchingBehavioursRewards2021}.
When considering more computational approaches in the study of choice behaviour, a popular formalization of such tasks is the \textit{multi-armed bandit problem} (MABP) \cite{averbeckTheoryChoiceBandit2015}. This settins is usually described in terms of a slot machine endowed with $K$ distinct levers.
During a round, the agent selects one of the levers and collects a reward $R$ according to an unknown reward probability specific to the chosen lever.
The goal is simply to maximizes the total reward after a given number steps, which is achieved by effectively updating a selection policy after each round.
This problem has been extensively studied in the context of reinforcement learning, and it is considered a fundamental building block for more complex tasks \cite{suttonReinforcementLearningProblem1998}.

% brief introduction of the k-armed bandit problem and previous algorithms.
\subsection{Related work}
% overview of the main solutions proposed in the literature from a computational perspective.
There exists various flavours of this problem, with the simplest having a stationary reward distribution, while the more challening ones have have \textit{concept drift}, where the reward distribution changes over time.
Over the years, several algorithms have been proposed, alongside with their theoretical guarantees. In this regard, Thompson sampling is a popular algorithm that has been shown to achieve near-optimal regret bounds in the stochastic setting \cite{agrawalAnalysisThompsonSampling2012}, which 
a Bayesian approach  the idea of maintaining a posterior distribution over the reward probabilities of the actions, and selecting actions according to the posterior distribution. Another popular algorithm is the Upper Confidence Bound (UCB) algorithm, which has been shown to achieve near-optimal
regret bounds in the adversarial setting \cite{auerFinitetimeAnalysisMultiarmed2002}. The algorithm is based on the idea of maintaining an upper confidence bound on the reward probabilities of the actions, and selecting actions according to the upper confidence bound.
\\
% outline of lack of bio-realism and what we currently know about how the brain might solve this task.
Despite the success of these algorithms in solving the k-armed bandit problem, they lack biological plausibility. In contrast, the brain has evolved a complex network of interconnected regions that work together to solve this task. In particular, the dopamine-acetylcholine system has been shown to
play a crucial role in learning and decision making \cite{dayanDecisionTheoryReinforcement2008}.

% aim of the work
In this work, we focus on a stochastic bandit problem, a more challenging variant of the original task endowed with \textit{concept drift}, where the reward distribution changes over time \cite{garivierUpperConfidenceBoundPolicies2008, besbesStochasticMultiArmedBanditProblem2014, cavenaghiNonStationaryMultiArmed2021}.
We propose a biologically plausible model using spiking neural networks (SNN), obtaining good performance over an arbitrary number of bandit trials.
Its architecture is composed of two parts. The first is a working memory component, whose scope is to maintain an active representation of the current stimulus, and it is inspired by the functionality of the dorsolateral PFC. Several previous studies have proposed bio-realistic models of WM
\cite{barakWorkingModelsWorking2014}, including focus on synaptic facilitation \cite{barakNeuronalPopulationCoding2010}, random connectivity \cite{bouchacourtFlexibleModelWorking2019}, excitation-inhibition balance (E-I)
\cite{brunelEffectsNeuromodulationCortical2001, vogelsGatingMultipleSignals2009}, and echo state networks \cite{pascanuNeurodynamicalModelWorking2011, fetteShortTermMemory2005}. Here in particular, we used a three-population SNN exploiting the E-I balance to maintain the stimulus trace, similarly to
\cite{chenSpikingNeuralNetwork2023}.
The second component is a one-layer hybrid network, and it is where a pre-defined architectural policy assign a subjective value to the available options \textit{i.e.} the bandits.
This component is modeled after the interaction between the OFC and the basal ganglia, in particular the striatum
\cite{bariDynamicDecisionMaking2021, frankAnatomyDecisionStriatoorbitofrontal2006}, given the role of the frontal lobe in reversal learning \cite{bartoloPrefrontalCortexPredicts2020},
various network models of the PFC and BG for decision-making have been proposed, centered on the role of dopamine in the basal ganglia \cite{bastonBiologicallyInspiredComputational2015}, spike-time dependent plasticity (STPD) \cite{kannanUnsupervisedSpikingNeural2023}, cortico-striatal interaction
\cite{frankInteractionsFrontalCortex2001}, and task switching processes \cite{zhaoBrainInspiredDecisionMakingSpiking2018, herdNeuralNetworkModel2014}.

Regarding our model, the novelty relies in two main elements. The first is the decision making process itself, implemented as a dynamic population interaction between the neural traces active in working memory and the values represented in the hybrid network, similarly to bump attractor networks for
perceptual computations \cite{carrollEncodingCertaintyBump2014, esnaola-acebesBumpAttractorDynamics2021}.
The second is learning, which is applied to the hybrid network, and it is based on a neuromodulated Hebbian-like synaptic plasticity rule with special weight-dependent kernels for long-term potentiation (LTP) and long-term depression (LTD).
This choice is motivated by the asymmetry of the dopamine signal in supporting LTP or LTD \cite{schultzNeuralSubstratePrediction1997, toblerAdaptiveCodingReward2005, reynoldsDopaminedependentPlasticityCorticostriatal2002, madadiaslDopaminergicModulationSynaptic2019}.


