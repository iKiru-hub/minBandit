
\section{Introduction}
\hfill \break
\vspace {0.5cm}

% brief introduction to decision making and the brain.
The ability to make decisions for long-term reward maximization is a fundamental aspect of cognition. The brain has evolved a complex web of interconnected regions that work together to express this behaviour under the constraints of biology. The Pre-Frontal Cortex (PFC) is considered
a fundamental high-level region for the attention and cognitive control, in particular the medial PFC \cite{millerIntegrativeTheoryPrefrontal2001a, sheynikhovichLongtermMemorySynaptic2023}.
Further, the orbitofrontal cortex (OFC) is thought to be involved in motivation and representation of the expected value of the actions, either positive or negative  \cite{odohertyAbstractRewardPunishment2001, ricebergRewardStabilityDetermines2012, tremblayRelativeRewardPreference1999}, and action
selection in uncertain environments \cite{elliottDissociableFunctionsMedial2000}.
A relevant element for online executive functions is working memory, which is usually defined as the capacity to hold and manipulate information over short periods of time \cite{baddeleyWorkingMemory1974}; thus functionality 
has been associated with the dorsolateral PFC \cite{dardenneRolePrefrontalCortex2012, cohenTemporalDynamicsBrain1997, constantinidisPersistentSpikingActivity2018, zylberbergMechanismsPersistentActivity2017}.
However, it has also been suggested that the PFC is instead exercising a more top-down control over more sensory regions \cite{laraRolePrefrontalCortex2015}. These cortical projections have been proposed to target also the basal ganglia, which are thought to rely on first-order reward statistics,
while the OFC is able to capture more complex contextual dynamics \cite{frankAnatomyDecisionStriatoorbitofrontal2006}.

% bridge between decision making and the k-armed bandit problem.
Considering decision-making, simple and well-studied ecological settings are foraging tasks, such as food search. In this problems, the agent is usually asked to choose between different options to maximize the expected reward.
In nature, animals have been shown to exhibit different strategies depending on context.
\textit{matching behaviour} is a well-known phenomenon in which the animal's decision patterns are proportional to the reward probability of the available options.
This behaviour is thought to result from the trade-off between exploration and exploitation \cite{suttonReinforcementLearningProblem1998, nivEvolutionReinforcementLearning2002}.
This is a well known phenomenon in the reinforcement learning literature, in which the agent is faced with the dilemma of exploring new alternatives, potentially more rewarding, or exploiting known options, although possibly sup-optimal.
Other behaviours dependant on contexts are \textit{input matching}, where social cues are considered, and \textit{probability matching}, where the animal's choice behaviour is proportional to the reward probability of the options \cite{bariDynamicDecisionMaking2021, houstonMatchingBehavioursRewards2021}.
When considering more computational approaches in the study of choice behaviour, a popular formalization of such tasks is the \textit{multi-armed bandit problem} (MABP) \cite{averbeckTheoryChoiceBandit2015}. This settins is usually described in terms of a slot machine endowed with $K$ distinct levers.
During a round, the agent selects one of the levers and collects a reward $R$ according to an unknown reward probability specific to the chosen lever.
The goal is simply to maximizes the total reward after a given number steps, which is achieved by effectively updating a selection policy after each round.
This problem has been extensively studied in the context of reinforcement learning, and it is considered a fundamental building block for more complex tasks \cite{suttonReinforcementLearningProblem1998}.

% brief introduction of the k-armed bandit problem and previous algorithms.
\subsection{Related work}
% overview of the main solutions proposed in the literature from a computational perspective.
There exists various flavours of this problem, with the simplest having a stationary reward distributione.

Over the years, several algorithms have been proposed, alongside with their theoretical guarantees.
In this regard, Thompson sampling is a popular algorithm that has been shown to achieve near-optimal regret bounds in the stochastic setting \cite{agrawalAnalysisThompsonSampling2012, kaufmannThompsonSamplingAsymptotically2012}.
This approach relies on Bayesian optimization, where the goal is to maintain a posterior distribution over the reward probabilities of the actions, and selecting actions accordingly.
Another popular algorithm is Upper Confidence Bound (UCB), which has been shown to achieve near-optimal regret bounds in the adversarial setting \cite{auerFinitetimeAnalysisMultiarmed2002}.
The approach is based on the idea of maintaining an upper limit on the reward probabilities of the actions, and selecting actions accordingly. 
Other successful algorithms are $\epsilon$-greedy and VDBE \cite{gittinsBanditProcessesDynamic1979, , banMultifacetContextualBandits2021, tokicAdaptiveEGreedyExploration2010, tokicValueDifferenceBasedExploration2011}.\\

% outline of lack of bio-realism and what we currently know about how the brain might solve this task.
Despite the success of these algorithms in solving the k-armed bandit problem, they lack biological plausibility.
The brain has evolved a complex network of interconnected regions that work together to solve this task.

% aim of the work
In this work, we focuses on stochastic bandit problems, more challenging variant of the original task endowed with \textit{concept drift}, where the reward distribution changes over time \cite{garivierUpperConfidenceBoundPolicies2008, besbesStochasticMultiArmedBanditProblem2014, cavenaghiNonStationaryMultiArmed2021}.

% brief overview of the model
We propose a biologically plausible model using rate neurons, obtaining good performance even with settings with a large amount of arms.
Its architecture is composed of two connected neuronal layers, both with as many neurons as the arm of the bandit task.
The first layer is inspired by the functionality of the OFC, and its scope is to maintain an active representation of the arms weighted by the input from the second layer.
The second, modeled after the ACC, is meant to represent the value of the arms, and its input connections are updated through a learning rule dependant on the reward history and current connectivity pattern.


% main motivations
Our models features two important aspects of brain decision making. Firstly, the decision making process itself is implemented as a dynamical interaction between neural populations, similarly to bump attractor networks for perceptual computations \cite{carrollEncodingCertaintyBump2014, esnaola-acebesBumpAttractorDynamics2021}.
The final choice of the arm is achieved by the agreement or disagreement between the two populations, and it depends on their underlying value representation \cite{bariDynamicDecisionMaking2021, houstonMatchingBehavioursRewards2021}.
Secondly, plasticity is based on a non-associative learning rule, endowed with a non-linear kernel for the weight update quantity.
Importantly, the specific values of a parametrization of the kernel has been optimized through an evolutionary algorithm.
Behind this design choice there is our hypothesis that the scale of neural synaptic updates should vary according to its magnitudeand with a non-linear shape of its scaling function represent a better choice that a constant one.
This considerations align with the idea that the brain is able to adapt its learning rate according to the context, and that the learning rate is not constant across neurons within the same network.
This approach has been already adopted in several computational architectures, for instance using spiking neurons \cite{inglisModulationDopamineAdaptive2021} and synaptic metaplasticity \cite{iigayaAdaptiveLearningDecisionmaking2016}.
Lastly, there is experimental evidence that this function of adaptive plasticity might be covered by dopamine \cite{toblerAdaptiveCodingReward2005}.
Indeed, it is role in prediction error and reward signaling is well established \cite{schultzNeuralSubstratePrediction1997}, together with its importance in modulating processes in the PFC \cite{didomenicoDopaminergicModulationPrefrontal2023, lohaniDopamineModulationPrefrontal2019,
dardenneRolePrefrontalCortex2012}. 
