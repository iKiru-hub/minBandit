
\section{Introduction}
% \hfill \break
% \vspace {0.5cm}

% brief introduction to decision making and the brain.
The ability to make decisions for long-term reward maximization is a fundamental aspect of cognition. The brain has evolved specialized and interconnected regions to implement this behaviour under the constraints of biology.

% bridge between decision making and the k-armed bandit problem.
Well-studied ecological settings of decision-making are foraging tasks, such as food search. In these problems, the agent is usually asked to choose between different options to maximize an expected reward.
In nature, animals have been shown to exhibit different strategies depending on context.
\textit{Matching behaviour} is a well-known phenomenon in which the animal's decision patterns are proportional to the reward probability of the available options.
Such behaviour is thought to result from the trade-off between exploration and exploitation \cite{suttonReinforcementLearningProblem1998, nivEvolutionReinforcementLearning2002}.
In fact, this is a well known phenomenon in the reinforcement learning literature, in which an agent is faced with the dilemma of exploring new alternatives, potentially more rewarding, or exploiting known options, despite being possibly sup-optimal.

A popular formalization of these type of tasks is the \textit{multi-armed bandit} problem (MAB) \cite{averbeckTheoryChoiceBandit2015}. This setting is usually described in terms of a slot machine endowed with $K$ distinct arms, also called levers.
During a round, the agent selects one of the arms and collects a reward $R$ according to an unknown reward probability specific to the chosen arm.
The goal is simply to maximize the total reward after a given number of steps, which is achieved by effectively updating a selection policy after each round.
This problem has been extensively studied in the context of reinforcement learning, and it is considered a fundamental building block for more complex tasks \cite{suttonReinforcementLearningProblem1998}.

% brief introduction of the k-armed bandit problem and previous algorithms.
% \subsection{Related work}
% \hfill \break

The multi-armed bandit problem comes in several variants, with the simplest featuring a stationary reward distribution. Researchers have proposed numerous algorithms to address this problem, each with distinct theoretical guarantees.
Thompson sampling, a widely adopted approach based on Bayesian optimization, maintains a posterior distribution over action reward probabilities and has demonstrated near-optimal regret bounds in stochastic settings \cite{agrawalAnalysisThompsonSampling2012, kaufmannThompsonSamplingAsymptotically2012}.
Similarly, the Upper Confidence Bound (UCB) algorithm achieves near-optimal regret bounds in adversarial settings by maintaining upper limits on action reward probabilities \cite{auerFinitetimeAnalysisMultiarmed2002}.
Other notable approaches include $\epsilon$-Greedy and VDBE \cite{gittinsBanditProcessesDynamic1979, banMultifacetContextualBandits2021, tokicAdaptiveEGreedyExploration2010, tokicValueDifferenceBasedExploration2011}.
However, these traditional algorithms, despite their effectiveness, lack biological plausibility â€“ they neither resemble neural circuits nor capture brain dynamics. While not the primary driver, these limitations align with a growing intereset in machine learning towards bio-inspired algorithms,
such as neural networks and predictive coding \cite{leeBraininspiredPredictiveCoding2022, spratlingReviewPredictiveCoding2017}, offering several advantages.
Indeed, these methods can achieve state-of-the-art performance in various domains, including the challenging \textit{machine-challenging tasks} (MCTs), set of problems that are hard for machines but relatively easy for humans \cite{schmidgallBraininspiredLearningArtificial2024, hassabisNeuroscienceInspiredArtificialIntelligence2017, leeBraininspiredPredictiveCoding2022}.
Further, bio-inspired models enhance algorithmic interpretability by clarifying the functional relationships between internal components. When applied to tasks with existing experimental data, these models can generate new insights into brain dynamics and suggest novel research directions \cite{liuSeeingBelievingBrainInspired2023}.
Finally, given the brain's remarkable energy efficiency, architectures inspired by its design may significantly reduce power consumption \cite{EvaluationBioInspiredModels}, particularly when implemented on specialized neuromorphic hardware \cite{ReviewNeuroscienceInspiredMachine}.
\hfill \break
% aim of the work
\indent In this work, we aimed at improving the biological plausibility of algorithms within multi-armed bandit tasks by introducing a novel model comprising two interacting rate neuron populations exhibiting synaptic plasticity.
Our model addresses stochastic bandit problems with \textit{concept drift} - scenarious where the reward distribution changes over time \cite{garivierUpperConfidenceBoundPolicies2008, besbesStochasticMultiArmedBanditProblem2014, cavenaghiNonStationaryMultiArmed2021}.
The proposed model achieves performance comparable to established algorithms like Thompson Sampling, $\epsilon$-Greedy and Upper Confidence Bound, while offering a more neurobiologically grounded interpretation of the decision-making process.

Architecturally, it is inspired by the prefrontal cortex, specifically modeling two critical regions: the orbitofrontal cortex (OFC) and anterior cingulate cortex (ACC). These brain areas are involved in motivational processing, expected value representation, action selection under uncertainty, and contextual interpretation \cite{odohertyAbstractRewardPunishment2001, ricebergRewardStabilityDetermines2012, tremblayRelativeRewardPreference1999, elliottDissociableFunctionsMedial2000, frankAnatomyDecisionStriatoorbitofrontal2006}.
The model's plasticity mechanism is based on a non-associative rule dependent on inter-population connection weight magnitudes. This approach aligns with synapse-type specific plasticity (STSP), a mechanism supported by emerging evidence linking learning rates to synaptic resource availability,
current states, and morphological characteristics \cite{larsenSynapsetypespecificPlasticityLocal2015, blackmanTargetcellspecificShorttermPlasticity2013, bartolHippocampalSpineHead2015, arielIntrinsicVariabilityPv2012}. Previous work has made use of such plasticity methods in spiking neural networks and synaptic metaplasticity models \cite{inglisModulationDopamineAdaptive2021, iigayaAdaptiveLearningDecisionmaking2016}.

Finally, the significance of our work is to bridge the gap between adaptive decision making under uncertainty and computational neuroscience.
Our approach represents a possible way for how neural systems may implement choice selection in non-stationary environments, providing insights that could inform both artificial intelligence design and experimental research on adaptive behavior.

\hfill \break
% structure of the paper
The remainder of this paper first describes our model design and learning, then presents experimental results and comparative analyses with established algorithms, lastly discusses the findings' broader implications and potential future directions.

