
\section{Discussion}


% work on the k-armed bandit problem and neuroscience
The process of making decision in uncertain settings is a remarkable aspect of cognition. For instance, such behaviour is implemented in animals during foraging and matching behaviour.
In the context of humans, it has been observed that the pool of adopted policies vary considerably \cite{steyversBayesianAnalysisHuman2009a}. However, the subjects seems able to integrate environmental uncertainty and trial generalization in their strategy, and Bayesian algorithms are generally a
good fit for the observed choices \cite{schulzFindingStructureMultiarmed2020, zhangForgetfulBayesMyopic2013}.
A useful formalization of such tasks is the multi-armed bandit problem, which has been extensively studied in the context of reinforcement learning \cite{suttonReinforcementLearningProblem1998}.
Although several algorithms have been proposed to solve the problem with robust theoretical guarentees, there is a general lack of biological plausibility of the architecture and dynamics.

%
In this work, we introduced a rate neural networks to address the binomial K-armed bandit problem in a non-stationary environment.
The results obtained from our model show that it is able to effectively adapt to changing reward distributions and maintain a greedy policy over time, on par with the standard algorithms.

The observed efficiency of our model can be attributed to that attractor dynamics and a non-associative synaptic plasticity rule.

These architectural choices were inspired by real-world decision-making processes observed in humans and other animals.


%meta-plasticity
