
\section{Discussion}


% work on the k-armed bandit problem and neuroscience
In the context of human behaviour, it has been observed that the adopted policies vary considerably \cite{steyversBayesianAnalysisHuman2009a}. However, the subjects seems able to integrate environmental uncertainty and trial generalization in their strategy, and Bayesian algorithms are generally a good fit for the observed behaviour \cite{schulzFindingStructureMultiarmed2020, zhangForgetfulBayesMyopic2013}.


%
In this work, we introduced a model based on spiking neural networks (SNN) that leverages neuromodulated Hebbian-like synaptic plasticity to address the binomial K-armed bandit problem in a non-stationary environment. Our results highlight the model's robustness and adaptability, characteristics that are particularly advantageous in dynamic settings where reward distributions change unpredictably.

The observed efficiency of our SNN model can be attributed to its incorporation of biologically plausible mechanisms, such as dopamine-modulated synaptic plasticity, which is crucial for learning and memory across species \cite{toblerAdaptiveCodingReward2005};
\cite{schulzFindingStructureMultiarmed2020, reynoldsDopaminedependentPlasticityCorticostriatal2002, frankAnatomyDecisionStriatoorbitofrontal2006}, who emphasize the significant role of striato-orbitofrontal interactions in decision-making and learning.

Moreover, the dynamics of our model echo real-world decision-making processes observed in humans and other animals, where decision-making strategies must adapt to changing environmental conditions \cite{nivEvolutionReinforcementLearning2002}. This adaptability is mirrored in the performance of our model, which
effectively handles the zero-steps and epsilon-steps distribution shifts, showcasing its potential to operate under varied and shifting conditions.

Furthermore, the use of a Hebbian-like rule for synaptic updates in our model introduces a level of flexibility and responsiveness that is not commonly found in traditional reinforcement learning algorithms, which often rely on fixed learning rates or reward probabilities
\cite{suttonReinforcementLearningProblem1998}. This approach may explain the superior performance of our model in environments where adaptability is critical \cite{besbesStochasticMultiArmedBanditProblem2014}.

In summary, the results of our study not only support the feasibility of using SNNs for complex decision-making tasks but also highlight the potential of neuromodulatory systems to enhance the adaptability and efficiency of artificial neural networks. Future work could explore the integration of
additional biological elements, such as the role of other neuromodulators like serotonin and acetylcholine, to further improve the model's performance and biological fidelity \cite{coolsChemistryAdaptiveMind2019, dayanDecisionTheoryReinforcement2008}.



