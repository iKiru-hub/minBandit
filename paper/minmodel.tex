
\subsection{Model description}
The model is constructed as a rate network composed of two neuronal populations, \textit{U} and \textit{V}. The first, \textit{U}, represents the memory traces of the \textit{K} available options (\textit{i.e.}, the bandits), while the second, \textit{V}, encodes their values under the current policy. This architecture is inspired by the prefrontal cortex (PFC) and its central role in decision-making processes. Specifically, the two populations correspond to the orbitofrontal cortex (OFC) and the anterior cingulate cortex (ACC), respectively.

The OFC is known to represent different options, updating their values based on reward history and outcomes \cite{lukChoiceCodingFrontal2013, kennerleyDecisionMakingReward2011a}. Meanwhile, the ACC has been associated with action valuation and regulating the balance between exploration and exploitation \cite{khamassiChapter22Medial2013}. Additionally, the dynamic interaction between the ACC and OFC has been linked to transient pre-stimulus activations, which bias decisions toward the most valuable option \cite{funahashiPrefrontalContributionDecisionMaking2017, marcosDeterminingMonkeyFree2016, balewskiValueDynamicsAffect2023}.

In our model, the first layer represents the available options, while learned connections to the second layer encode their values based on recent reward history. A key simplification is the lumping of option representations into single neurons. While this choice abstracts the more distributed encoding found in actual brain networks, it allows for a more tractable model design \cite{martinRepresentationObjectConcepts2007a}.

More formally, the model is defined by a set of coupled ordinary differential equations (ODEs). The first equation describes the evolution of the neural activity $\textbf{u}$ in population \textit{U}, while the second governs the activity $\textbf{v}$ in population \textit{V}, each evolving with its respective time constant $\tau$.

\begin{equation}
\begin{aligned}
    \tau_{u} \dot{\textbf{u}}&= -\textbf{u} + \textbf{W}^{VU}\phi_{v}(\textbf{v}) + \textbf{I}_{\text{ext}} \\
    \tau_{v} \dot{\textbf{v}}&= -\textbf{v} + \widetilde{\textbf{W}}^{UV}\phi_{u}(\textbf{u})
\end{aligned}
\end{equation}\label{eq:main}

\noindent The external input $\textbf{I}_{\text{ext}}$ is a constant input that is used to set the initial conditions of the neural activity $\textbf{u}$.
The activation functions $\phi_{v},\phi_{u}$ are applied to population $v$ and $u$ respectively, and represent two distinct neural response functions tailored to each population vector.
They have been chosen to be a step-function with threshold $\theta_{v},\theta_{u}$ applied to a generalized sigmoid with gain $g_{v},g_{u}$ and offset $s_{v},s_{u}$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/minb_architecture.png}
    \caption{\textsc{Model architecture} - \textit{The model is composed of a layer $U$ (blue), receiving a feedfoward input $I_{\text{ext}}$, a layer $V$ (orange), and connections $\textbf{W}^{UV}$ and $\textbf{W}^{VU}$. Additionally, two indexes $k_{U}, k_{V}$  are extracted from the layers and
    corresponds to the selection made by the two populations as $k_{U}=\text{argmax}_{k} \{\textbf{u}\}$, $k_{V}=\text{argmax}_{k} \{\textbf{v}\}$.}}
    \label{fig:main_architecture}
\end{figure}

\noindent Importantly, the two layers are not fully connected and the matrices are diagonal.
More in detail, the weight matrix $\textbf{W}^{VU}$ is simply made of $1$s, while $\widetilde{\textbf{W}}^{UV}$ is a function of the actual weights $\Phi_{v}(\textbf{W}^{UV})$ and it represents the contribution of the active options $\textbf{u}$ to the value representation $\textbf{v}$, it is thus
referred to as \textit{option value function}. The matrix $\textbf{W}^{UV}$ is initialized to all zeroes.
The function $\Phi_{v}$ is defined as weighted sum of a generalized sigmoid and a Gaussian, whose shape is characterized by a bell curve smoothly settling to a constant value. For details see the appendix \ref{sec:appendix}.

The motivation behind our choice of $\Phi_{v}$ is to be agnostic about its final form, and allow competition or integration of two distinct traits of the function shape.
In particular, one corresponds to a smooth transition to a plateau value with a certain steepness (or gain), which can represent a saturation once a threshold is crossed, such features has been reported for both biological and artificial neurons \cite{ockerFlexibleNeuralConnectivity2020, apicellaSurveyModernTrainable2021}.
The other is a bell-shaped curve with a defined center and width, which can allow for placing emphasis on values only within a given window and modulate information transfer \cite{millerCombinedMechanismsNeural2019}.

The model hyperparameters were optimized for maximizing the average total reward over multiple runs. In particular, given the non-differentiability of the model with respect to the fitness function we employed an evolutionary algorithm, more specifically CMA-ES.

\subsubsection{Option selection}
The decision-making process within a single round is structured in two distinct phases. Initially, the model receives a constant external input targeting all neurons in the memory population \textit{U} equally.
During this phase, $\textbf{I}_{\text{ext}}$ works as an equilibrium value while the reciprocal interactions with population \textit{V} push $\textbf{u}$ to different values, depending on the current policy encoded in $\widetilde{\textbf{W}}^{UV}$.
Importantly, the weights $\textbf{W}^{UV}$ are initialized to zero, and thus the input from $U$ to $V$ is uniform. This approach ensures the absence of biases towards any arm by having all weights equal, and corresponds to a completely untrained network.
After a fixed amount of time $\sim 2 \text{s}$, the second phase begins. Here, the external input is removed and the model is left to evolve autonomously, and since there are no recurrent connections in neither population the dynamics are entirely driven by their coupling.
A selection $k$ is sampled after another fixed amount of time $\sim 5 \text{s}$, and it is defined according to the following rule:

\begin{equation*}
    k =
    \left\{
        \begin{array}{ll}
            \text{argmax}_{k}\{\textbf{v}\} & \text{\textit{if}}\; \text{argmax}_{k} \{\textbf{v}\} = \text{argmax}_{k} \{\textbf{u}\} \\
            \text{random}(K) & \text{\textit{otherwise}}
        \end{array}
    \right.
\end{equation*}

\noindent The selection rule is simple: if the value representation $\textbf{v}$ is in agreement with the memory trace $\textbf{u}$, then the option with the highest value is selected. Otherwise, a random option is chosen.
This rule is a way to express the exploration-exploitation trade-off, and it is dependent on the current value of the weights $\widetilde{\textbf{W}}^{UV}$. \\ Below in subsection \ref{alg:decision1}, it is reported the pseudo-code for the algorithm behind the selection process, which is applied during each round $t$.


% --- ALGORITHM
\begin{algorithm}[ht]
\caption{Two-phases option selection process}
\label{alg:decision}
\SetAlgoLined
\KwIn{External input $\textbf{I}_{\text{ext}}$, population $\textbf{u}$, population $\textbf{v}$, weights $\widetilde{\textbf{W}}^{UV}$}
\KwOut{Selected action $k$}

\SetKwComment{Comment}{// }{ }

\textbf{Phase 1:} \textit{external input} \Comment*[r]{Duration: $\sim$2s}
Define constant $\textbf{I}_{\text{ext}}$\;
Update populations $\textbf{u}, \textbf{v}$ according to \ref{eq:main}\;

\textbf{Phase 2:} \textit{autonomous evolution} \Comment*[r]{Duration: $\sim$2s}
Remove external input $\textbf{I}_{\text{ext}}$\;
Let system evolve through population coupling according to \ref{eq:main}\;

\textbf{Selection process:}\;
$k_{u} \gets \text{argmax}_{k}\{\textbf{u}\}$\;
$k_{v} \gets \text{argmax}_{k}\{\textbf{v}\}$\;
\eIf{$k_{u} = k_{v}$}{
    $k \gets k_{v}$ \Comment*[r]{Exploitation}}{
    $k \gets \text{random}(K)$ \Comment*[r]{Exploration}
}
\Return $k$
\end{algorithm}\label{alg:decision1}

\noindent Lastly, the structure of the option selection process resembles the prefrontal circuitry, as choices emerge from network state sampling following a period of autonomous neural activity. The stability of these neural activations depends on the strength and reliability of the highest-valued option \cite{backmanEffectsWorkingMemoryTraining2011, enelStableDynamicRepresentations2020}.

\noindent According to the values of the policy's parameters, the behaviour of the model displays periods of exploration followed by a steady exploitation, which can be reverted in case of a change in the environment's reward distribution.

\subsection{Learning}
Given a selected option $k$, the environment (set of bandits) samples and returns a reward $R\in \{0, 1\}$ with probability $p_{k}$.
Then, the weights $\textbf{W}^{UV}$ for the neuron corresponding to the option $k$ are updated according to the following plasticity rule:

\begin{equation}
    \Delta \textbf{W}^{UV}_{k} = \tilde{\eta}_{k} \left(R\cdot w^{+}- \textbf{W}^{UV}_{k}\right)
\end{equation}

\noindent where $w^{+}$ is a constant maximum synaptic weight, while $\tilde{\eta}_{k}$ is the learning rate for the option $k$ determined by a function $\Phi_{\eta}$ of the current weights $\textbf{W}^{UV}_{k}$, referred to as \textit{learning rate function}.

The shape of $\Phi_{\eta}$ is again a Gaussian-sigmoid but with different parameters, giving evolution the opportunity to combine the two characteristic traits of plateau and bell-shaped tuning.
In particular, these features can be combined so to define mechanisms of synapse-type specific plasticity as a function of the current synaptic strenght \cite{larsenSynapsetypespecificPlasticityLocal2015}, as well the application of other useful homeostatic constraints with computational advantages, such as synaptic scaling and proportional updates \cite{citriSynapticPlasticityMultiple2008, kennedySynapticSignalingLearning2016, samavatSynapticInformationStorage2024}.


