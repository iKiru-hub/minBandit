
\subsection{Model description}
The model is constructed as a rate network of two populations of neurons \textit{U} and \textit{V}, the former representing the memory trace of the \textit{K} available options (\textit{i.e.} the bandits), and the latter the value of the options under the current policy.
More formally, the model is defined by a set of coupled ordinary differential equations (ODEs).
The first equation tracks the evolution of the neural activity $\textbf{u}$ of population \textit{U}, while the second tracks the activity $\textbf{v}$ of the population \textit{V}. The time constant $\tau$ is the same for both equations and it is set to $10$ms.

\begin{equation}
\begin{aligned}
    \tau \dot{\textbf{u}}&= -\textbf{u} + \textbf{W}^{VU}\phi_{v}(\textbf{v}) + \textbf{I}_{\text{ext}} \\
    \tau \dot{\textbf{v}}&= -\textbf{v} + \tilde{\textbf{W}}^{UV}\phi_{u}\textbf{u}
\end{aligned}
\end{equation}\label{eq:main}

\noindent The external input $\textbf{I}_{\text{ext}}$ is a constant input that is used to set the initial conditions of the neural activity $\textbf{u}$.
The activation functions $\phi_{v},\phi_{u}$ are applied to population $v$, and represent two distinct neural response function tailored to each population. They have been chosen to be a step-function with threshold $\theta_{v},\theta_{v}$ applied to a generalized sigmoid with gain $g_{v},g_{u}$ and offset $s_{v},s_{u}$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/minb_architecture.png}
    \caption{\textsc{Model architecture} - \textit{The model is composed of a layer $U$ (blue), receiving a feedfoward input $I_{\text{ext}}$, a layer $V$ (orange), and connections $\textbf{W}^{UV}$ and $\textbf{W}^{VU}$. Additionally, two indexes $k_{U}, k_{V}$  can be extracted from the layers and
    corresponds to the selection made by the two populations as $k_{U}=\text{argmax}_{k} \{\textbf{u}\}$, $k_{V}=\text{argmax}_{k} \{\textbf{v}\}$.}}
    \label{fig:main_architecture}
\end{figure}

\noindent Importantly, the two layers are not fully connected and the matrices are diagonal.
More in detail, the weight matrix $\textbf{W}^{VU}$ is simply made of $1$s, while $\tilde{\textbf{W}}^{UV}$ is a function of the actual weights $\Phi_{v}(\textbf{W}^{UV})$ and it represents the contribution of the
active options $\textbf{u}$ to the value representation $\textbf{v}$, it is thus referred to as \textit{option value function}.
The function $\Phi_{v}$ is defined as the sum of a generalized sigmoid and a Gaussian, whose shape is characterized by a bell curve smoothly settling to a constant value. See more in the appendinx \ref{sec:appendix}.

\subsubsection{Option selection}
The decision-making process within a single round is structured in two distinct phases. Initially, the model receives a constant external input targeting all neurons in the memory population \textit{U} equally.
During this phase, $\textbf{I}_{\text{ext}}$ works as an equilibrium value while the reciprocal interactions with population \textit{V} push $\textbf{u}$ to different values, depending on the current policy encoded in $\tilde{\textbf{W}}^{UV}$.
Importantly, the weights $\textbf{W}^{UV}$ are initialized to zero, and thus the input from $U$ to $V$ is uniform. This approach ensures the absence of biases towards any arm by having all weights equal, and corresponds to an untrained network with blank connections\footnote{\textbf{NB} do \textit{blank connections} even make sense}.
After a fixed amount of time $\sim 2 \text{s}$, the second phase begins. Here, the external input is removed and the model is left to evolve autonomously, and since there are no recurrent connections in neither population the dynamics are entirely driven by their coupling.
A selection $k$ is sampled after another fixed amount of time $\sim 5 \text{s}$, and it is defined according to the following rule:

\begin{equation*}
    k =
    \left\{
        \begin{array}{ll}
            \text{argmax}_{k}\{\textbf{v}\} & \text{\textit{if}}\; \text{argmax}_{k} \{\textbf{v}\} = \text{argmax}_{k} \{\textbf{u}\} \\
            \text{random}(K) & \text{\textit{otherwise}}
        \end{array}
    \right.
\end{equation*}

\noindent The selection rule is simple: if the value representation $\textbf{v}$ is in agreement with the memory trace $\textbf{u}$, then the option with the highest value is selected. Otherwise, a random option is chosen.
This rule is a way to express the exploration-exploitation trade-off, and it is dependent on the current policy $\tilde{\textbf{W}}^{UV}$. \\ Below \ref{alg:decision1}, is reported the pseudo-code for algorithm behind the selection process, which is applied during each round $t$.

% --- ALGORITHM
\begin{algorithm}[ht]
\caption{Two-phases option selection process}
\label{alg:decision}
\SetAlgoLined
\KwIn{External input $\textbf{I}_{\text{ext}}$, population $\textbf{u}$, population $\textbf{v}$, weights $\tilde{\textbf{W}}^{UV}$}
\KwOut{Selected action $k$}

\SetKwComment{Comment}{// }{ }

\textbf{Phase 1:} \textit{external input} \Comment*[r]{Duration: $\sim$2s}
Define constant $\textbf{I}_{\text{ext}}$\;
Update populations $\textbf{u}, \textbf{v}$ according to \ref{eq:main}\;

\textbf{Phase 2:} \textit{autonomous evolution} \Comment*[r]{Duration: $\sim$2s}
Remove external input $\textbf{I}_{\text{ext}}$\;
Let system evolve through population coupling according to \ref{eq:main}\;

\textbf{Selection process:}\;
$k_{u} \gets \text{argmax}_{k}\{\textbf{u}\}$\;
$k_{v} \gets \text{argmax}_{k}\{\textbf{v}\}$\;
\eIf{$k_{u} = k_{v}$}{
    $k \gets k_{v}$ \Comment*[r]{Exploitation}}{
    $k \gets \text{random}(K)$ \Comment*[r]{Exploration}
}
\Return $k$
\end{algorithm}\label{alg:decision1}

\noindent In figure \ref{fig:sel1} it is shown the history of selections over three trials. The initial rounds feature higher variability. In particular, it can be noted how the policy adopted by the model encounters periods of exploration and successive settling over an explotative strategy, which can be reverted in case of a change in the environment's reward distribution.

% selections over time
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/selections_1.png}
    \caption{\textsc{Selection evolution over rounds} - \textit{the y-axis represents the available arms, while the x-axis the number of rounds, with the dotted vertical lines indicating the start of a new trial with 150 rounds each.
The model selections are the black vertical lines for an arm and a round. The red horizontal lines signal the arm with the highest reward probability, thus representing the best (and greediest) selection.}}
    \label{fig:sel1}
\end{figure}


\subsection{Learning}
Given a selected option $k$, the environment (set of bandits) samples and returns a reward $R\in [0, 1]$ with probability $p_{k}$.
Then, the connections $\textbf{W}^{UV}$ for the neuron corresponding to the option $k$ are updated according to the following plasticity rule:

\begin{equation}
    \Delta \textbf{W}^{UV}_{k} = \tilde{\eta}_{k} \left(R\cdot w^{+}- \textbf{W}^{UV}_{k}\right)
\end{equation}

\noindent where $w^{+}$ is a constant maximum synaptic weight, while $\tilde{\eta}_{k}$ is the learning rate for the option $k$ determined by a function $\Phi_{\eta}$ of the current weights $\textbf{W}^{UV}_{k}$, referred to as \textit{learning rate function}, and its shape is the same as $\Phi_{v}$, but with different parameters.


\subsection{Bio-inspired features}

The model is inspired by the functioning of the prefrontal cortex (PFC) and its importance in decision-making processes. In particular, the two population $U, V$ of the model can be related to the orbito-frontal cortex (OFC) and anterior cingulate cortex (ACC), respectively.
More specifically, the OFC is known to be involved in the representation of the state different options and update their value with respect to rewarding outcomes and their history \cite{lukChoiceCodingFrontal2013, kennerleyDecisionMakingReward2011a}.
The ACC has been associated to action values and influencing the exploration-exploitation assessment \cite{khamassiChapter22Medial2013}. Further, its dynamic interplay with the OFC is observed to elicit transient pre-stimulus activation, which biases the decision towards the most valuable option \cite{funahashiPrefrontalContributionDecisionMaking2017, marcosDeterminingMonkeyFree2016, balewskiValueDynamicsAffect2023}.

In the model, the first layer represents the available options, while the learned connections with the second layer encode their values based on the recent reward history.
Another similarity with this particular pre-frontal circuit is the realization of a choice as a sample of the network state after a period of autonomous neural activity, where the stability of the neural activations depend on the strength and reliability of the highest option value \cite{backmanEffectsWorkingMemoryTraining2011, enelStableDynamicRepresentations2020}.
Moreover, the application of the function $\Phi_{v}$ on the connections $\textbf{W}^{UV}$ can be regarded as meta-plasticity, mediated by a neuromodulator \cite{wangMetalearningNaturalArtificial2021}.
Lastly, learning follows a simple hetero-synaptic rule with a symmetric kernel (\textit{i.e.} there is no bias towards synaptic potentiation or depression), a features that is not uncommon for learning rules \cite{parkSymmetryLearningRate2017}.
The learning rate is modulated by a non-linear function of the weights, which can be again regarded as a form of meta-learning \cite{inglisModulationDopamineAdaptive2021, iigayaAdaptiveLearningDecisionmaking2016}.
Further, the synaptic specificity of the plasticity rate is a well documented trait of biolgical neurons, known as synapse-type specific plasticity (STSP) \cite{larsenSynapsetypespecificPlasticityLocal2015}.

