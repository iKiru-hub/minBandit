\section{Results}

The model has been tested in a series of benchmark environments, each with a different number of arms and reward distributions. The performance has been compared with the following algorithms: Random Baseline, Upper-Confidence Bound (UCB), Thompson Sampling, and Epsilon-Greedy. The results are summarized in table \ref{tab:model_performance}.

% Description of the environments
\subsection{Game variants}
\noindent The game environments considered in this work are non-stationary K-Armed Bandits with Binomial rewards. In particular, the agent is evaluated over a number $T_{\text{trials}}$ of \textit{trials}, each composed by an arbitrary number $T_{\text{rounds}}$ of \textit{rounds}; each \textit{trial} is characterized by a different reward distribution $\mathbf{p}\sim\mathcal{U}(0,1)^{K}$ (although in practice the bounds have been set to $(0.1, 0.9)$ such that the distributions are less trivial). Our goal in this work is to investigate the performance of the agent in a non-stationary environment with Binomial reward distributions, meaning that its underlying distribution changes over time.
We choose this setting as it resembles an ecological scenario in which an animal has to forage in a patchy environment, where the reward of a given patch can change over time.
More specifically, we used four different variants:

\noindent \textbf{Zero-steps distribution shift} [\textsc{KAB-0}]: the reward distribution changes immediately at the end of a trial $i$ to a new one $i+1$ as $\mathbf{\omega}_{i} \to \mathbf{\omega}_{i+1}$. 

\noindent \textbf{Epsilon-steps distribution shift} [\textsc{KAB-$\epsilon$}]: the reward distribution $\omega$ changes gradually over rounds, tracked as time $t$, such that its shape tends towards a target distribution $\hat{\omega}_{i}$ as $\tau_{\omega}\dot{\omega}_{t}=\hat{\omega}_{i}-\omega_{t}$. Once distance is below a threshold $\epsilon$ as $\vert \hat{\omega}_{i} - \omega_{t}\vert < \epsilon$, the target distribution is changed to a new one $\hat{\omega}_{i}\to\hat{\omega}_{i+1}$.

\noindent \textbf{Sinusoidal distribution shift} [\textsc{KAB-$\sin$}]: the reward distribution changes over rounds, with the probability of each arm following a sine wave with a specific frequency $\lambda_{k}$ and amplitude $1$. At any given time $t$, the distribution is $\omega_{t}=\{sin(2\pi f t)\}$ and it is
normalized as $\omega_{t} = \omega_{t}(\sum_{k} \omega_{t,i})^{-1}$ such that it sums to $1$.

\noindent \textbf{Partial sinusoidal distribution shift} [\textsc{KAB-$\sin$P}]: identical to the sinusoidal distribution shift, but only a subset of the arms changes sinusoidally while the rest keep a constant value.



\subsection{Table of results}

\begin{table}[htbp]
\centering
\caption{Performance comparison across benchmark environments}
\label{tab:model_performance}
\begin{tabular}{l c c c c}
\toprule
\textbf{Model} & \textbf{\textsc{KAB-0}} & \textbf{\textsc{KAB-$\epsilon$}} & \textbf{\textsc{KAB-$\sin$}} & \textbf{KAB-$\sin$P}\\
\midrule
Random Baseline & $12.3 \pm 3.4$ & $5.1 \pm 2.1$ & $25.6 \pm 4.2$ & $10.7 \pm 3.1$ \\
UCB & $215.7 \pm 22.3$ & $87.3 \pm 15.6$ & $182.4 \pm 18.7$ & $65.2 \pm 12.4$ \\
Thompson & $187.5 \pm 19.8$ & $145.6 \pm 24.3$ & $201.3 \pm 20.1$ & $92.7 \pm 17.5$ \\
$\epsilon$-Greedy & $232.1 \pm 25.6$ & $156.4 \pm 26.7$ & $215.9 \pm 22.3$ & $108.3 \pm 19.2$ \\
Proposed Method & $\mathbf{276.4 \pm 30.2}$ & $\mathbf{194.7 \pm 32.1}$ & $\mathbf{248.3 \pm 26.5}$ & $\mathbf{135.6 \pm 22.9}$ \\
\bottomrule
\end{tabular}
\end{table}
