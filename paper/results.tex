
\section{Results}

The model has been tested in a series of benchmark environments, each with a different number of arms and reward distributions. The performance has been compared with the following algorithms: Random Baseline, Upper-Confidence Bound (UCB), Thompson Sampling, and Epsilon-Greedy. The results are summarized in table \ref{tab:model_performance}.

\subsection{Zero-steps distribution shift}
In this first setting, as the end of a trial $i$ the arm distribution changes immediately to a new one $i+1$ as $\mathbf{\pi}_{i} \to \mathbf{\pi}_{i+1}$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/drawing.png}
    \caption{\textsc{Performance with variable number of arms} - \textit{each plot is a simulation with K numbers of arms, the x-axis are rounds, the central vertical line signals the start of the second trial, the y-axis is the reward fraction.
            The shaded area is the reasonable reward
    range, where the lower bound is the chance level and the upper bound the best reward (following the optimal policy). The model performance is in red, while Upper-Confidence Bound green, Thompson Sampling blue, and Epsilon-Greedy orange. }}
\label{fig:zero_1}
\end{figure}


\noindent From figure \ref{fig:zero_1} above, it is clear the ability of the model (in red) to reach almost always the optimal reward policy (\textit{i.e.} the greediest) for all trials, even after the distribution shift.
In comparison, the other algorithms start to struggle when the arms are more $100$ and the distribution changes.

Next, it has been enquired how the model selection policy evolves over time and in comparison with the other algorithms, as visualized in figure \ref{fig:sel2}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/selections_many.png}
    \caption{\textsc{Selection evolution over rounds for multiple models} - \textit{the individual plots follow the same schema of \ref{fig:sel1}, with the model name and reward per round fraction}}
    \label{fig:sel2}
\end{figure}

\noindent The principal distinction is the model's strictly greedy behaviour once a good arm is found. Only in the case of a meaningful decrease in reward the exploration is resumed, in contrast with the other approaches in which occasional sub-optimal choices are made.


\subsection{Epsilon-steps distribution shift}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/drawing2.png}
    \caption{\textsc{Performance with variable number of arms} - \textit{each plot is a simulation with K-numbers of arms, and the rest is also the same as before in \ref{fig:zero_1}}. Each trial has 3 rounds, meaning that every three steps the distribution change.}
    \label{fig:eps_1}
\end{figure}

\noindent In the setting with a smooth distribution shift the difficulty of the problem is increased, especially since short-sighted greedy behaviours are easily sub-optimal. The model (always in red) is capable of reaching and maintaining a successful profile, even with many arms available.


\subsection{Table of results}

\begin{table}[htbp]
\centering
\caption{Performance comparison across benchmark environments}
\label{tab:model_performance}
\begin{tabular}{l c c c c}
\toprule
\textbf{Model} & \textbf{\textsc{KAB-0}} & \textbf{\textsc{KAB-$\epsilon$}} & \textbf{\textsc{KAB-$\sin$}} & \textbf{KAB-$\sin$P}\\
\midrule
Random Baseline & $12.3 \pm 3.4$ & $5.1 \pm 2.1$ & $25.6 \pm 4.2$ & $10.7 \pm 3.1$ \\
UCB & $215.7 \pm 22.3$ & $87.3 \pm 15.6$ & $182.4 \pm 18.7$ & $65.2 \pm 12.4$ \\
Thompson & $187.5 \pm 19.8$ & $145.6 \pm 24.3$ & $201.3 \pm 20.1$ & $92.7 \pm 17.5$ \\
$\epsilon$-Greedy & $232.1 \pm 25.6$ & $156.4 \pm 26.7$ & $215.9 \pm 22.3$ & $108.3 \pm 19.2$ \\
Proposed Method & $\mathbf{276.4 \pm 30.2}$ & $\mathbf{194.7 \pm 32.1}$ & $\mathbf{248.3 \pm 26.5}$ & $\mathbf{135.6 \pm 22.9}$ \\
\bottomrule
\end{tabular}
\end{table}
